{
  
    
        "post0": {
            "title": "중간고사해설",
            "content": "imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . 2022-05-04 14:57:40.284732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . (풀이) . sigma = tf.Variable(3.0) mu = tf.Variable(2.0) . with tf.GradientTape() as tape: pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2) logL = tf.reduce_sum(tnp.log(pdf) ) tape.gradient(logL,[mu,sigma]) . [&lt;tf.Tensor: shape=(), dtype=float32, numpy=1129.3353&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431&gt;] . for i in range(1000): with tf.GradientTape() as tape: pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2) logL = tf.reduce_sum(tnp.log(pdf) ) slope1, slope2 = tape.gradient(logL,[mu,sigma]) mu.assign_add(slope1* 0.1/10000) # N=10000 sigma.assign_add(slope2* 0.1/10000) . mu,sigma . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.0163972&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9870595&gt;) . (2) 아래는 $X_i overset{iid}{ sim} Ber(0.8)$을 생성하는 코드이다. . tf.random.set_seed(43052) x= tf.constant(np.random.binomial(1,0.8,(10000,))) x . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])&gt; . 함수 $L(p)$을 최대화하는 $p$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $p$의 초기값은 0.3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)=p^{x_i}(1-p)^{1-x_i}$$ . (풀이) . p=tf.Variable(0.3) for i in range(1000): with tf.GradientTape() as tape: pdf = p**x * (1-p)**(1-x) logL = tf.reduce_sum(tnp.log(pdf)) slope = tape.gradient(logL,p) p.assign_add(slope* 0.1/10000) # N=10000 . p . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.8002&gt; . (3) 아래의 모형에 따라서 $ {Y_i }_{i=1}^{10000}$를 생성하는 코드를 작성하라. . $Y_i overset{iid}{ sim} N( mu_i,1)$ | $ mu_i = beta_0 + beta_1 x_i = 0.5 + 2 x_i$ , where $x_i = frac{i}{10000}$ | . (풀이) . x= tf.constant(np.arange(1,10001)/10000) y= tnp.random.randn(10000) + (0.5 + 2*x) . beta0= tf.Variable(1.0) beta1= tf.Variable(1.0) for i in range(2000): with tf.GradientTape() as tape: mu = beta0 + beta1*x pdf = tnp.exp(-0.5*(y-mu)**2) logL = tf.reduce_sum(tnp.log(pdf)) slope1, slope2 = tape.gradient(logL,[beta0,beta1]) beta0.assign_add(slope1* 0.1/10000) # N=10000 beta1.assign_add(slope2* 0.1/10000) . beta0, beta1 . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.5316726&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9530618&gt;) . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]).reshape(10,1) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (풀이) . tf.linalg.inv(X.T @ X ) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.94457323], [2.21570461]])&gt; . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (풀이) . beta= tnp.array([5,10]).reshape(2,1) . for i in range(50000): beta = beta - 0.0015 * (-2*X.T @y + 2*X.T@X@beta)/10 . beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.28579424], [2.24168098]])&gt; . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (풀이) . beta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) opt = tf.optimizers.SGD(0.0015) for i in range(50000): with tf.GradientTape() as tape: loss = (y-X@beta).T @ (y-X@beta) / 10 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.28579425], [2.24168098]])&gt; . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . (풀이) . beta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) opt = tf.optimizers.SGD(0.0015) loss_fn = lambda: (y-X@beta).T @ (y-X@beta) / 10 for i in range(50000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.28579425], [2.24168098]])&gt; . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7f85c0093c40&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (풀이) . y = y.reshape(1000,1) x1 = np.cos(t) x2 = np.cos(2*t) x3 = np.cos(3*t) x4 = np.cos(4*t) x5 = np.cos(5*t) X = tf.stack([x1,x2,x3,x4,x5],axis=1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;sgd&#39;) net.fit(X,y,batch_size=1000, epochs = 1000, verbose=0) . &lt;keras.callbacks.History at 0x7f8544703100&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(5, 1) dtype=float32, numpy= array([[ 3.0008404e+00], [ 1.0067019e+00], [ 1.8562055e-03], [-3.8460968e-03], [ 4.9710521e-01]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-2.0122595], dtype=float32)&gt;] . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7f854453eda0&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . (풀이) . x= x.reshape(2000,1) y= y.reshape(2000,1) . net= tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;sgd&#39;, loss= tf.losses.binary_crossentropy) net.fit(x,y,epochs=10000,batch_size=2000, verbose=0) . &lt;keras.callbacks.History at 0x7f854458fee0&gt; . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[4.232856]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_1/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.90837014], dtype=float32)&gt;] . plt.plot(y,&#39;.&#39;) plt.plot(net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f86aef068c0&gt;] . 4. Piecewise-linear regression (15&#51216;) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (1) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ hat{y} = hat{ beta}_0+ hat{ beta}_1x $ | . tf.random.set_seed(43054) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . &lt;keras.callbacks.History at 0x7f85c004b2e0&gt; . 케라스에 의해 추정된 $ hat{ beta}_0, hat{ beta}_1$을 구하라. . net.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . (풀이) . $ hat{ beta}_0= 0.6069048$ | $ hat{ beta}_1= 2.2616348$ | . (2) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ boldsymbol{u}= x boldsymbol{W}^{(1)}+ boldsymbol{b}^{(1)}$ | $ boldsymbol{v}= text{relu}(u)$ | $yhat= boldsymbol{v} boldsymbol{W}^{(2)}+b^{(2)}$ | . tf.random.set_seed(43056) ## 1단계 net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f86aefa36a0&gt; . ${ boldsymbol u}$를 이용하여 ${ boldsymbol v}$를 만드는 코드와 ${ boldsymbol v}$를 이용하여 $yhat$를 만드는 코드를 작성하라. . (풀이) . u=net.layers[0](x) v=net.layers[1](u) yhat=net.layers[2](v) . (3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라. . (곤이) (2) 모형은 활성화함수로 relu를 사용하였다. . (철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다. . (아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. . (짝귀) (1) 의 모형은 오버피팅의 위험이 있다. . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제나 전역최소해를 찾을 수 있다. . (2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. . (3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. . (4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. . (5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%ED%95%B4%EC%84%A4.html",
            "relUrl": "/2022/05/29/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%ED%95%B4%EC%84%A4.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "중간고사예상문제-4월 28일",
            "content": "&#51473;&#44036;&#44256;&#49324; &#50696;&#49345;&#47928;&#51228; . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . import matplotlib.pyplot as plt . tnp.experimental_enable_numpy_behavior() . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. (10점) . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . 2022-04-25 18:15:05.074138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . hint: $L( mu, sigma)$를 최대화하는 $( mu, sigma)$는 $ log L( mu, sigma)$를 역시 최대화한다는 사실을 이용할 것. . hint: $ mu$의 참값은 3, $ sigma$의 참값은 2이다. (따라서 $ mu$와 $ sigma$는 각각 2와 3근처로 추정되어야 한다.) . - 풀이 . N = 10000 . y_true=(x-3)**2/2**2 . epsilon = tnp.random.randn(N)*0.5 y=(x-3)**2/2**2+epsilon . x.shape, y.shape . (TensorShape([10000]), TensorShape([10000])) . beta = tf.Variable(2.0) alpha = tf.Variable(3.0) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = (x-beta)**2/(alpha**2) loss = tf.reduce_sum((y-yhat)**2)/N slope0,slope1 = tape.gradient(loss,[beta,alpha]) beta.assign_sub(alpha * slope0) alpha.assign_sub(alpha * slope1) . beta, alpha . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=161.78185&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=159.62784&gt;) . yhat=(x-beta)**2/(alpha**2) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;r.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5c1c17e650&gt;] . (2) . (3) . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]) # X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], # [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . -풀이 . N = 10 . y = y.reshape(N,1) . X = tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) . y=y.reshape(N,1) x=x.reshape(N,1) y.shape, X.shape . (TensorShape([10, 1]), TensorShape([10, 2])) . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.94457323], [2.21570461]])&gt; . y_hat=9.94457323+2.21570461*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5c1c0a6170&gt;] . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . -풀이 . N = 10 . y=y.reshape(N,1) X=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1) y.shape,X.shape . InvalidArgumentError Traceback (most recent call last) Input In [23], in &lt;cell line: 2&gt;() 1 y=y.reshape(N,1) -&gt; 2 X=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1) 3 y.shape,X.shape File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [10] != values[1].shape = [10,1] [Op:Pack] name: stack . y=y.reshape(N,1) X.shape, y.shape . (TensorShape([10, 2]), TensorShape([10, 1])) . beta_hat = tnp.array([9,2]).reshape(2,1) beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[9], [2]])&gt; . alpha = 0.001 . for epoc in range(1000): slope = (-2*X.T @ y + 2*X.T@ X @ beta_hat)/N beta_hat = beta_hat - alpha * slope . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.03545357], [2.25155218]])&gt; . y_hat=9.03545357+2.25155218*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5b7c5b8f10&gt;] . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . -풀이 . opt.apply_gradients([(slope,beta_hat)]) beta . NameError Traceback (most recent call last) Input In [2], in &lt;cell line: 1&gt;() -&gt; 1 opt.apply_gradients([(slope,beta_hat)]) 2 beta NameError: name &#39;opt&#39; is not defined . X.shape,y.shape . (TensorShape([10, 2]), TensorShape([10, 1])) . beta_hat = tf.Variable(tnp.array([9,2],dtype=&#39;float64&#39;).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.], [2.]])&gt; . alpha=0.001 opt = tf.keras.optimizers.SGD(alpha) . NameError Traceback (most recent call last) Input In [1], in &lt;cell line: 2&gt;() 1 alpha=0.001 -&gt; 2 opt = tf.keras.optimizers.SGD(alpha) NameError: name &#39;tf&#39; is not defined . for epoc in range(1000): with tf.GradientTape() as tape: yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) opt.apply_gradients( [(slope,beta_hat)] ) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.03545358], [2.25155218]])&gt; . y_hat=9.03545358+2.25155218*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5b7c43fa30&gt;] . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . - 풀이 . mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.9609637260437012&gt; . mseloss_fn=tf.losses.MeanSquaredError() . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([10, 2]), TensorShape([10, 1])) . beta_hat = tf.Variable(tnp.array([9,2],dtype=&#39;float64&#39;).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.], [2.]])&gt; . alpha=0.0015 opt = tf.keras.optimizers.SGD(alpha) . mseloss_fn(y.reshape(-1),yhat.reshape(-1)) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.960963785648346&gt; . def loss_fn(): yhat= X@beta_hat loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.04793931], [2.25105986]])&gt; . y_hat=9.03545358+2.25155218*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5b7c4a7460&gt;] . hint1 alpha=0.0015로 설정할 것 . hint2 epoc은 10000번정도 반복실행하며 적당한 횟수를 찾을 것 . hint3 (1)의 최적값에 반드시 정확히 수렴시킬 필요는 없음 (너무 많은 에폭이 소모됨) . hint4 초기값으로 [5,10] 정도 이용할 것 . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.1 plt.plot(t,y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7f5b7c3cf640&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . 6주차 keras 예제3번 활용 | . X = np.stack([np.ones(1000),np.cos(1*t),np.cos(2*t),np.cos(3*t),np.cos(4*t),np.cos(5*t)],axis=1) y = y.reshape(1000,1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,use_bias=False)) net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) net.fit(X,y,epochs=30, batch_size=N) . Epoch 1/30 100/100 [==============================] - 0s 711us/step - loss: 0.4868 Epoch 2/30 100/100 [==============================] - 0s 852us/step - loss: 0.0094 Epoch 3/30 100/100 [==============================] - 0s 789us/step - loss: 0.0095 Epoch 4/30 100/100 [==============================] - 0s 747us/step - loss: 0.0094 Epoch 5/30 100/100 [==============================] - 0s 744us/step - loss: 0.0095 Epoch 6/30 100/100 [==============================] - 0s 713us/step - loss: 0.0094 Epoch 7/30 100/100 [==============================] - 0s 721us/step - loss: 0.0092 Epoch 8/30 100/100 [==============================] - 0s 788us/step - loss: 0.0093 Epoch 9/30 100/100 [==============================] - 0s 759us/step - loss: 0.0093 Epoch 10/30 100/100 [==============================] - 0s 729us/step - loss: 0.0095 Epoch 11/30 100/100 [==============================] - 0s 795us/step - loss: 0.0093 Epoch 12/30 100/100 [==============================] - 0s 742us/step - loss: 0.0093 Epoch 13/30 100/100 [==============================] - 0s 725us/step - loss: 0.0093 Epoch 14/30 100/100 [==============================] - 0s 761us/step - loss: 0.0095 Epoch 15/30 100/100 [==============================] - 0s 690us/step - loss: 0.0094 Epoch 16/30 100/100 [==============================] - 0s 699us/step - loss: 0.0093 Epoch 17/30 100/100 [==============================] - 0s 598us/step - loss: 0.0095 Epoch 18/30 100/100 [==============================] - 0s 612us/step - loss: 0.0094 Epoch 19/30 100/100 [==============================] - 0s 696us/step - loss: 0.0095 Epoch 20/30 100/100 [==============================] - 0s 661us/step - loss: 0.0095 Epoch 21/30 100/100 [==============================] - 0s 687us/step - loss: 0.0092 Epoch 22/30 100/100 [==============================] - 0s 671us/step - loss: 0.0094 Epoch 23/30 100/100 [==============================] - 0s 691us/step - loss: 0.0095 Epoch 24/30 100/100 [==============================] - 0s 706us/step - loss: 0.0094 Epoch 25/30 100/100 [==============================] - 0s 695us/step - loss: 0.0094 Epoch 26/30 100/100 [==============================] - 0s 706us/step - loss: 0.0095 Epoch 27/30 100/100 [==============================] - 0s 683us/step - loss: 0.0095 Epoch 28/30 100/100 [==============================] - 0s 743us/step - loss: 0.0094 Epoch 29/30 100/100 [==============================] - 0s 800us/step - loss: 0.0094 Epoch 30/30 100/100 [==============================] - 0s 797us/step - loss: 0.0095 . &lt;keras.callbacks.History at 0x7f5b7c2e5e10&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(6, 1) dtype=float32, numpy= array([[-1.9899189e+00], [ 3.0180848e+00], [ 1.0091044e+00], [-1.8584070e-03], [-4.8465207e-03], [ 4.9381223e-01]], dtype=float32)&gt;] . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7f5b747d0550&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . 7주차 Logistic regression 예제 참고 | . x.shape, y.shape . (TensorShape([2000]), TensorShape([2000])) . x=x.reshape(2000,1) x.shape, y.shape . (TensorShape([2000, 1]), TensorShape([2000])) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) bceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat)) net.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=10000,verbose=0,batch_size=2000) . &lt;keras.callbacks.History at 0x7f5d075eed10&gt; . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[5.09306]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_1/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-1.0963831], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5d0730e5f0&gt;] . 4. Piecewise-linear regression (15&#51216;) . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. . (2) . (3) . (4) . (5) . some notes . - 용어를 모르겠는 분은 질문하시기 바랍니다. . - 풀다가 에러나는 코드 질문하면 에러 수정해드립니다. .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C(4%EC%9B%94-28%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C(4%EC%9B%94-28%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "10주차-5월 09일",
            "content": "imports . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . tf.config.experimental.list_physical_devices() . [PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . 2022-05-23 15:02:33.756672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . softmax function . &#47196;&#51648;&#49828;&#54001; &#47784;&#54805; (1): &#54876;&#49457;&#54868;&#54632;&#49688;&#47196; sigmoid &#49440;&#53469; . - 기본버전은 아래와 같다 $$y_i approx text{sigmoid}(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})= frac{ exp(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})}{1+ exp(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})}$$ . - 벡터버전은 아래와 같다. $${ boldsymbol y} approx text{sigmoid}({ bf X}{ bf W} + b) = frac{ exp({ bf XW} +b)}{1+ exp({ bf XW} +b)}$$ . - 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다. . ${ bf X}$: (n,784) matrix . | ${ boldsymbol y}$: (n,1) matrix . | ${ bf W}$: (784,1) matrix . | $b$: (1,1) matrix . | +, exp 는 브로드캐스팅 . | . &#47196;&#51648;&#49828;&#54001; &#47784;&#54805; (2): &#54876;&#49457;&#54868;&#54632;&#49688;&#47196; softmax &#49440;&#53469; . - $y_i=0 text{ or } 1$ 대신에 $ boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] text { or } [0,1]$와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?) . - 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다. . $$[{ boldsymbol y}_1 ~ { boldsymbol y}_2] propto [ { bf X}{ bf W}_1 ~ { bf X}{ bf W}_2] + [b_1 ~ b_2]= { bf X} [{ bf W}_1 { bf W}_2] + [b_1 ~ b_2]= { bf X}{ bf W} + { boldsymbol b}$$ . 여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다. . ${ bf X}$: (n,784) matrix . | ${ boldsymbol y}_1,{ boldsymbol y}_2$: (n,1) matrix . | ${ boldsymbol y}:=[{ boldsymbol y}_1~ { boldsymbol y}_2]$: (n,2) matrix . | ${ bf W}_1$, ${ bf W}_2$: (784,1) matrix . | ${ bf W}:=[{ bf W}_1~ { bf W}_2]$: (784,2) matrix . | $b_1,b_2$: (1,1) matrix . | $ boldsymbol{b}:= [b_1 ~b_2] $: (1,2) matrix . | + 는 브로드캐스팅 . | . - 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 ${ bf X} { bf W}_1 + b_1$와 ${ bf X} { bf W}_2 + b_2$의 row값이 클수록 ${ boldsymbol y}_1$와 ${ boldsymbol y}_2$의 row값이 1이어야 함 . ${ boldsymbol y}_1 propto { bf X} { bf W}_1 + b_1$ $ to$ ${ bf X} { bf W}_1 + b_1$의 row값이 클수록 $ boldsymbol{y}_1$의 row 값이 1이라면 모형계수를 잘 추정한것 | ${ boldsymbol y}_2 propto { bf X} { bf W}_2 + b_2$ $ to$ ${ bf X} { bf W}_2 + b_2$의 row값이 클수록 $ boldsymbol{y}_2$의 row 값을 1이라면 모형계수를 잘 추정한것 | . - (문제) ${ bf X}{ bf W}_1 +b_1$의 값이 500, ${ bf X}{ bf W}_2 +b_2$의 값이 200 인 row가 있다고 하자. 대응하는 $ boldsymbol{y}_1, boldsymbol{y}_2$의 row값은 얼마로 적합되어야 하는가? . (1) $[0,0]$ . (2) $[0,1]$ . (3) $[1,0]$ &lt;-- 이게 답이다! . (4) $[1,1]$ . . Note: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. ${ bf X}{ bf W}_1 +b_1$의 값이 ${ bf X}{ bf W}_2 +b_2$의 값보다 크므로 (3)번이 합리적임 . - 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 $ hat{ boldsymbol{y}}_1, hat{ boldsymbol{y}}_2$를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다. . (1) $ hat{ boldsymbol{y}}_1$, $ hat{ boldsymbol{y}}_2$의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까) . (2) $ hat{ boldsymbol{y}}_1+ hat{ boldsymbol{y}}_2={ bf 1}$ 이어야 한다. (확률의 총합은 1이니까!) . (3) $ hat{ boldsymbol{y}}_1$와 $ hat{ boldsymbol{y}}_2$를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. . - 아래와 같은 활성화 함수를 도입하면 어떨까? . $$ hat{ boldsymbol{y}}=[ hat{ boldsymbol y}_1 ~ hat{ boldsymbol y}_2] = big[ frac{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} ~~ frac{ exp({ bf X} hat{ bf W}_2+ hat{b}_2)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} big]$$ . - (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다 . (1) $ hat{ boldsymbol{y}}_1$, $ hat{ boldsymbol{y}}_2$의 각 원소는 0보다 크고 1보다 작아야 한다. --&gt; OK! . (2) $ hat{ boldsymbol{y}}_1+ hat{ boldsymbol{y}}_2={ bf 1}$ 이어야 한다. --&gt; OK! . (3) $ hat{ boldsymbol{y}}_1$와 $ hat{ boldsymbol{y}}_2$를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. --&gt; ??? . - 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고) . - 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자. $$ hat{ boldsymbol y} = text{softmax}({ bf X} hat{ bf W} + { boldsymbol b}) = big[ frac{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} ~~ frac{ exp({ bf X} hat{ bf W}_2+ hat{b}_2)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} big] $$ . sigmoid, softmax . softmax&#45716; sigmoid&#51032; &#54869;&#51109;&#54805; . - 아래의 수식을 관찰하자. $$ frac{ exp( beta_0+ beta_1 x_i)}{1+ exp( beta_0+ beta_1x_i)}= frac{ exp( beta_0+ beta_1 x_i)}{e^0+ exp( beta_0+ beta_1x_i)}$$ . - 1을 $e^0$로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. . 모형2: ${ bf X} hat{ bf W}_1 + hat{b}_1$ 와 ${ bf X} hat{ bf W}_2 + hat{b}_2$ 의 크기를 비교하고 확률 결정 | 모형1: ${ bf X} hat{ bf W} + hat{b}$ 와 $0$의 크기를 비교하고 확률 결정 = ${ bf X} hat{ bf W} + hat{b}$의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측 | . - 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다. . &#53364;&#47000;&#49828;&#51032; &#49688;&#44032; 2&#51064; &#44221;&#50864; softmax vs sigmoid . - 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 $y=0 text{ or } 1$와 같이 정리하지 않고 $y=[0,1] text{ or } [1,0]$ 와 같이 정리해도 무방할 듯 하다. . - 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 $y= 0 text{ or } 1$로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다. . - 표현력이 동등한 이유? 아래 수식을 관찰하자. $$ big( frac{e^{300}}{e^{300}+e^{500}}, frac{e^{500}}{e^{300}+e^{500}} big) = big( frac{e^{0}}{e^{0}+e^{200}}, frac{e^{200}}{e^{0}+e^{200}} big)$$ . $ big( frac{e^{300}}{e^{300}+e^{500}}, frac{e^{500}}{e^{300}+e^{500}} big)$를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다. | $( hat{ boldsymbol{y}}_1, hat{ boldsymbol{y}}_2)$의 표현에서도 ${ bf X} hat{ bf W}_1 + hat{b}_1$ 와 ${ bf X} hat{ bf W}_2 + hat{b}_2$ 라는 숫자 각각이 필요한 것이 아니고 $({ bf X} hat{ bf W}_1 + hat{b}_1)-({ bf X} hat{ bf W}_2 + hat{b}_2)$의 값만 알면 된다. | . - 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다. . &#48516;&#47448;&#54624; &#53364;&#47000;&#49828;&#44032; 3&#44060; &#51060;&#49345;&#51068; &#44221;&#50864; &#49888;&#44221;&#47581; &#47784;&#54805;&#51032; &#49444;&#44228; . - y의 모양: [0 1 0 0 0 0 0 0 0 0] . - 활성화함수의 선택: softmax . - 손실함수의 선택: cross entropy . Fashion_MNIST &#50668;&#47084;&#53364;&#47000;&#49828;&#51032; &#48516;&#47448; (softmax&#51032; &#49892;&#49845;) . - 데이터정리 . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X= x_train.reshape(-1,784) y= tf.keras.utils.to_categorical(y_train) XX = x_test.reshape(-1,784) yy = tf.keras.utils.to_categorical(y_test) . - 시도1: 간단한 신경망 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y10&quot; &quot;node2&quot; -&gt; &quot;y10&quot; &quot;...&quot; -&gt; &quot;y10&quot; &quot;node30&quot; -&gt; &quot;y10&quot; &quot;node1&quot; -&gt; &quot;y1&quot; &quot;node2&quot; -&gt; &quot;y1&quot; &quot;...&quot; -&gt; &quot;y1&quot; &quot;node30&quot; -&gt; &quot;y1&quot; &quot;node1&quot; -&gt; &quot;.&quot; &quot;node2&quot; -&gt; &quot;.&quot; &quot;...&quot; -&gt; &quot;.&quot; &quot;node30&quot; -&gt; &quot;.&quot; label = &quot;Layer 2: softmax&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7efdfdb1dbd0&gt; . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 972us/step - loss: 2.5908 - accuracy: 0.4238 Epoch 2/5 1875/1875 [==============================] - 2s 983us/step - loss: 1.0855 - accuracy: 0.5799 Epoch 3/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.9115 - accuracy: 0.6509 Epoch 4/5 1875/1875 [==============================] - 2s 930us/step - loss: 0.7500 - accuracy: 0.7206 Epoch 5/5 1875/1875 [==============================] - 2s 946us/step - loss: 0.7011 - accuracy: 0.7351 . &lt;keras.callbacks.History at 0x7efdf1cafcd0&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 1ms/step - loss: 0.8616 - accuracy: 0.6925 . [0.8615812659263611, 0.6924999952316284] . net.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 30) 23550 dense_1 (Dense) (None, 10) 310 ================================================================= Total params: 23,860 Trainable params: 23,860 Non-trainable params: 0 _________________________________________________________________ . - 시도2: 더 깊은 신경망 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node500&quot; &quot;x2&quot; -&gt; &quot;node500&quot; &quot;..&quot; -&gt; &quot;node500&quot; &quot;x784&quot; -&gt; &quot;node500&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;node1(2)&quot; &quot;node2&quot; -&gt; &quot;node1(2)&quot; &quot;...&quot; -&gt; &quot;node1(2)&quot; &quot;node500&quot; -&gt; &quot;node1(2)&quot; &quot;node1&quot; -&gt; &quot;node2(2)&quot; &quot;node2&quot; -&gt; &quot;node2(2)&quot; &quot;...&quot; -&gt; &quot;node2(2)&quot; &quot;node500&quot; -&gt; &quot;node2(2)&quot; &quot;node1&quot; -&gt; &quot;....&quot; &quot;node2&quot; -&gt; &quot;....&quot; &quot;...&quot; -&gt; &quot;....&quot; &quot;node500&quot; -&gt; &quot;....&quot; &quot;node1&quot; -&gt; &quot;node500(2)&quot; &quot;node2&quot; -&gt; &quot;node500(2)&quot; &quot;...&quot; -&gt; &quot;node500(2)&quot; &quot;node500&quot; -&gt; &quot;node500(2)&quot; label = &quot;Layer 2: relu&quot; } subgraph cluster_4{ style=filled; color=lightgrey; &quot;node1(2)&quot; -&gt; &quot;y10&quot; &quot;node2(2)&quot; -&gt; &quot;y10&quot; &quot;....&quot; -&gt; &quot;y10&quot; &quot;node500(2)&quot; -&gt; &quot;y10&quot; &quot;node1(2)&quot; -&gt; &quot;y1&quot; &quot;node2(2)&quot; -&gt; &quot;y1&quot; &quot;....&quot; -&gt; &quot;y1&quot; &quot;node500(2)&quot; -&gt; &quot;y1&quot; &quot;node1(2)&quot; -&gt; &quot;.&quot; &quot;node2(2)&quot; -&gt; &quot;.&quot; &quot;....&quot; -&gt; &quot;.&quot; &quot;node500(2)&quot; -&gt; &quot;.&quot; label = &quot;Layer 3: softmax&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7efdf0043dc0&gt; . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 3s 1ms/step - loss: 2.1336 - accuracy: 0.7445 Epoch 2/5 1875/1875 [==============================] - 3s 1ms/step - loss: 0.6902 - accuracy: 0.7739 Epoch 3/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5551 - accuracy: 0.8127 Epoch 4/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4745 - accuracy: 0.8339 Epoch 5/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4505 - accuracy: 0.8405 . &lt;keras.callbacks.History at 0x7efd8c27d210&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 1s 1ms/step - loss: 0.4442 - accuracy: 0.8425 . [0.4442218244075775, 0.8424999713897705] . net.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 500) 392500 dense_3 (Dense) (None, 500) 250500 dense_4 (Dense) (None, 10) 5010 ================================================================= Total params: 648,010 Trainable params: 648,010 Non-trainable params: 0 _________________________________________________________________ . &#54217;&#44032;&#51648;&#54364; . &#45796;&#50577;&#54620; &#54217;&#44032;&#51648;&#54364;&#46308; . - 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?) . - 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values . 이걸 다 암기하는건 불가능함. | 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자! | . confusion matrix&#51032; &#51060;&#54644; . - 표1 . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP | FN | . 안나감(실제) | FP | TN | . - 표2 (책에없음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | $(y, hat{y})= $ (O,O) | $(y, hat{y})= $(O,X) | . 안나감(실제) | $(y, hat{y})= $(X,O) | $(y, hat{y})= $(X,X) | . - 표3 (책에없음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP, $ # O/O$ | FN, $ #O/X$ | . 안나감(실제) | FP, $ #X/O$ | TN, $ #X/X$ | . 암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것 | . - 표4 (위키등에 있음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP, $ # O/O$ | FN, $ # O/X$ | Sensitivity(민감도)=Recall(재현율)=$ frac{TP}{TP+FN}$=$ frac{ #O/O}{ # O/O+ #O/X}$ | . 안나감(실제) | FP, $ # X/O$ | TN, $ # X/X$ | | . | Precision(프리시즌)=$ frac{TP}{TP+FP}$=$ frac{ # O/O}{ # O/O+ # X/O}$ | | Accuracy(애큐러시)=$ frac{TP+TN}{total}$=$ frac{ #O/O+ # X/X}{total}$ | . &#49345;&#54889;&#44537; . - 최규빈은 입사하여 &quot;퇴사자 예측시스템&quot;의 개발에 들어갔다. . - 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다. . Accuracy . - 정의: Accuracy(애큐러시)=$ frac{TP+TN}{total}$=$ frac{ #O/O+ #X/X}{total}$ . 한국말로는 정확도, 정분류율이라고 한다. | 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음) | . - (상확극 시점1) 왜 애큐러시는 불충분한가? . 회사: 퇴사자예측프로그램 개발해 | 최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy | . 모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다? . Sensitivity(&#48124;&#44048;&#46020;), Recall(&#51116;&#54788;&#50984;), True Positive Rate(TPR) . - 정의: Sensitivity(민감도)=Recall(재현율)=$ frac{TP}{TP+FN}$=$ frac{ # O/O}{ # O/O+ # O/X}$ . 분모: 실제 O인 관측치 수 | 분자: 실제 O를 O라고 예측한 관측치 수 | 뜻: 실제 O를 O라고 예측한 비율 | . - (상황극 시점2) recall을 봐야하는 이유 . 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) | 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요? | . 인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다! 예시1:실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50% &gt; 예시2:최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0% . | . 결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다) | . Precision . - 정의: Precision(프리시즌)=$ frac{TP}{TP+FP}$=$ frac{ # O/O}{ # O/O+ # X/O}$ . 분모: O라고 예측한 관측치 | 분자: O라고 예측한 관측치중 진짜 O인 관측치 | 뜻: O라고 예측한 관측치중 진짜 O인 비율 | . - (상황극 시점3) recall 만으로 불충분한 이유 . 최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감! | . 이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로). . 인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요. . | 인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요. . | . 최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요? | . 인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 $ frac{10}{100}$이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!) | . F1 score . - 정의: recall과 precision의 조화평균 . - (상황극 시점4) recall, precision을 모두 고려 . 최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다. . | 최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까? . | . 인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요. | . Specificity(&#53945;&#51060;&#46020;), False Positive Rate(FPR) . - 정의: . (1) Specificity(특이도)=$ frac{TN}{FP+TN}$=$ frac{ # X/X}{ # X/O+ # X/X}$ . (2) False Positive Rate (FPR) = 1-Specificity(특이도) = $ frac{FP}{FP+TN}$=$ frac{ # X/O}{ # X/O+ # X/X}$ . - 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ . specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. | FPR은 recall을 올리기 위해서 &quot;실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들&quot; 의 비율이다. | . 즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율.. . ROC curve . - 정의: $x$축=FPR, $y$축=TPR 을 그린 커브 . - 의미: . 결국 &quot;오해해서 미안해 vs recall&quot;을 그린 곡선이 ROC커브이다. | 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. | 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다. | . Fashion MNIST &#45796;&#50577;&#54620; &#54217;&#44032;&#51648;&#54364;&#54876;&#50857; . - data . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X= x_train.reshape(-1,784) y= tf.keras.utils.to_categorical(y_train) XX = x_test.reshape(-1,784) yy = tf.keras.utils.to_categorical(y_test) . - 다양한 평가지표를 넣는 방법 (1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 4s 2ms/step - loss: 2.1982 - accuracy: 0.7396 - recall: 0.7006 Epoch 2/5 1875/1875 [==============================] - 5s 2ms/step - loss: 0.6612 - accuracy: 0.7814 - recall: 0.7237 Epoch 3/5 1875/1875 [==============================] - 5s 2ms/step - loss: 0.5543 - accuracy: 0.8109 - recall: 0.7559 Epoch 4/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4823 - accuracy: 0.8318 - recall: 0.7839 Epoch 5/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4441 - accuracy: 0.8426 - recall: 0.7980 . &lt;keras.callbacks.History at 0x7efd8c18bdf0&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 1s 2ms/step - loss: 0.4919 - accuracy: 0.8307 - recall: 0.7898 . [0.49188804626464844, 0.8306999802589417, 0.7897999882698059] . - 다양한 평가지표를 넣는 방법 (2) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 4s 2ms/step - loss: 2.0967 - categorical_accuracy: 0.7334 - recall: 0.6851 Epoch 2/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.6815 - categorical_accuracy: 0.7700 - recall: 0.7007 Epoch 3/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5806 - categorical_accuracy: 0.8023 - recall: 0.7419 Epoch 4/5 1875/1875 [==============================] - 5s 3ms/step - loss: 0.4913 - categorical_accuracy: 0.8245 - recall: 0.7784 Epoch 5/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4579 - categorical_accuracy: 0.8375 - recall: 0.7947 . &lt;keras.callbacks.History at 0x7efdf2911b40&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 1s 1ms/step - loss: 0.5067 - categorical_accuracy: 0.8273 - recall: 0.7802 . [0.5067203044891357, 0.8273000121116638, 0.7802000045776367] . flatten layer . - 이미지 데이터를 분류하기 좋은 형태로 자료를 재정리하자. . X = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64) y = tf.keras.utils.to_categorical(y_train) XX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64) yy = tf.keras.utils.to_categorical(y_test) . X.shape,XX.shape,y.shape,yy.shape . (TensorShape([60000, 28, 28, 1]), TensorShape([10000, 28, 28, 1]), (60000, 10), (10000, 10)) . - 일반적인 이미지 분석 모형을 적용하기 용이한 데이터 형태로 정리했다. -&gt; 그런데 모형에 넣고 돌릴려면 다시 차원을 펼쳐야 하지 않을까? . - 안펼치고 하고싶다. . flttn = tf.keras.layers.Flatten() . set(dir(flttn)) &amp; {&#39;__call__&#39;} . {&#39;__call__&#39;} . X.shape,flttn(X).shape, X.reshape(-1,784).shape . (TensorShape([60000, 28, 28, 1]), TensorShape([60000, 784]), TensorShape([60000, 784])) . - flttn . net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 4s 2ms/step - loss: 2.0587 - categorical_accuracy: 0.7566 - recall_1: 0.7263 Epoch 2/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6286 - categorical_accuracy: 0.7935 - recall_1: 0.7429 Epoch 3/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5358 - categorical_accuracy: 0.8181 - recall_1: 0.7689 Epoch 4/5 1875/1875 [==============================] - 5s 3ms/step - loss: 0.4679 - categorical_accuracy: 0.8367 - recall_1: 0.7949 Epoch 5/5 1875/1875 [==============================] - 6s 3ms/step - loss: 0.4414 - categorical_accuracy: 0.8422 - recall_1: 0.8032 . &lt;keras.callbacks.History at 0x7efdf26dbcd0&gt; . net.layers . [&lt;keras.layers.core.flatten.Flatten at 0x7efdf28b1a80&gt;, &lt;keras.layers.core.dense.Dense at 0x7efdf2a9c1f0&gt;, &lt;keras.layers.core.dense.Dense at 0x7efdf28b25c0&gt;, &lt;keras.layers.core.dense.Dense at 0x7efdf28b2830&gt;] . print(X.shape) print(net.layers[0](X).shape) print(net.layers[1](net.layers[0](X)).shape) print(net.layers[2](net.layers[1](net.layers[0](X))).shape) . (60000, 28, 28, 1) (60000, 784) (60000, 500) (60000, 500) . - 좀 더 복잡한 네트워크 -&gt; 하지만 한계가 보인다 -&gt; 좀 더 나은 아키텍처는 없을까 . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=&#39;accuracy&#39;) net.fit(X,y,epochs=10) . Epoch 1/10 1875/1875 [==============================] - 3s 1ms/step - loss: 1.1107 - accuracy: 0.7893 Epoch 2/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4604 - accuracy: 0.8360 Epoch 3/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4147 - accuracy: 0.8522 Epoch 4/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3931 - accuracy: 0.8605 Epoch 5/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3743 - accuracy: 0.8678 Epoch 6/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3645 - accuracy: 0.8696 Epoch 7/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3448 - accuracy: 0.8782 Epoch 8/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3394 - accuracy: 0.8787 Epoch 9/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3352 - accuracy: 0.8789 Epoch 10/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3238 - accuracy: 0.8845 . &lt;keras.callbacks.History at 0x7efdf012f130&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 1s 2ms/step - loss: 0.4113 - accuracy: 0.8583 . [0.4112727642059326, 0.858299970626831] . - layer중에 우리는 끽해야 Dense정도 쓰고있었음. $ to$ flatten과 같은 다른 layer도 많음. $ to$ 이런것도 써보자 .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-9%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-9%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "9주차-5월 02일 (2)",
            "content": "import . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import tensorflow.experimental.numpy as tnp . tf.config.experimental.list_physical_devices() . 2022-05-09 16:55:02.512547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . &#51473;&#44036;&#44256;&#49324; &#44288;&#47144; &#51105;&#45812; . &#51473;&#44036;&#44256;&#49324; 3&#48264;&#47928;&#51228; . - 특이한모형: 오버핏이 일어날 수 없는 모형이다. . 유의미한 coef: 상수항(bias), $ cos(t)$의 계수, $ cos(2t)$의 계수, $ cos(5t)$의 계수. | 유의미하지 않은 coef: $ cos(3t)$의 계수, $ cos(4t)$의 계수 | 유의미하지 않은 계수는 $n%$이 커질수록 0으로 추정된다 = $ cos(3t)$와 $ cos(5t)$는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌 | . - 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임. . 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 $ beta$를 좀 더 정확히 추론하는게 차라리 더 이득) | 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다) | . - 직교기저의 예시 . 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다. | 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다. | 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 $ text{color}= text{red}* beta_1 + text{blue}* beta_2 + text{yellow}* beta_3$ 이다. | (빨,파,노)는 색을 표현하는 basis이다. (적절한 $ beta_1, beta_2, beta_3$을 구하기만 하면 임의의 색도 표현가능) | (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까) | (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까) | (빨,파,노)는 직교기저이다. | . - 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함 . &#51473;&#44036;&#44256;&#49324; 1-(3)&#48264; &#47928;&#51228; . - 그림을 그려보자. . _x= tf.constant(np.arange(1,10001)/10000) _y= tnp.random.randn(10000) + (0.5 + 2*_x) plt.plot(_x,_y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7fe5db8a7ac0&gt;] . - 저것 꼭 10000개 다 모아서 loss계산해야할까? . plt.plot(_x,_y,&#39;.&#39;,alpha=0.1) plt.plot(_x[::10],_y[::10],&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe5d033ed70&gt;] . - 대충 이정도만 모아서 해도 비슷하지 않을까? $ to$ 해보자! . &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277; . ver1: &#47784;&#46304; &#49368;&#54540;&#51012; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . - 단순회귀분석에서 샘플 10개 관측: $(x_1,y_1), dots,(x_{10},y_{10})$. . (epoch1) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ . (epoch2) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ . ... . ver2: &#54616;&#45208;&#51032; &#49368;&#54540;&#47564; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . (epoch1) . $loss=(y_1- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | $loss=(y_2- beta_0- beta_1x_2)^2 quad to quad slope quad to quad update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . (epoch2) . $loss=(y_1- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | $loss=(y_2- beta_0- beta_1x_2)^2 quad to quad slope quad to quad update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . ... . ver3: $m( leq n)$&#44060;&#51032; &#49368;&#54540;&#47564; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . $m=3$이라고 하자. . (epoch1) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . (epoch2) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . ... . &#50857;&#50612;&#51032; &#51221;&#47532; . &#50715;&#45216; (&#51328; &#45908; &#50628;&#48128;) . - ver1: gradient descent, batch gradient descent . - ver2: stochastic gradient descent . - ver3: mini-batch gradient descent, mini-batch stochastic gradient descent . &#50836;&#51608; . - ver1: gradient descent . - ver2: stochastic gradient descent with batch size = 1 . - ver3: stochastic gradient descent . https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고. | . note: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로 . ver1,2,3 &#51060;&#50808;&#50640; &#51328; &#45908; &#51648;&#51200;&#48516;&#54620; &#44163;&#46308;&#51060; &#51080;&#45796;. . - ver2,3에서 샘플을 셔플할 수도 있다. . - ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다. . - 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다. . Discussion . - 핵심개념 . 메모리사용량: ver1 &gt; ver3 &gt; ver2 | 계산속도: ver1 &gt; ver3 &gt; ver2 | local-min에 갇힘: ver1 &gt; ver3 &gt; ver2 | . - 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다. . - 틀리진 않지만 어색한 블로그 정리 내용들 . 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. --&gt; 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 | 경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. --&gt; 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 $ beta$를 빠르게 얻을 수 있다는 의미는 아님 | . fashion_mnist &#47784;&#46280; . tf.keras.datasets.fashion_mnist.load_data() . - tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사 . tf.keras.datasets.fashion_mnist.load_data?? . Signature: tf.keras.datasets.fashion_mnist.load_data() Source: @keras_export(&#39;keras.datasets.fashion_mnist.load_data&#39;) def load_data(): &#34;&#34;&#34;Loads the Fashion-MNIST dataset. This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The classes are: | Label | Description | |:--:|-| | 0 | T-shirt/top | | 1 | Trouser | | 2 | Pullover | | 3 | Dress | | 4 | Coat | | 5 | Sandal | | 6 | Shirt | | 7 | Sneaker | | 8 | Bag | | 9 | Ankle boot | Returns: Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`. **x_train**: uint8 NumPy array of grayscale image data with shapes `(60000, 28, 28)`, containing the training data. **y_train**: uint8 NumPy array of labels (integers in range 0-9) with shape `(60000,)` for the training data. **x_test**: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data. **y_test**: uint8 NumPy array of labels (integers in range 0-9) with shape `(10000,)` for the test data. Example: python (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() assert x_train.shape == (60000, 28, 28) assert x_test.shape == (10000, 28, 28) assert y_train.shape == (60000,) assert y_test.shape == (10000,) License: The copyright for Fashion-MNIST is held by Zalando SE. Fashion-MNIST is licensed under the [MIT license]( https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE). &#34;&#34;&#34; dirname = os.path.join(&#39;datasets&#39;, &#39;fashion-mnist&#39;) base = &#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/&#39; files = [ &#39;train-labels-idx1-ubyte.gz&#39;, &#39;train-images-idx3-ubyte.gz&#39;, &#39;t10k-labels-idx1-ubyte.gz&#39;, &#39;t10k-images-idx3-ubyte.gz&#39; ] paths = [] for fname in files: paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname)) with gzip.open(paths[0], &#39;rb&#39;) as lbpath: y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(paths[1], &#39;rb&#39;) as imgpath: x_train = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28) with gzip.open(paths[2], &#39;rb&#39;) as lbpath: y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(paths[3], &#39;rb&#39;) as imgpath: x_test = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28) return (x_train, y_train), (x_test, y_test) File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/datasets/fashion_mnist.py Type: function . &#45936;&#51060;&#53552;&#49373;&#49457; &#48143; &#53456;&#49353; . - tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성 . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 2us/step 40960/29515 [=========================================] - 0s 2us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 1s 0us/step 26435584/26421880 [==============================] - 1s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step . - 차원확인 . x_train.shape, y_train.shape, x_test.shape,y_test.shape . ((60000, 28, 28), (60000,), (10000, 28, 28), (10000,)) . 60000은 obs숫자인듯 | (28,28)은 28픽셀,28픽셀을 의미하는듯 | train/test는 6:1로 나눈것 같음 | . - 첫번째 obs . plt.imshow(x_train[0]) . &lt;matplotlib.image.AxesImage at 0x7fe5d0202620&gt; . y_train[0] . 9 . 첫번쨰 obs에 대응하는 라벨 | . - 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자. . np.where(y_train==9) . (array([ 0, 11, 15, ..., 59932, 59970, 59978]),) . y_train[11] . 9 . plt.imshow(x_train[11]) . &lt;matplotlib.image.AxesImage at 0x7fe5d00646a0&gt; . &#45936;&#51060;&#53552;&#44396;&#51312; . - ${ bf X}$: (n,28,28) . - ${ bf y}$: (n,) , $y=0,1,2,3, dots,9$ . &#50696;&#51228;1 . &#45936;&#51060;&#53552; &#51221;&#47532; . - y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까) . y= y_train[(y_train==0) | (y_train==1)].reshape(-1,1) X= x_train[(y_train==0) | (y_train==1)].reshape(-1,784) yy= y_test[(y_test==0) | (y_test==1)].reshape(-1,1) XX= x_test[(y_test==0) | (y_test==1)].reshape(-1,784) . X.shape, y.shape, XX.shape, yy.shape . ((12000, 784), (12000, 1), (2000, 784), (2000, 1)) . &#54400;&#51060;1: &#51008;&#45769;&#52789;&#51012; &#54252;&#54632;&#54620; &#49888;&#44221;&#47581; // epochs=100 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: sigmoid&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7fe56859de10&gt; . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;sgd&#39;,loss=tf.losses.binary_crossentropy) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 492ms/step - loss: 220.9145 Epoch 2/100 1/1 [==============================] - 0s 3ms/step - loss: 6800.3174 Epoch 3/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7045 Epoch 4/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7012 Epoch 5/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7004 Epoch 6/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6997 Epoch 7/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6991 Epoch 8/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6985 Epoch 9/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6979 Epoch 10/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6976 Epoch 11/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6973 Epoch 12/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6970 Epoch 13/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6968 Epoch 14/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6966 Epoch 15/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6964 Epoch 16/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6962 Epoch 17/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6961 Epoch 18/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6959 Epoch 19/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6958 Epoch 20/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6956 Epoch 21/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6955 Epoch 22/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6953 Epoch 23/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6952 Epoch 24/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6951 Epoch 25/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6949 Epoch 26/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6948 Epoch 27/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6947 Epoch 28/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6946 Epoch 29/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6945 Epoch 30/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6944 Epoch 31/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6943 Epoch 32/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6942 Epoch 33/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6942 Epoch 34/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6941 Epoch 35/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6940 Epoch 36/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6940 Epoch 37/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6939 Epoch 38/100 1/1 [==============================] - 0s 4ms/step - loss: 0.6939 Epoch 39/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6938 Epoch 40/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6937 Epoch 41/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6937 Epoch 42/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6936 Epoch 43/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6936 Epoch 44/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6935 Epoch 45/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6935 Epoch 46/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6934 Epoch 47/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6934 Epoch 48/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6934 Epoch 49/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 50/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 51/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 52/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 53/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 54/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 55/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 56/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 57/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 58/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 59/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 60/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 61/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 62/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 63/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 64/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 65/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 66/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6933 Epoch 67/100 1/1 [==============================] - 0s 5ms/step - loss: 0.6933 Epoch 68/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 69/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 70/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 71/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 72/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 73/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 74/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 75/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 76/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 77/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 78/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 79/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 80/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 81/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 82/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 83/100 1/1 [==============================] - 0s 4ms/step - loss: 0.6932 Epoch 84/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 85/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 86/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 87/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 88/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 89/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 90/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 91/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 92/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 Epoch 93/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 94/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 95/100 1/1 [==============================] - 0s 4ms/step - loss: 0.6932 Epoch 96/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 97/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 98/100 1/1 [==============================] - 0s 2ms/step - loss: 0.6932 Epoch 99/100 1/1 [==============================] - 0s 5ms/step - loss: 0.6932 Epoch 100/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6932 . &lt;keras.callbacks.History at 0x7fe568414cd0&gt; . np.mean((net(X)&gt;0.5) == y) . 0.5000833333333333 . np.mean((net(XX)&gt;0.5) == yy) . 0.5 . &#54400;&#51060;2: &#50741;&#54000;&#47560;&#51060;&#51200; &#44060;&#49440; . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 154ms/step - loss: 220.9145 Epoch 2/100 1/1 [==============================] - 0s 3ms/step - loss: 88.9451 Epoch 3/100 1/1 [==============================] - 0s 4ms/step - loss: 7.5899 Epoch 4/100 1/1 [==============================] - 0s 3ms/step - loss: 33.7523 Epoch 5/100 1/1 [==============================] - 0s 3ms/step - loss: 40.2278 Epoch 6/100 1/1 [==============================] - 0s 3ms/step - loss: 28.9657 Epoch 7/100 1/1 [==============================] - 0s 3ms/step - loss: 16.5118 Epoch 8/100 1/1 [==============================] - 0s 3ms/step - loss: 9.4905 Epoch 9/100 1/1 [==============================] - 0s 3ms/step - loss: 6.2025 Epoch 10/100 1/1 [==============================] - 0s 3ms/step - loss: 5.2417 Epoch 11/100 1/1 [==============================] - 0s 3ms/step - loss: 5.5173 Epoch 12/100 1/1 [==============================] - 0s 3ms/step - loss: 6.5902 Epoch 13/100 1/1 [==============================] - 0s 3ms/step - loss: 7.8607 Epoch 14/100 1/1 [==============================] - 0s 3ms/step - loss: 8.5884 Epoch 15/100 1/1 [==============================] - 0s 3ms/step - loss: 8.3990 Epoch 16/100 1/1 [==============================] - 0s 3ms/step - loss: 7.4674 Epoch 17/100 1/1 [==============================] - 0s 3ms/step - loss: 6.2580 Epoch 18/100 1/1 [==============================] - 0s 3ms/step - loss: 5.1273 Epoch 19/100 1/1 [==============================] - 0s 3ms/step - loss: 4.2381 Epoch 20/100 1/1 [==============================] - 0s 3ms/step - loss: 3.6032 Epoch 21/100 1/1 [==============================] - 0s 3ms/step - loss: 3.1860 Epoch 22/100 1/1 [==============================] - 0s 3ms/step - loss: 2.9232 Epoch 23/100 1/1 [==============================] - 0s 3ms/step - loss: 2.7559 Epoch 24/100 1/1 [==============================] - 0s 3ms/step - loss: 2.6420 Epoch 25/100 1/1 [==============================] - 0s 3ms/step - loss: 2.5490 Epoch 26/100 1/1 [==============================] - 0s 3ms/step - loss: 2.4612 Epoch 27/100 1/1 [==============================] - 0s 3ms/step - loss: 2.3617 Epoch 28/100 1/1 [==============================] - 0s 3ms/step - loss: 2.2378 Epoch 29/100 1/1 [==============================] - 0s 3ms/step - loss: 2.0873 Epoch 30/100 1/1 [==============================] - 0s 3ms/step - loss: 1.9117 Epoch 31/100 1/1 [==============================] - 0s 3ms/step - loss: 1.7239 Epoch 32/100 1/1 [==============================] - 0s 3ms/step - loss: 1.5408 Epoch 33/100 1/1 [==============================] - 0s 3ms/step - loss: 1.3663 Epoch 34/100 1/1 [==============================] - 0s 3ms/step - loss: 1.2210 Epoch 35/100 1/1 [==============================] - 0s 3ms/step - loss: 1.1035 Epoch 36/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0208 Epoch 37/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9766 Epoch 38/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9628 Epoch 39/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9717 Epoch 40/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9883 Epoch 41/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0039 Epoch 42/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0156 Epoch 43/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0181 Epoch 44/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0067 Epoch 45/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9808 Epoch 46/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9443 Epoch 47/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9019 Epoch 48/100 1/1 [==============================] - 0s 3ms/step - loss: 0.8571 Epoch 49/100 1/1 [==============================] - 0s 3ms/step - loss: 0.8146 Epoch 50/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7768 Epoch 51/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7489 Epoch 52/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7294 Epoch 53/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7186 Epoch 54/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7124 Epoch 55/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7080 Epoch 56/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7044 Epoch 57/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7002 Epoch 58/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6949 Epoch 59/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6884 Epoch 60/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6806 Epoch 61/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6715 Epoch 62/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6615 Epoch 63/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6510 Epoch 64/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6404 Epoch 65/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6302 Epoch 66/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6209 Epoch 67/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6127 Epoch 68/100 1/1 [==============================] - 0s 4ms/step - loss: 0.6060 Epoch 69/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6006 Epoch 70/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5963 Epoch 71/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5924 Epoch 72/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5888 Epoch 73/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5853 Epoch 74/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5816 Epoch 75/100 1/1 [==============================] - 0s 4ms/step - loss: 0.5778 Epoch 76/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5736 Epoch 77/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5691 Epoch 78/100 1/1 [==============================] - 0s 4ms/step - loss: 0.5644 Epoch 79/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5595 Epoch 80/100 1/1 [==============================] - 0s 4ms/step - loss: 0.5547 Epoch 81/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5501 Epoch 82/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5457 Epoch 83/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5417 Epoch 84/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5379 Epoch 85/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5343 Epoch 86/100 1/1 [==============================] - 0s 4ms/step - loss: 0.5309 Epoch 87/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5276 Epoch 88/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5242 Epoch 89/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5208 Epoch 90/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5174 Epoch 91/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5140 Epoch 92/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5107 Epoch 93/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5075 Epoch 94/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5044 Epoch 95/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5014 Epoch 96/100 1/1 [==============================] - 0s 3ms/step - loss: 0.4986 Epoch 97/100 1/1 [==============================] - 0s 3ms/step - loss: 0.4960 Epoch 98/100 1/1 [==============================] - 0s 3ms/step - loss: 0.4935 Epoch 99/100 1/1 [==============================] - 0s 3ms/step - loss: 0.4909 Epoch 100/100 1/1 [==============================] - 0s 3ms/step - loss: 0.4885 . &lt;keras.callbacks.History at 0x7fe5401346a0&gt; . np.mean((net(X)&gt;0.5) == y) . 0.98125 . np.mean((net(XX)&gt;0.5) == yy) . 0.977 . &#54400;&#51060;3: &#52980;&#54028;&#51068;&#49884; metrics=[&#39;accuracy&#39;] &#52628;&#44032; . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 169ms/step - loss: 220.9145 - accuracy: 0.5000 Epoch 2/100 1/1 [==============================] - 0s 3ms/step - loss: 88.9451 - accuracy: 0.5073 Epoch 3/100 1/1 [==============================] - 0s 3ms/step - loss: 7.5899 - accuracy: 0.8208 Epoch 4/100 1/1 [==============================] - 0s 4ms/step - loss: 33.7523 - accuracy: 0.5972 Epoch 5/100 1/1 [==============================] - 0s 4ms/step - loss: 40.2278 - accuracy: 0.5723 Epoch 6/100 1/1 [==============================] - 0s 4ms/step - loss: 28.9657 - accuracy: 0.6442 Epoch 7/100 1/1 [==============================] - 0s 3ms/step - loss: 16.5118 - accuracy: 0.8061 Epoch 8/100 1/1 [==============================] - 0s 3ms/step - loss: 9.4905 - accuracy: 0.8947 Epoch 9/100 1/1 [==============================] - 0s 3ms/step - loss: 6.2025 - accuracy: 0.9355 Epoch 10/100 1/1 [==============================] - 0s 5ms/step - loss: 5.2417 - accuracy: 0.9404 Epoch 11/100 1/1 [==============================] - 0s 3ms/step - loss: 5.5173 - accuracy: 0.9270 Epoch 12/100 1/1 [==============================] - 0s 3ms/step - loss: 6.5902 - accuracy: 0.9021 Epoch 13/100 1/1 [==============================] - 0s 3ms/step - loss: 7.8607 - accuracy: 0.8788 Epoch 14/100 1/1 [==============================] - 0s 3ms/step - loss: 8.5884 - accuracy: 0.8647 Epoch 15/100 1/1 [==============================] - 0s 3ms/step - loss: 8.3990 - accuracy: 0.8664 Epoch 16/100 1/1 [==============================] - 0s 3ms/step - loss: 7.4674 - accuracy: 0.8793 Epoch 17/100 1/1 [==============================] - 0s 3ms/step - loss: 6.2580 - accuracy: 0.8982 Epoch 18/100 1/1 [==============================] - 0s 3ms/step - loss: 5.1273 - accuracy: 0.9156 Epoch 19/100 1/1 [==============================] - 0s 3ms/step - loss: 4.2381 - accuracy: 0.9302 Epoch 20/100 1/1 [==============================] - 0s 3ms/step - loss: 3.6032 - accuracy: 0.9426 Epoch 21/100 1/1 [==============================] - 0s 3ms/step - loss: 3.1860 - accuracy: 0.9509 Epoch 22/100 1/1 [==============================] - 0s 3ms/step - loss: 2.9232 - accuracy: 0.9551 Epoch 23/100 1/1 [==============================] - 0s 4ms/step - loss: 2.7559 - accuracy: 0.9574 Epoch 24/100 1/1 [==============================] - 0s 3ms/step - loss: 2.6420 - accuracy: 0.9594 Epoch 25/100 1/1 [==============================] - 0s 3ms/step - loss: 2.5490 - accuracy: 0.9599 Epoch 26/100 1/1 [==============================] - 0s 3ms/step - loss: 2.4612 - accuracy: 0.9603 Epoch 27/100 1/1 [==============================] - 0s 3ms/step - loss: 2.3617 - accuracy: 0.9608 Epoch 28/100 1/1 [==============================] - 0s 3ms/step - loss: 2.2378 - accuracy: 0.9612 Epoch 29/100 1/1 [==============================] - 0s 3ms/step - loss: 2.0873 - accuracy: 0.9619 Epoch 30/100 1/1 [==============================] - 0s 3ms/step - loss: 1.9117 - accuracy: 0.9630 Epoch 31/100 1/1 [==============================] - 0s 3ms/step - loss: 1.7239 - accuracy: 0.9641 Epoch 32/100 1/1 [==============================] - 0s 3ms/step - loss: 1.5408 - accuracy: 0.9657 Epoch 33/100 1/1 [==============================] - 0s 3ms/step - loss: 1.3663 - accuracy: 0.9670 Epoch 34/100 1/1 [==============================] - 0s 3ms/step - loss: 1.2210 - accuracy: 0.9685 Epoch 35/100 1/1 [==============================] - 0s 3ms/step - loss: 1.1035 - accuracy: 0.9688 Epoch 36/100 1/1 [==============================] - 0s 4ms/step - loss: 1.0208 - accuracy: 0.9696 Epoch 37/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9766 - accuracy: 0.9705 Epoch 38/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9628 - accuracy: 0.9708 Epoch 39/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9717 - accuracy: 0.9715 Epoch 40/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9883 - accuracy: 0.9706 Epoch 41/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0039 - accuracy: 0.9699 Epoch 42/100 1/1 [==============================] - 0s 3ms/step - loss: 1.0156 - accuracy: 0.9685 Epoch 43/100 1/1 [==============================] - 0s 4ms/step - loss: 1.0181 - accuracy: 0.9681 Epoch 44/100 1/1 [==============================] - 0s 4ms/step - loss: 1.0067 - accuracy: 0.9686 Epoch 45/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9808 - accuracy: 0.9693 Epoch 46/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9443 - accuracy: 0.9703 Epoch 47/100 1/1 [==============================] - 0s 3ms/step - loss: 0.9019 - accuracy: 0.9711 Epoch 48/100 1/1 [==============================] - 0s 3ms/step - loss: 0.8571 - accuracy: 0.9722 Epoch 49/100 1/1 [==============================] - 0s 3ms/step - loss: 0.8146 - accuracy: 0.9737 Epoch 50/100 1/1 [==============================] - 0s 4ms/step - loss: 0.7768 - accuracy: 0.9743 Epoch 51/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7489 - accuracy: 0.9753 Epoch 52/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7294 - accuracy: 0.9759 Epoch 53/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7186 - accuracy: 0.9767 Epoch 54/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7124 - accuracy: 0.9774 Epoch 55/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7080 - accuracy: 0.9776 Epoch 56/100 1/1 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.9777 Epoch 57/100 1/1 [==============================] - 0s 4ms/step - loss: 0.7002 - accuracy: 0.9776 Epoch 58/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.9778 Epoch 59/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.9779 Epoch 60/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6806 - accuracy: 0.9784 Epoch 61/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6715 - accuracy: 0.9786 Epoch 62/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6615 - accuracy: 0.9786 Epoch 63/100 1/1 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.9784 Epoch 64/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.9786 Epoch 65/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6302 - accuracy: 0.9787 Epoch 66/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.9791 Epoch 67/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6127 - accuracy: 0.9787 Epoch 68/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.9791 Epoch 69/100 1/1 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.9792 Epoch 70/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.9795 Epoch 71/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.9793 Epoch 72/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5888 - accuracy: 0.9791 Epoch 73/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.9790 Epoch 74/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.9793 Epoch 75/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.9794 Epoch 76/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.9795 Epoch 77/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.9794 Epoch 78/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.9794 Epoch 79/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5595 - accuracy: 0.9796 Epoch 80/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.9796 Epoch 81/100 1/1 [==============================] - 0s 5ms/step - loss: 0.5501 - accuracy: 0.9798 Epoch 82/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5457 - accuracy: 0.9800 Epoch 83/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.9800 Epoch 84/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5379 - accuracy: 0.9804 Epoch 85/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.9807 Epoch 86/100 1/1 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.9807 Epoch 87/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.9807 Epoch 88/100 1/1 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.9808 Epoch 89/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5208 - accuracy: 0.9808 Epoch 90/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.9811 Epoch 91/100 1/1 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.9812 Epoch 92/100 1/1 [==============================] - 0s 5ms/step - loss: 0.5107 - accuracy: 0.9812 Epoch 93/100 1/1 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.9813 Epoch 94/100 1/1 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.9814 Epoch 95/100 1/1 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.9816 Epoch 96/100 1/1 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.9815 Epoch 97/100 1/1 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.9815 Epoch 98/100 1/1 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.9812 Epoch 99/100 1/1 [==============================] - 0s 4ms/step - loss: 0.4909 - accuracy: 0.9812 Epoch 100/100 1/1 [==============================] - 0s 3ms/step - loss: 0.4885 - accuracy: 0.9812 . &lt;keras.callbacks.History at 0x7fe5401ee830&gt; . net.evaluate(X,y) . 375/375 [==============================] - 0s 862us/step - loss: 0.4860 - accuracy: 0.9812 . [0.4859827756881714, 0.981249988079071] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.9770 . [0.42935940623283386, 0.9769999980926514] . &#54400;&#51060;4: &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277; &#51060;&#50857; // epochs=10 . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=10,batch_size=120) . Epoch 1/10 100/100 [==============================] - 0s 1ms/step - loss: 5.6482 - accuracy: 0.9418 Epoch 2/10 100/100 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.9792 Epoch 3/10 100/100 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.9820 Epoch 4/10 100/100 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.9829 Epoch 5/10 100/100 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9855 Epoch 6/10 100/100 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9870 Epoch 7/10 100/100 [==============================] - 0s 2ms/step - loss: 0.1767 - accuracy: 0.9893 Epoch 8/10 100/100 [==============================] - 0s 2ms/step - loss: 0.1602 - accuracy: 0.9884 Epoch 9/10 100/100 [==============================] - 0s 2ms/step - loss: 0.1798 - accuracy: 0.9874 Epoch 10/10 100/100 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9906 . &lt;keras.callbacks.History at 0x7fe5246999c0&gt; . net.evaluate(X,y) . 375/375 [==============================] - 0s 789us/step - loss: 0.0984 - accuracy: 0.9911 . [0.09842231124639511, 0.9910833239555359] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9845 . [0.2526112496852875, 0.984499990940094] . &#44284;&#51228; . tf.random.set_seed(43055) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=10,batch_size=240) . Epoch 1/10 50/50 [==============================] - 0s 1ms/step - loss: 6.3153 - accuracy: 0.9025 Epoch 2/10 50/50 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.9726 Epoch 3/10 50/50 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.9791 Epoch 4/10 50/50 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9822 Epoch 5/10 50/50 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9860 Epoch 6/10 50/50 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9863 Epoch 7/10 50/50 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9877 Epoch 8/10 50/50 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9893 Epoch 9/10 50/50 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9905 Epoch 10/10 50/50 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9913 . &lt;keras.callbacks.History at 0x7fe524564e20&gt; . net.evaluate(X,y) . 375/375 [==============================] - 0s 831us/step - loss: 0.0592 - accuracy: 0.9898 . [0.05924727022647858, 0.9898333549499512] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9805 . [0.18036103248596191, 0.9804999828338623] .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-2%EC%9D%BC)(2)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-2%EC%9D%BC)(2)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "9주차-5월 02일 (1)",
            "content": "imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . &#50864;&#46020;&#54632;&#49688;&#50752; &#52572;&#45824;&#50864;&#46020;&#52628;&#51221;&#47049; . 예제 . $X_i overset{iid}{ sim} Ber(p)$에서 얻은 샘플이 아래와 같다고 하자. . x=[0,1,0,1] x . [0, 1, 0, 1] . $p$는 얼마라고 볼 수 있는가? --&gt; 0.5 . 왜?? $p$가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가? . - suppose: $p=0.1$ 이라고 하자. . 그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. . 0.9 * 0.1 * 0.9 * 0.1 . 0.008100000000000001 . - suppose: $p=0.2$ 이라고 하자. . 그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. . 0.8 * 0.2 * 0.8 * 0.2 . 0.025600000000000008 . - 질문1: $p=0.1$인것 같냐? 아니면 $p=0.2$인것 같냐? -&gt; $p=0.2$ . 왜?? $p=0.2$일 확률이 더 크다! | . (여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다. . - 0.0256은 &quot;$p=0.2$일 경우 샘플 (0,1,0,1)이 얻어질 확률&quot;이지 &quot;$p=0.2$일 확률&quot;은 아니다. . &quot;$p=0.2$인 확률&quot; 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다) . _plist = np.linspace(0.499,0.501,1000) sum([(1-p)*p*(1-p)*p for p in _plist]) . 62.49983299986714 . - 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -&gt; 가능도라는 말을 쓰자. . 0.0256 $=$ $p$가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 $=$ $p$가 0.2일 가능도 | . - 다시 질문1로 돌아가자! . 질문1: $p=0.1$인 것 같냐? 아니면 $p=0.2$인 것 같냐? -&gt; 답 $p=0.2$ -&gt; 왜? $p=0.2$인 가능도가 더 크니까! | 질문2: $p=0.2$인 것 같냐? 아니면 $p=0.3$인 것 같냐? -&gt; 답 $p=0.3$ -&gt; 왜? $p=0.3$인 가능도가 더 크니까! | 질문3: ... | . - 궁극의 질문: $p$가 뭐일 것 같아? . $p$가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. | 그 함수를 최대화하는 $p$를 찾자. | 그 $p$가 궁극의 질문에 대한 대답이 된다. | . - 잠깐 용어정리 . 가능도함수 $=$ 우도함수 $=$ likelihood function $:=$ $L(p)$ | $p$의 maximum likelihood estimator $=$ p의 MLE $:=$ $ hat{p}^{mle}$ $=$ $ text{argmax}_p L(p)$ $=$ $ hat{p}$ | . (예제의 풀이) . - 이 예제의 경우 가능도함수를 정의하자. . $L(p)$: $p$의 가능도함수 = $p$가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = $p$가 모수일때 $x_1$이 0일 확률 $ times dots times$ $p$가 모수일때 $x_4$가 1일 확률 | $L(p)= prod_{i=1}^{4} f(x_i;p)= prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}$ | . . Note: 참고로 이 과정을 일반화 하면 $X_1, dots,X_n overset{iid}{ sim} Ber(p)$ 일때 $p$의 likelihood function은 $ prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$ 라고 볼 수 있다. . . Note: 더 일반화: $x_1, dots,x_n$이 pdf가 $f(x)$인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 $ prod_{i=1}^{n}f(x_i)$라고 볼 수 있다. . - 이 예제의 경우 $p$의 최대우도추정량을 구하면 . $$ hat{p}^{mle} = text{argmax}_p L(p) = text{argmax}_p big { p^2(1-p)^2 big }= frac{1}{2}$$ . &#51473;&#44036;&#44256;&#49324; 1&#48264; . (1) $N( mu, sigma)$에서 얻은 샘플이 아래와 같다고 할때 $ mu, sigma$의 MLE를 구하여라. . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . (2) $Ber(p)$에서 얻은 샘플이 아래와 같다고 할 때 $p$의 MLE를 구하여라. . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt; . (3) $y_i = beta_0 + beta_1 x_i + epsilon_i$, $ epsilon_i overset{iid}{ sim} N(0,1)$ 일때 $( beta_0, beta_1)$의 MLE를 구하여라. (회귀모형) . (풀이) 가능도함수 . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . 를 최대화하는 $ beta_0, beta_1$을 구하면된다. 그런데 이것은 아래를 최소화하는 $ beta_0, beta_1$을 구하는 것과 같다. . $$- log L( beta_0, beta_1) = sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$$ . 위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $ beta_0, beta_1$을 구하면 된다. . 중간고사 1-(3)의 다른 풀이 . step1: 생성 . x= tf.constant(np.arange(1,10001)/10000) y= tnp.random.randn(10000) + (0.5 + 2*x) . step2: minimize MSEloss (원래는 maximize log-likelihood) . maximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 $ beta_0, beta_1$은 MSE를 최소화하는 $ beta_0, beta_1$과 동일하므로 | . beta0= tf.Variable(1.0) beta1= tf.Variable(1.0) for i in range(2000): with tf.GradientTape() as tape: #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2) loss = tf.reduce_sum((y-beta0-beta1*x)**2) slope1, slope2 = tape.gradient(loss,[beta0,beta1]) beta0.assign_sub(slope1* 0.1/10000) # N=10000 beta1.assign_sub(slope2* 0.1/10000) . beta0,beta1 . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.47993016&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.020918&gt;) . - 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다? . 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함 | . (4) 출제하지 못한 중간고사 문제 . 아래의 모형을 생각하자. . $Y_i overset{iid}{ sim} Ber( pi_i)$ | $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}= frac{ exp(-1+5x_i)}{1+ exp(-1+5x_i)}$ | . 아래는 위의 모형에서 얻은 샘플이다. . x = tnp.linspace(-1,1,2000) pi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x)) y = np.random.binomial(1,pi) y = tf.constant(y) . 함수 $L(w_0,w_1)$을 최대화하는 $(w_0,w_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $(w_0,w_1)$의 초기값은 모두 0.1로 설정할 것) . $$L(w_0,w_1)= prod_{i=1}^{n}f(y_i), quad f(x_i)={ pi_i}^{y_i}(1- pi_i)^{1-y_i}, quad pi_i= text{sigmoid}(w_0+w_1x_i)$$ . (풀이1) . w0hat = tf.Variable(1.0) w1hat = tf.Variable(1.0) . for i in range(1000): with tf.GradientTape() as tape: pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x)) pdf = pihat**y * (1-pihat)**(1-y) logL = tf.reduce_mean(tnp.log(pdf)) slope1,slope2 = tape.gradient(logL,[w0hat,w1hat]) w0hat.assign_add(slope1*0.1) w1hat.assign_add(slope2*0.1) . w0hat,w1hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-0.88931483&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=4.231925&gt;) . (해석) - 로지스틱에서 가능도함수와 BCEloss의 관계 . $L(w_0,w_1)$를 최대화하는 $w_0,w_1$은 아래를 최소화하는 $w_0,w_1$와 같다. . $$- log L(w_0,w_1) = - sum_{i=1}^{n} big(y_i log( pi_i) + (1-y_i) log(1- pi_i) big)$$ . 이것은 최적의 $w_0,w_1$을 $ hat{w}_0, hat{w}_1$이라고 하면 $ hat{ pi}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= hat{y}_i$이 되고 따라서 위의 식은 $n times$BCEloss의 형태임을 쉽게 알 수 있다. . 결국 로지스틱 모형에서 $(w_0,w_1)$의 MLE를 구하기 위해서는 BCEloss를 최소화하는 $(w_0,w_1)$을 구하면 된다! . (풀이2) . w0hat = tf.Variable(1.0) w1hat = tf.Variable(1.0) . for i in range(1000): with tf.GradientTape() as tape: yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x)) loss = tf.losses.binary_crossentropy(y,yhat) slope1,slope2 = tape.gradient(loss,[w0hat,w1hat]) w0hat.assign_sub(slope1*0.1) w1hat.assign_sub(slope2*0.1) . w0hat,w1hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-0.88931495&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=4.2319255&gt;) . &#49552;&#49892;&#54632;&#49688;&#51032; &#49444;&#44228; (&#49440;&#53469;) . - 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. . 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 | 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다 | . - minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. &lt; 참고만하세요, 이 수업에서는 안중요합니다. . 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 $ hat{ beta}$은 여전히 MSEloss를 최소화하는 $ beta$를 구함으로써 얻을 수 있다. | 이 경우 MSEloss를 쓰는 이론적근거? $ hat{ beta}$이 BLUE가 되기 때문임 (가우스-마코프정리) | .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-2%EC%9D%BC)(1)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-2%EC%9D%BC)(1)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "12주차-5월 23일",
            "content": "imports . import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt import numpy as np . CNN . CONV&#51032; &#50669;&#54624; . - 데이터생성 (그냥 흑백대비 데이터) . _X1 = tnp.ones([50,25])*10 _X1 . &lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy= array([[10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], ..., [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.]])&gt; . _X2 = tnp.zeros([50,25])*10 _X2 . &lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]])&gt; . tf.concat([_X1,_X2],axis=1) . &lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy= array([[10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], ..., [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.]])&gt; . _noise = tnp.random.randn(50*50).reshape(50,50) _noise . &lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy= array([[-0.80634969, -0.66826189, 0.36372325, ..., -1.06445681, -0.83282315, -0.35537343], [-0.67039156, -1.80738609, -0.24149345, ..., -1.62882575, 1.00820182, 0.84525625], [ 1.60575991, 1.66966338, 0.04901293, ..., 1.79991319, 0.01740724, -1.19340719], ..., [-0.48333526, -0.69582694, -0.69678897, ..., -1.70067824, 0.67909495, -3.36243945], [-1.43498341, 0.38814103, 0.82544769, ..., -1.79549964, 0.25662507, 0.00873188], [-0.31509164, -1.61303034, -0.73647243, ..., 0.47527916, -0.55977842, -1.09083859]])&gt; . XXX = tf.concat([_X1,_X2],axis=1) + _noise . XXX=XXX.reshape(1,50,50,1) . plt.imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f176f954370&gt; . - conv layer 생성 . conv = tf.keras.layers.Conv2D(2,(2,2)) . conv.weights # 처음에는 가중치가 없음 . [] . conv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴 conv.weights # 이제 가중치가 생김 . [&lt;tf.Variable &#39;conv2d/kernel:0&#39; shape=(2, 2, 1, 2) dtype=float32, numpy= array([[[[ 0.29928392, -0.4381207 ]], [[ 0.3800245 , -0.4090048 ]]], [[[ 0.01464921, -0.44651937]], [[-0.12845653, 0.6931189 ]]]], dtype=float32)&gt;, &lt;tf.Variable &#39;conv2d/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;] . - 가중치의 값을 확인해보자. . conv.weights[0] # kernel에 해당하는것 . &lt;tf.Variable &#39;conv2d/kernel:0&#39; shape=(2, 2, 1, 2) dtype=float32, numpy= array([[[[ 0.29928392, -0.4381207 ]], [[ 0.3800245 , -0.4090048 ]]], [[[ 0.01464921, -0.44651937]], [[-0.12845653, 0.6931189 ]]]], dtype=float32)&gt; . conv.weights[1] # bias에 해당하는것 . &lt;tf.Variable &#39;conv2d/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt; . - 필터값을 원하는 것으로 변경해보자. . w0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다. w1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출) . w=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1) w . array([[[[ 0.25, -1. ]], [[ 0.25, 1. ]]], [[[ 0.25, -1. ]], [[ 0.25, 1. ]]]]) . b= np.array([0.0,0.0]) b . array([0., 0.]) . conv.set_weights([w,b]) conv.get_weights() . [array([[[[ 0.25, -1. ]], [[ 0.25, 1. ]]], [[[ 0.25, -1. ]], [[ 0.25, 1. ]]]], dtype=float32), array([0., 0.], dtype=float32)] . 첫번째는 평균을 구하는 필터, | 두번째는 엣지를 검출하는 필터 | . - 필터를 넣은 결과를 확인 . XXX0=conv(XXX)[...,0] # 채널0 XXX0 . &lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy= array([[[ 9.011903 , 9.411646 , 9.61511 , ..., -1.1405526 , -0.629476 , 0.16631536], [10.199411 , 9.91745 , 10.020853 , ..., 0.14333892, 0.29917413, 0.16936457], [10.985146 , 10.166304 , 9.914722 , ..., 0.93729585, 0.59122133, -0.19406855], ..., [ 9.74448 , 10.227768 , 10.116489 , ..., -0.66212255, -0.42774785, -0.65288776], [ 9.4435 , 9.955243 , 9.940023 , ..., -0.89556384, -0.6401144 , -0.60449684], [ 9.256259 , 9.716022 , 9.764608 , ..., -0.6431877 , -0.4058435 , -0.34631503]]], dtype=float32)&gt; . XXX1=conv(XXX)[...,1] # 채널1 XXX1 . &lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy= array([[[-0.9989071 , 2.5978775 , -1.7840195 , ..., -0.82435447, 2.8686614 , 0.31450415], [-1.0730915 , -0.05475712, 0.46837234, ..., -0.2311809 , 0.8545218 , -1.37376 ], [-2.7608395 , -0.5145273 , -0.4917965 , ..., 0.7865187 , -2.1708167 , -0.9703431 ], ..., [ 3.459155 , -1.526001 , 1.0808859 , ..., -0.91478837, 1.8522873 , -2.752847 ], [ 1.6106329 , 0.4363451 , -0.49722385, ..., -3.4101005 , 4.431898 , -4.2894278 ], [ 0.5251856 , 1.3138657 , -1.1195154 , ..., -0.06769031, 1.0170672 , -0.7789533 ]]], dtype=float32)&gt; . - 각 채널을 시각화 . fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) . ax1.imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f17004acb20&gt; . ax3.imshow(XXX0.reshape(49,49),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f17003bb460&gt; . ax4.imshow(XXX1.reshape(49,49),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f17003db070&gt; . fig . 2사분면: 원래이미지 | 3사분면: 원래이미지 -&gt; 평균을 의미하는 conv적용 | 4사분면: 원래이미지 -&gt; 엣지를 검출하는 conv적용 | . - conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자 . conv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 평균필터 conv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 엣지필터 conv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 평균필터 conv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 엣지필터 . &lt;tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy= array([[[ 4.615119 , -3.8587675 , 3.5757427 , ..., -0.74950653, 4.7787185 , -4.782439 ], [ 3.2646465 , 0.5458603 , 2.8233604 , ..., -2.038361 , -1.8716327 , -1.0278082 ], [ 7.5155373 , -0.49682903, 4.5650854 , ..., -0.6898642 , -5.1890845 , 4.030332 ], ..., [-11.65044 , 5.765974 , -1.2397356 , ..., -1.8393549 , 3.7282696 , -5.562973 ], [ -6.159444 , 1.6733179 , -4.236967 , ..., -3.1072135 , 10.609074 , -13.32646 ], [ -0.38560772, -3.36695 , -0.7375593 , ..., 0.02712733, 8.926756 , -10.517346 ]]], dtype=float32)&gt; . fig,ax =plt.subplots(3,4) . ax[0][0].imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) # 원래이미지 . &lt;matplotlib.image.AxesImage at 0x7f17000f1b40&gt; . ax[1][0].imshow(XXX0.reshape(49,49),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 평균필터 ax[1][2].imshow(XXX1.reshape(49,49),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 엣지필터 . &lt;matplotlib.image.AxesImage at 0x7f17003bbac0&gt; . ax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 평균필터 ax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 엣지필터 ax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 평균필터 ax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 엣지필터 . &lt;matplotlib.image.AxesImage at 0x7f16e8722470&gt; . fig.set_figheight(8) fig.set_figwidth(16) fig.tight_layout() fig . - 요약 . conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. | 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. | 채널수를 많이 만들고 다양한 웨이트조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수도 있을 것이다? | 컨볼루션 레이어의 역할 = 이미지의 특징을 추출하는 역할 | . - 참고: 스트라이드, 패딩 . 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 | 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다. | . MAXPOOL . - 기본적역할: 이미지의 크기를 줄이는 것 . 이미지의의 크기를 줄여야하는 이유? 어차피 최종적으로 10차원으로 줄어야하므로 | 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~ | . - 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌) . - 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. . 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀) | . CNN &#50500;&#53412;&#53581;&#52376;&#51032; &#54364;&#54788;&#48169;&#48277; . - 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음 . . - 물론 아래와 같이 그리는 경우도 있음 . . Discusstion about CNN . - 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. . 시계열 (1차원격자), 이미지 (2차원격자) | . - 실제응용에서 엄청난 성공을 거두었다. . - 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 . 컨볼루션은 조금 특별한 선형변환이다. | . - 신경과학의 원리가 심층학습에 영향을 미친 사례이다. . CNN&#51032; &#47784;&#54000;&#48652; . - 희소성 + 매개변수의 공유 . 다소 철학적인 모티브임 | 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미 | 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다. | . - 매개변수 공유효과로 인해서 파라메터가 확 줄어든다. . (예시) (1,6,6,1) -&gt; (1,5,5,2) . MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 =&gt; 1800개의 매개변수 필요 | CNN은 8개의 매개변수 필요 | . CNN &#49888;&#44221;&#47581;&#51032; &#44592;&#48376;&#44396;&#51312; . - 기본유닛 . conv - activation - pooling | conv - conv - activation - pooling | . &#47784;&#54805;&#51032; &#49457;&#45733;&#51012; &#50732;&#47532;&#44592; &#50948;&#54620; &#45432;&#47141;&#46308; . dropout . - 아래의 예제를 복습하자. . np.random.seed(43052) x = np.linspace(0,1,100).reshape(100,1) y = np.random.normal(loc=0,scale=0.01,size=(100,1)) plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7f16e8670610&gt;] . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2048,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(x,y,epochs=5000,verbose=0,batch_size=100) . &lt;keras.callbacks.History at 0x7f16e8548f40&gt; . plt.plot(x,y) plt.plot(x,net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f16e83a4bb0&gt;] . - train/test로 나누어서 생각해보자. . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2048,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80) . &lt;keras.callbacks.History at 0x7f16e83d93f0&gt; . plt.plot(x,y) plt.plot(x[:80],net(x[:80]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f16e8673910&gt;] . plt.plot(x,y) plt.plot(x[:80],net(x[:80]),&#39;--&#39;) plt.plot(x[80:],net(x[80:]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f16e81481c0&gt;] . train에서 추세를 따라가는게 좋은게 아니다 $ to$ 그냥 직선으로 핏하는거 이외에는 다 오버핏이다. | . - 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 $ to$ 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?) . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2048,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dropout(0.8)) net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80) . &lt;keras.callbacks.History at 0x7f16e81a1630&gt; . plt.plot(x,y) plt.plot(x[:80],net(x[:80]),&#39;--&#39;) plt.plot(x[80:],net(x[80:]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f16e81b2e00&gt;] . - 드랍아웃에 대한 summary . 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) | note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. | note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임) | . train / val / test . - data . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -&gt; 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜 y = tf.keras.utils.to_categorical(y_train) XX = x_test.reshape(-1,28,28,1)/255 yy = tf.keras.utils.to_categorical(y_test) . net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . cb1 = tf.keras.callbacks.TensorBoard() net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 2ms/step - loss: 0.7013 - accuracy: 0.7666 - val_loss: 0.4976 - val_accuracy: 0.8320 Epoch 2/200 240/240 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.8400 - val_loss: 0.4822 - val_accuracy: 0.8320 Epoch 3/200 240/240 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.8518 - val_loss: 0.4339 - val_accuracy: 0.8535 Epoch 4/200 240/240 [==============================] - 0s 1ms/step - loss: 0.4061 - accuracy: 0.8592 - val_loss: 0.4077 - val_accuracy: 0.8568 Epoch 5/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3851 - accuracy: 0.8661 - val_loss: 0.3948 - val_accuracy: 0.8619 Epoch 6/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8699 - val_loss: 0.3900 - val_accuracy: 0.8617 Epoch 7/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8746 - val_loss: 0.3846 - val_accuracy: 0.8678 Epoch 8/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8768 - val_loss: 0.3684 - val_accuracy: 0.8716 Epoch 9/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8788 - val_loss: 0.3677 - val_accuracy: 0.8711 Epoch 10/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3305 - accuracy: 0.8805 - val_loss: 0.3617 - val_accuracy: 0.8725 Epoch 11/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8845 - val_loss: 0.3612 - val_accuracy: 0.8713 Epoch 12/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8859 - val_loss: 0.3578 - val_accuracy: 0.8726 Epoch 13/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8888 - val_loss: 0.3571 - val_accuracy: 0.8729 Epoch 14/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3024 - accuracy: 0.8913 - val_loss: 0.3555 - val_accuracy: 0.8733 Epoch 15/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8938 - val_loss: 0.3510 - val_accuracy: 0.8769 Epoch 16/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2954 - accuracy: 0.8937 - val_loss: 0.3741 - val_accuracy: 0.8648 Epoch 17/200 240/240 [==============================] - 1s 2ms/step - loss: 0.2909 - accuracy: 0.8945 - val_loss: 0.3422 - val_accuracy: 0.8788 Epoch 18/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8964 - val_loss: 0.3484 - val_accuracy: 0.8767 Epoch 19/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.8978 - val_loss: 0.3474 - val_accuracy: 0.8790 Epoch 20/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.9001 - val_loss: 0.3390 - val_accuracy: 0.8808 Epoch 21/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.9012 - val_loss: 0.3355 - val_accuracy: 0.8838 Epoch 22/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.9039 - val_loss: 0.3324 - val_accuracy: 0.8822 Epoch 23/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9054 - val_loss: 0.3384 - val_accuracy: 0.8768 Epoch 24/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9074 - val_loss: 0.3435 - val_accuracy: 0.8792 Epoch 25/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2537 - accuracy: 0.9082 - val_loss: 0.3315 - val_accuracy: 0.8828 Epoch 26/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.9092 - val_loss: 0.3371 - val_accuracy: 0.8830 Epoch 27/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.9110 - val_loss: 0.3368 - val_accuracy: 0.8816 Epoch 28/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9125 - val_loss: 0.3389 - val_accuracy: 0.8810 Epoch 29/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9125 - val_loss: 0.3354 - val_accuracy: 0.8831 Epoch 30/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.9132 - val_loss: 0.3364 - val_accuracy: 0.8798 Epoch 31/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.9149 - val_loss: 0.3382 - val_accuracy: 0.8808 Epoch 32/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2354 - accuracy: 0.9149 - val_loss: 0.3380 - val_accuracy: 0.8852 Epoch 33/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.9167 - val_loss: 0.3352 - val_accuracy: 0.8846 Epoch 34/200 240/240 [==============================] - 1s 2ms/step - loss: 0.2289 - accuracy: 0.9177 - val_loss: 0.3494 - val_accuracy: 0.8803 Epoch 35/200 240/240 [==============================] - 1s 2ms/step - loss: 0.2276 - accuracy: 0.9181 - val_loss: 0.3367 - val_accuracy: 0.8817 Epoch 36/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9199 - val_loss: 0.3390 - val_accuracy: 0.8852 Epoch 37/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9183 - val_loss: 0.3443 - val_accuracy: 0.8813 Epoch 38/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9193 - val_loss: 0.3374 - val_accuracy: 0.8821 Epoch 39/200 240/240 [==============================] - 1s 2ms/step - loss: 0.2191 - accuracy: 0.9214 - val_loss: 0.3294 - val_accuracy: 0.8863 Epoch 40/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9231 - val_loss: 0.3331 - val_accuracy: 0.8848 Epoch 41/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9243 - val_loss: 0.3388 - val_accuracy: 0.8832 Epoch 42/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2147 - accuracy: 0.9238 - val_loss: 0.3339 - val_accuracy: 0.8869 Epoch 43/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9245 - val_loss: 0.3365 - val_accuracy: 0.8857 Epoch 44/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9256 - val_loss: 0.3385 - val_accuracy: 0.8845 Epoch 45/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9263 - val_loss: 0.3337 - val_accuracy: 0.8861 Epoch 46/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9267 - val_loss: 0.3548 - val_accuracy: 0.8803 Epoch 47/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9264 - val_loss: 0.3489 - val_accuracy: 0.8833 Epoch 48/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1979 - accuracy: 0.9299 - val_loss: 0.3416 - val_accuracy: 0.8853 Epoch 49/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9283 - val_loss: 0.3575 - val_accuracy: 0.8811 Epoch 50/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9297 - val_loss: 0.3463 - val_accuracy: 0.8832 Epoch 51/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9285 - val_loss: 0.3497 - val_accuracy: 0.8848 Epoch 52/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1928 - accuracy: 0.9302 - val_loss: 0.3626 - val_accuracy: 0.8795 Epoch 53/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9309 - val_loss: 0.3577 - val_accuracy: 0.8817 Epoch 54/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9303 - val_loss: 0.3524 - val_accuracy: 0.8827 Epoch 55/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9303 - val_loss: 0.3657 - val_accuracy: 0.8795 Epoch 56/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1880 - accuracy: 0.9331 - val_loss: 0.3655 - val_accuracy: 0.8808 Epoch 57/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9329 - val_loss: 0.3522 - val_accuracy: 0.8840 Epoch 58/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9350 - val_loss: 0.3581 - val_accuracy: 0.8823 Epoch 59/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9337 - val_loss: 0.3606 - val_accuracy: 0.8824 Epoch 60/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9364 - val_loss: 0.3683 - val_accuracy: 0.8813 Epoch 61/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1772 - accuracy: 0.9368 - val_loss: 0.3619 - val_accuracy: 0.8826 Epoch 62/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9365 - val_loss: 0.3654 - val_accuracy: 0.8830 Epoch 63/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9363 - val_loss: 0.3653 - val_accuracy: 0.8820 Epoch 64/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9368 - val_loss: 0.3690 - val_accuracy: 0.8829 Epoch 65/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9381 - val_loss: 0.3756 - val_accuracy: 0.8848 Epoch 66/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9369 - val_loss: 0.3704 - val_accuracy: 0.8834 Epoch 67/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9389 - val_loss: 0.3724 - val_accuracy: 0.8834 Epoch 68/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9390 - val_loss: 0.3799 - val_accuracy: 0.8830 Epoch 69/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9402 - val_loss: 0.3716 - val_accuracy: 0.8842 Epoch 70/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9401 - val_loss: 0.3793 - val_accuracy: 0.8823 Epoch 71/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9400 - val_loss: 0.3847 - val_accuracy: 0.8815 Epoch 72/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 0.9399 - val_loss: 0.3821 - val_accuracy: 0.8825 Epoch 73/200 240/240 [==============================] - 1s 2ms/step - loss: 0.1605 - accuracy: 0.9432 - val_loss: 0.3840 - val_accuracy: 0.8823 Epoch 74/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9412 - val_loss: 0.3879 - val_accuracy: 0.8813 Epoch 75/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1578 - accuracy: 0.9438 - val_loss: 0.3829 - val_accuracy: 0.8827 Epoch 76/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1595 - accuracy: 0.9430 - val_loss: 0.4191 - val_accuracy: 0.8742 Epoch 77/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9424 - val_loss: 0.3865 - val_accuracy: 0.8823 Epoch 78/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9421 - val_loss: 0.4061 - val_accuracy: 0.8779 Epoch 79/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9438 - val_loss: 0.3882 - val_accuracy: 0.8830 Epoch 80/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1559 - accuracy: 0.9445 - val_loss: 0.4050 - val_accuracy: 0.8792 Epoch 81/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1537 - accuracy: 0.9455 - val_loss: 0.3947 - val_accuracy: 0.8814 Epoch 82/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 0.9464 - val_loss: 0.4008 - val_accuracy: 0.8798 Epoch 83/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9461 - val_loss: 0.4187 - val_accuracy: 0.8779 Epoch 84/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9465 - val_loss: 0.3945 - val_accuracy: 0.8823 Epoch 85/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9469 - val_loss: 0.4101 - val_accuracy: 0.8804 Epoch 86/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9452 - val_loss: 0.4034 - val_accuracy: 0.8794 Epoch 87/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9491 - val_loss: 0.4268 - val_accuracy: 0.8758 Epoch 88/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9479 - val_loss: 0.4116 - val_accuracy: 0.8798 Epoch 89/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9478 - val_loss: 0.4097 - val_accuracy: 0.8811 Epoch 90/200 240/240 [==============================] - 1s 2ms/step - loss: 0.1459 - accuracy: 0.9480 - val_loss: 0.4089 - val_accuracy: 0.8833 Epoch 91/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1431 - accuracy: 0.9487 - val_loss: 0.4111 - val_accuracy: 0.8811 Epoch 92/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9471 - val_loss: 0.4172 - val_accuracy: 0.8801 Epoch 93/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9493 - val_loss: 0.4150 - val_accuracy: 0.8811 Epoch 94/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9489 - val_loss: 0.4193 - val_accuracy: 0.8824 Epoch 95/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9506 - val_loss: 0.4204 - val_accuracy: 0.8813 Epoch 96/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9503 - val_loss: 0.4208 - val_accuracy: 0.8823 Epoch 97/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9514 - val_loss: 0.4217 - val_accuracy: 0.8820 Epoch 98/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9522 - val_loss: 0.4241 - val_accuracy: 0.8784 Epoch 99/200 240/240 [==============================] - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9522 - val_loss: 0.4335 - val_accuracy: 0.8778 Epoch 100/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9513 - val_loss: 0.4289 - val_accuracy: 0.8807 Epoch 101/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9534 - val_loss: 0.4396 - val_accuracy: 0.8788 Epoch 102/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9528 - val_loss: 0.4436 - val_accuracy: 0.8763 Epoch 103/200 240/240 [==============================] - 1s 2ms/step - loss: 0.1315 - accuracy: 0.9534 - val_loss: 0.4398 - val_accuracy: 0.8780 Epoch 104/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1327 - accuracy: 0.9529 - val_loss: 0.4395 - val_accuracy: 0.8787 Epoch 105/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9535 - val_loss: 0.4600 - val_accuracy: 0.8743 Epoch 106/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9546 - val_loss: 0.4430 - val_accuracy: 0.8801 Epoch 107/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9545 - val_loss: 0.4417 - val_accuracy: 0.8785 Epoch 108/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9547 - val_loss: 0.4529 - val_accuracy: 0.8773 Epoch 109/200 240/240 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9549 - val_loss: 0.4501 - val_accuracy: 0.8774 Epoch 110/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9541 - val_loss: 0.4490 - val_accuracy: 0.8767 Epoch 111/200 240/240 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9549 - val_loss: 0.4586 - val_accuracy: 0.8762 Epoch 112/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9566 - val_loss: 0.4441 - val_accuracy: 0.8771 Epoch 113/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9550 - val_loss: 0.4583 - val_accuracy: 0.8777 Epoch 114/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9576 - val_loss: 0.4618 - val_accuracy: 0.8748 Epoch 115/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9562 - val_loss: 0.4569 - val_accuracy: 0.8790 Epoch 116/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1238 - accuracy: 0.9563 - val_loss: 0.4730 - val_accuracy: 0.8763 Epoch 117/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1180 - accuracy: 0.9584 - val_loss: 0.4572 - val_accuracy: 0.8798 Epoch 118/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9578 - val_loss: 0.4736 - val_accuracy: 0.8772 Epoch 119/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9569 - val_loss: 0.4706 - val_accuracy: 0.8783 Epoch 120/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9564 - val_loss: 0.4631 - val_accuracy: 0.8789 Epoch 121/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9602 - val_loss: 0.4829 - val_accuracy: 0.8754 Epoch 122/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9591 - val_loss: 0.4767 - val_accuracy: 0.8751 Epoch 123/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9575 - val_loss: 0.4848 - val_accuracy: 0.8785 Epoch 124/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9583 - val_loss: 0.4890 - val_accuracy: 0.8753 Epoch 125/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9586 - val_loss: 0.4816 - val_accuracy: 0.8780 Epoch 126/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9599 - val_loss: 0.4894 - val_accuracy: 0.8756 Epoch 127/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9597 - val_loss: 0.4962 - val_accuracy: 0.8738 Epoch 128/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9611 - val_loss: 0.4793 - val_accuracy: 0.8771 Epoch 129/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9598 - val_loss: 0.4997 - val_accuracy: 0.8741 Epoch 130/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9615 - val_loss: 0.4841 - val_accuracy: 0.8788 Epoch 131/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9614 - val_loss: 0.4910 - val_accuracy: 0.8775 Epoch 132/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9611 - val_loss: 0.4896 - val_accuracy: 0.8777 Epoch 133/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9606 - val_loss: 0.5045 - val_accuracy: 0.8742 Epoch 134/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9633 - val_loss: 0.5088 - val_accuracy: 0.8744 Epoch 135/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1089 - accuracy: 0.9625 - val_loss: 0.5210 - val_accuracy: 0.8723 Epoch 136/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1075 - accuracy: 0.9621 - val_loss: 0.4973 - val_accuracy: 0.8777 Epoch 137/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9632 - val_loss: 0.5145 - val_accuracy: 0.8767 Epoch 138/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.9613 - val_loss: 0.5111 - val_accuracy: 0.8773 Epoch 139/200 240/240 [==============================] - 0s 2ms/step - loss: 0.1068 - accuracy: 0.9623 - val_loss: 0.5193 - val_accuracy: 0.8747 Epoch 140/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9640 - val_loss: 0.5121 - val_accuracy: 0.8764 Epoch 141/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9615 - val_loss: 0.5251 - val_accuracy: 0.8723 Epoch 142/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9616 - val_loss: 0.5145 - val_accuracy: 0.8742 Epoch 143/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9623 - val_loss: 0.5349 - val_accuracy: 0.8716 Epoch 144/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9643 - val_loss: 0.5369 - val_accuracy: 0.8732 Epoch 145/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9648 - val_loss: 0.5294 - val_accuracy: 0.8750 Epoch 146/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9634 - val_loss: 0.5301 - val_accuracy: 0.8755 Epoch 147/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9640 - val_loss: 0.5316 - val_accuracy: 0.8763 Epoch 148/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9635 - val_loss: 0.5387 - val_accuracy: 0.8747 Epoch 149/200 240/240 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9652 - val_loss: 0.5307 - val_accuracy: 0.8764 Epoch 150/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0987 - accuracy: 0.9653 - val_loss: 0.5296 - val_accuracy: 0.8756 Epoch 151/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9670 - val_loss: 0.5342 - val_accuracy: 0.8748 Epoch 152/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9664 - val_loss: 0.5357 - val_accuracy: 0.8777 Epoch 153/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.9654 - val_loss: 0.5743 - val_accuracy: 0.8718 Epoch 154/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9648 - val_loss: 0.5371 - val_accuracy: 0.8769 Epoch 155/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0996 - accuracy: 0.9650 - val_loss: 0.5439 - val_accuracy: 0.8737 Epoch 156/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.9659 - val_loss: 0.5551 - val_accuracy: 0.8755 Epoch 157/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9659 - val_loss: 0.5496 - val_accuracy: 0.8748 Epoch 158/200 240/240 [==============================] - 1s 2ms/step - loss: 0.0923 - accuracy: 0.9680 - val_loss: 0.5531 - val_accuracy: 0.8756 Epoch 159/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9662 - val_loss: 0.5743 - val_accuracy: 0.8734 Epoch 160/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9680 - val_loss: 0.5738 - val_accuracy: 0.8728 Epoch 161/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0921 - accuracy: 0.9683 - val_loss: 0.5636 - val_accuracy: 0.8764 Epoch 162/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9693 - val_loss: 0.5640 - val_accuracy: 0.8773 Epoch 163/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9683 - val_loss: 0.5668 - val_accuracy: 0.8764 Epoch 164/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9677 - val_loss: 0.5689 - val_accuracy: 0.8758 Epoch 165/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9697 - val_loss: 0.5758 - val_accuracy: 0.8702 Epoch 166/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9665 - val_loss: 0.5881 - val_accuracy: 0.8727 Epoch 167/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9689 - val_loss: 0.5930 - val_accuracy: 0.8724 Epoch 168/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9679 - val_loss: 0.5920 - val_accuracy: 0.8740 Epoch 169/200 240/240 [==============================] - 1s 2ms/step - loss: 0.0880 - accuracy: 0.9691 - val_loss: 0.5736 - val_accuracy: 0.8730 Epoch 170/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 0.9698 - val_loss: 0.5754 - val_accuracy: 0.8734 Epoch 171/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9671 - val_loss: 0.5923 - val_accuracy: 0.8743 Epoch 172/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9703 - val_loss: 0.5758 - val_accuracy: 0.8753 Epoch 173/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0864 - accuracy: 0.9700 - val_loss: 0.6101 - val_accuracy: 0.8705 Epoch 174/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9702 - val_loss: 0.5810 - val_accuracy: 0.8750 Epoch 175/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9695 - val_loss: 0.5768 - val_accuracy: 0.8757 Epoch 176/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9710 - val_loss: 0.5940 - val_accuracy: 0.8751 Epoch 177/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9676 - val_loss: 0.5863 - val_accuracy: 0.8751 Epoch 178/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9702 - val_loss: 0.6071 - val_accuracy: 0.8735 Epoch 179/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9698 - val_loss: 0.6243 - val_accuracy: 0.8692 Epoch 180/200 240/240 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.9700 - val_loss: 0.5947 - val_accuracy: 0.8748 Epoch 181/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9719 - val_loss: 0.6140 - val_accuracy: 0.8761 Epoch 182/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9692 - val_loss: 0.6268 - val_accuracy: 0.8682 Epoch 183/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9714 - val_loss: 0.6151 - val_accuracy: 0.8693 Epoch 184/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9721 - val_loss: 0.6285 - val_accuracy: 0.8736 Epoch 185/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0805 - accuracy: 0.9722 - val_loss: 0.6107 - val_accuracy: 0.8728 Epoch 186/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9705 - val_loss: 0.6130 - val_accuracy: 0.8724 Epoch 187/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9703 - val_loss: 0.6242 - val_accuracy: 0.8719 Epoch 188/200 240/240 [==============================] - 1s 2ms/step - loss: 0.0817 - accuracy: 0.9715 - val_loss: 0.6165 - val_accuracy: 0.8740 Epoch 189/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0823 - accuracy: 0.9716 - val_loss: 0.6304 - val_accuracy: 0.8698 Epoch 190/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9729 - val_loss: 0.6261 - val_accuracy: 0.8752 Epoch 191/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.9739 - val_loss: 0.6376 - val_accuracy: 0.8702 Epoch 192/200 240/240 [==============================] - 1s 2ms/step - loss: 0.0774 - accuracy: 0.9733 - val_loss: 0.6256 - val_accuracy: 0.8737 Epoch 193/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9728 - val_loss: 0.6324 - val_accuracy: 0.8732 Epoch 194/200 240/240 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9730 - val_loss: 0.6201 - val_accuracy: 0.8747 Epoch 195/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9728 - val_loss: 0.6491 - val_accuracy: 0.8728 Epoch 196/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9733 - val_loss: 0.6267 - val_accuracy: 0.8743 Epoch 197/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9725 - val_loss: 0.6708 - val_accuracy: 0.8676 Epoch 198/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9711 - val_loss: 0.6481 - val_accuracy: 0.8733 Epoch 199/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9753 - val_loss: 0.6512 - val_accuracy: 0.8707 Epoch 200/200 240/240 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9741 - val_loss: 0.6442 - val_accuracy: 0.8731 . &lt;keras.callbacks.History at 0x7f16d07e5bd0&gt; . . - 텐서보드 여는 방법1 . %load_ext tensorboard # 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션 . !rm -rf logs !kill 313799 . /bin/bash: line 0: kill: (313799) - No such process . # %tensorboard --logdir logs --host 0.0.0.0 # %tensorboard --logdir logs &lt;-- 실습에서는 이렇게 하면됩니다. . (참고사항) 파이썬 3.10의 경우 아래의 수정이 필요 . ?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고 . from collections import Mapping ### 수정전 from collections.abc import Mapping ### 수정후 . 와 같이 수정한다. . 왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜 | . - 텐서보드를 실행하는 방법2 . # !tensorboard --logdir logs --host 0.0.0.0 # !tensorboard --logdir logs &lt;-- 실습에서는 이렇게 하면됩니다. . &#51312;&#44592;&#51333;&#47308; . - 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) $ to$ 그 특정에폭까지만 학습해보자 . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 과적합좀 시키려고 net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.5486 - accuracy: 0.8125 - val_loss: 0.4033 - val_accuracy: 0.8545 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.3588 - accuracy: 0.8666 - val_loss: 0.3655 - val_accuracy: 0.8677 Epoch 3/200 240/240 [==============================] - 2s 6ms/step - loss: 0.3219 - accuracy: 0.8792 - val_loss: 0.3597 - val_accuracy: 0.8700 Epoch 4/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2965 - accuracy: 0.8895 - val_loss: 0.3406 - val_accuracy: 0.8800 Epoch 5/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2797 - accuracy: 0.8955 - val_loss: 0.3240 - val_accuracy: 0.8816 Epoch 6/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2624 - accuracy: 0.9012 - val_loss: 0.3120 - val_accuracy: 0.8862 Epoch 7/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2488 - accuracy: 0.9063 - val_loss: 0.3430 - val_accuracy: 0.8804 . &lt;keras.callbacks.History at 0x7f16d056f760&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.2389 - accuracy: 0.9076 - val_loss: 0.3323 - val_accuracy: 0.8798 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2240 - accuracy: 0.9147 - val_loss: 0.3496 - val_accuracy: 0.8788 . &lt;keras.callbacks.History at 0x7f179aa81510&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.2138 - accuracy: 0.9187 - val_loss: 0.3252 - val_accuracy: 0.8848 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2008 - accuracy: 0.9232 - val_loss: 0.3383 - val_accuracy: 0.8868 . &lt;keras.callbacks.History at 0x7f16e8325840&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.1936 - accuracy: 0.9250 - val_loss: 0.3641 - val_accuracy: 0.8862 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1812 - accuracy: 0.9296 - val_loss: 0.3449 - val_accuracy: 0.8869 Epoch 3/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1854 - accuracy: 0.9272 - val_loss: 0.3366 - val_accuracy: 0.8931 Epoch 4/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1744 - accuracy: 0.9331 - val_loss: 0.3356 - val_accuracy: 0.8975 Epoch 5/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1650 - accuracy: 0.9373 - val_loss: 0.3358 - val_accuracy: 0.8967 . &lt;keras.callbacks.History at 0x7f16d056e110&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.1521 - accuracy: 0.9399 - val_loss: 0.3884 - val_accuracy: 0.8892 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1464 - accuracy: 0.9424 - val_loss: 0.3712 - val_accuracy: 0.8899 Epoch 3/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1455 - accuracy: 0.9439 - val_loss: 0.4089 - val_accuracy: 0.8866 . &lt;keras.callbacks.History at 0x7f176016df00&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.1338 - accuracy: 0.9480 - val_loss: 0.4026 - val_accuracy: 0.8889 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1254 - accuracy: 0.9515 - val_loss: 0.3872 - val_accuracy: 0.8932 Epoch 3/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1231 - accuracy: 0.9517 - val_loss: 0.4383 - val_accuracy: 0.8903 . &lt;keras.callbacks.History at 0x7f16d056fa90&gt; . - 몇 번 좀 참았다가 멈추면 좋겠다. . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 과적합좀 시키려고 net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 2s 7ms/step - loss: 0.5483 - accuracy: 0.8130 - val_loss: 0.4226 - val_accuracy: 0.8457 Epoch 2/200 240/240 [==============================] - 2s 6ms/step - loss: 0.3569 - accuracy: 0.8672 - val_loss: 0.3609 - val_accuracy: 0.8700 Epoch 3/200 240/240 [==============================] - 2s 6ms/step - loss: 0.3240 - accuracy: 0.8791 - val_loss: 0.3546 - val_accuracy: 0.8732 Epoch 4/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2973 - accuracy: 0.8889 - val_loss: 0.3496 - val_accuracy: 0.8784 Epoch 5/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2817 - accuracy: 0.8951 - val_loss: 0.3302 - val_accuracy: 0.8807 Epoch 6/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2635 - accuracy: 0.8991 - val_loss: 0.3229 - val_accuracy: 0.8824 Epoch 7/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2487 - accuracy: 0.9064 - val_loss: 0.3288 - val_accuracy: 0.8843 Epoch 8/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2348 - accuracy: 0.9102 - val_loss: 0.2994 - val_accuracy: 0.8954 Epoch 9/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2262 - accuracy: 0.9149 - val_loss: 0.3088 - val_accuracy: 0.8917 Epoch 10/200 240/240 [==============================] - 2s 6ms/step - loss: 0.2145 - accuracy: 0.9189 - val_loss: 0.3157 - val_accuracy: 0.8915 Epoch 11/200 240/240 [==============================] - 1s 6ms/step - loss: 0.2045 - accuracy: 0.9220 - val_loss: 0.3349 - val_accuracy: 0.8833 Epoch 12/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1981 - accuracy: 0.9236 - val_loss: 0.3379 - val_accuracy: 0.8924 Epoch 13/200 240/240 [==============================] - 2s 6ms/step - loss: 0.1865 - accuracy: 0.9276 - val_loss: 0.3422 - val_accuracy: 0.8922 . &lt;keras.callbacks.History at 0x7f16e8314100&gt; . - 텐서보드로 그려보자? . # %tensorboard --logdir logs --host 0.0.0.0 # 아무것도 안나온다 -&gt; 왜? cb1을 써야 텐서보드가 나옴 . - 조기종료와 텐서보드를 같이 쓰려면? . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) . Epoch 1/200 240/240 [==============================] - 1s 2ms/step - loss: 0.7184 - accuracy: 0.7581 - val_loss: 0.5077 - val_accuracy: 0.8276 Epoch 2/200 240/240 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.8386 - val_loss: 0.4793 - val_accuracy: 0.8342 Epoch 3/200 240/240 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8517 - val_loss: 0.4386 - val_accuracy: 0.8497 Epoch 4/200 240/240 [==============================] - 0s 1ms/step - loss: 0.4048 - accuracy: 0.8582 - val_loss: 0.4029 - val_accuracy: 0.8603 Epoch 5/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8669 - val_loss: 0.3932 - val_accuracy: 0.8619 Epoch 6/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8705 - val_loss: 0.3842 - val_accuracy: 0.8657 Epoch 7/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8759 - val_loss: 0.3844 - val_accuracy: 0.8668 Epoch 8/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8774 - val_loss: 0.3679 - val_accuracy: 0.8708 Epoch 9/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8799 - val_loss: 0.3602 - val_accuracy: 0.8719 Epoch 10/200 240/240 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8820 - val_loss: 0.3610 - val_accuracy: 0.8748 Epoch 11/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8858 - val_loss: 0.3574 - val_accuracy: 0.8717 Epoch 12/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.8873 - val_loss: 0.3572 - val_accuracy: 0.8743 Epoch 13/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8899 - val_loss: 0.3545 - val_accuracy: 0.8761 Epoch 14/200 240/240 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8914 - val_loss: 0.3493 - val_accuracy: 0.8759 Epoch 15/200 240/240 [==============================] - 1s 2ms/step - loss: 0.3011 - accuracy: 0.8928 - val_loss: 0.3483 - val_accuracy: 0.8776 Epoch 16/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8935 - val_loss: 0.3733 - val_accuracy: 0.8716 Epoch 17/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8947 - val_loss: 0.3481 - val_accuracy: 0.8768 Epoch 18/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8951 - val_loss: 0.3396 - val_accuracy: 0.8801 Epoch 19/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.8982 - val_loss: 0.3439 - val_accuracy: 0.8798 Epoch 20/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8986 - val_loss: 0.3489 - val_accuracy: 0.8779 Epoch 21/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.9006 - val_loss: 0.3348 - val_accuracy: 0.8819 Epoch 22/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.9020 - val_loss: 0.3349 - val_accuracy: 0.8822 Epoch 23/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2688 - accuracy: 0.9026 - val_loss: 0.3382 - val_accuracy: 0.8817 Epoch 24/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.9045 - val_loss: 0.3441 - val_accuracy: 0.8806 Epoch 25/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.9057 - val_loss: 0.3383 - val_accuracy: 0.8825 Epoch 26/200 240/240 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.9058 - val_loss: 0.3417 - val_accuracy: 0.8813 Epoch 27/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.9071 - val_loss: 0.3390 - val_accuracy: 0.8827 Epoch 28/200 240/240 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.9099 - val_loss: 0.3476 - val_accuracy: 0.8802 . &lt;keras.callbacks.History at 0x7f179a9e4ca0&gt; . # 조기종료가 구현된 그림이 출력 # %tensorboard --logdir logs --host 0.0.0.0 . &#54616;&#51060;&#54140;&#54028;&#46972;&#47700;&#53552; &#49440;&#53469; . - 하이퍼파라메터 설정 . from tensorboard.plugins.hparams import api as hp . a=net.evaluate(XX,yy) . 313/313 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.8394 - recall: 0.7954 . !rm -rf logs for u in [50,5000]: for d in [0.0,0.5]: for o in [&#39;adam&#39;,&#39;sgd&#39;]: logdir = &#39;logs/hpguebin_{}_{}_{}&#39;.format(u,d,o) with tf.summary.create_file_writer(logdir).as_default(): net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(u,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dropout(d)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) cb3 = hp.KerasCallback(logdir, {&#39;유닛수&#39;:u, &#39;드랍아웃비율&#39;:d, &#39;옵티마이저&#39;:o}) net.fit(X,y,epochs=3,callbacks=cb3) _rslt=net.evaluate(XX,yy) _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2 tf.summary.scalar(&#39;애큐러시와리컬의가중평균(테스트셋)&#39;, _mymetric, step=1) . Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5188 - accuracy: 0.8189 - recall: 0.7576 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3968 - accuracy: 0.8585 - recall: 0.8284 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3607 - accuracy: 0.8710 - recall: 0.8455 313/313 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8678 - recall: 0.8395 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.7694 - accuracy: 0.7455 - recall: 0.5802 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5252 - accuracy: 0.8219 - recall: 0.7510 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4776 - accuracy: 0.8345 - recall: 0.7822 313/313 [==============================] - 0s 1ms/step - loss: 0.4939 - accuracy: 0.8275 - recall: 0.7832 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.7581 - accuracy: 0.7297 - recall: 0.6071 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5701 - accuracy: 0.7951 - recall: 0.7187 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5344 - accuracy: 0.8073 - recall: 0.7408 313/313 [==============================] - 1s 1ms/step - loss: 0.4294 - accuracy: 0.8452 - recall: 0.8059 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 1.0712 - accuracy: 0.6312 - recall: 0.4066 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.7434 - accuracy: 0.7427 - recall: 0.5969 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6629 - accuracy: 0.7724 - recall: 0.6581 313/313 [==============================] - 0s 1ms/step - loss: 0.5274 - accuracy: 0.8141 - recall: 0.7337 Epoch 1/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.4770 - accuracy: 0.8273 - recall: 0.7879 Epoch 2/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3609 - accuracy: 0.8680 - recall: 0.8424 Epoch 3/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3249 - accuracy: 0.8802 - recall: 0.8590 313/313 [==============================] - 1s 1ms/step - loss: 0.3491 - accuracy: 0.8738 - recall: 0.8532 Epoch 1/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.6690 - accuracy: 0.7883 - recall: 0.6449 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4818 - accuracy: 0.8358 - recall: 0.7764 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4416 - accuracy: 0.8486 - recall: 0.8011 313/313 [==============================] - 1s 1ms/step - loss: 0.4618 - accuracy: 0.8383 - recall: 0.7964 Epoch 1/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.5733 - accuracy: 0.7972 - recall: 0.7537 Epoch 2/3 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4442 - accuracy: 0.8388 - recall: 0.8066 Epoch 3/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.4108 - accuracy: 0.8513 - recall: 0.8211 313/313 [==============================] - 1s 1ms/step - loss: 0.3738 - accuracy: 0.8658 - recall: 0.8398 Epoch 1/3 1875/1875 [==============================] - 3s 1ms/step - loss: 0.6975 - accuracy: 0.7744 - recall: 0.6331 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5039 - accuracy: 0.8293 - recall: 0.7648 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4612 - accuracy: 0.8407 - recall: 0.7899 313/313 [==============================] - 1s 1ms/step - loss: 0.4620 - accuracy: 0.8387 - recall: 0.7933 . #%tensorboard --logdir logs --host 0.0.0.0 . &#49689;&#51228; . - 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라. . net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) . !rm -rf logs tf.random.set_seed(202150754) net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Flatten()) net1.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net1.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) . cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=7) net1.fit(X,y,epochs=5,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) . Epoch 1/5 240/240 [==============================] - 1s 3ms/step - loss: 0.7224 - accuracy: 0.7586 - recall: 0.6304 - val_loss: 0.4817 - val_accuracy: 0.8329 - val_recall: 0.7788 Epoch 2/5 240/240 [==============================] - 1s 2ms/step - loss: 0.4508 - accuracy: 0.8419 - recall: 0.7951 - val_loss: 0.4497 - val_accuracy: 0.8349 - val_recall: 0.7932 Epoch 3/5 240/240 [==============================] - 1s 3ms/step - loss: 0.4064 - accuracy: 0.8567 - recall: 0.8202 - val_loss: 0.4013 - val_accuracy: 0.8540 - val_recall: 0.8188 Epoch 4/5 240/240 [==============================] - 1s 3ms/step - loss: 0.3728 - accuracy: 0.8685 - recall: 0.8364 - val_loss: 0.3892 - val_accuracy: 0.8611 - val_recall: 0.8289 Epoch 5/5 240/240 [==============================] - 1s 3ms/step - loss: 0.3571 - accuracy: 0.8714 - recall: 0.8434 - val_loss: 0.3857 - val_accuracy: 0.8620 - val_recall: 0.8403 . &lt;keras.callbacks.History at 0x7f179a6b9d50&gt; . . - sgd 적합 . !rm -rf logs tf.random.set_seed(202150754) net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Flatten()) net2.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net2.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net2.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net2.compile(optimizer=&#39;sgd&#39;,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) . net2.fit(X,y,epochs=5,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) . Epoch 1/5 240/240 [==============================] - 1s 3ms/step - loss: 1.6588 - accuracy: 0.4999 - recall: 0.0831 - val_loss: 1.1223 - val_accuracy: 0.6730 - val_recall: 0.2842 Epoch 2/5 240/240 [==============================] - 1s 3ms/step - loss: 0.9301 - accuracy: 0.6911 - recall: 0.4377 - val_loss: 0.8035 - val_accuracy: 0.7196 - val_recall: 0.5241 Epoch 3/5 240/240 [==============================] - 0s 2ms/step - loss: 0.7534 - accuracy: 0.7372 - recall: 0.5661 - val_loss: 0.7023 - val_accuracy: 0.7613 - val_recall: 0.6117 Epoch 4/5 240/240 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.7693 - recall: 0.6336 - val_loss: 0.6443 - val_accuracy: 0.7802 - val_recall: 0.6617 Epoch 5/5 240/240 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.7891 - recall: 0.6790 - val_loss: 0.6030 - val_accuracy: 0.7968 - val_recall: 0.6976 . &lt;keras.callbacks.History at 0x7f17f6e58a90&gt; . . net1.evaluate(XX,yy)[0] . 313/313 [==============================] - 1s 1ms/step - loss: 0.4171 - accuracy: 0.8543 - recall: 0.8275 . 0.41707104444503784 . net2.evaluate(XX,yy)[0] . 313/313 [==============================] - 1s 1ms/step - loss: 0.6277 - accuracy: 0.7835 - recall: 0.6898 . 0.6277241706848145 . adam을 선택했을떄 loss는 0.41707104444503784 이고, . sgd을 선택했을때 loss는 0.6277241706848145 이다. . loss 값을 보면 sgd보다 adam 값이 더 낮다. .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-23%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-23%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "11주차-5월 16일",
            "content": "imports . import tensorflow as tf import tensorflow.experimental.numpy as tnp import numpy as np import matplotlib.pyplot as plt . tnp.experimental_enable_numpy_behavior() . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64) y = tf.keras.utils.to_categorical(y_train) XX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64) yy = tf.keras.utils.to_categorical(y_test) . - 첫시도 . net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Flatten()) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net1.compile(optimizer=&#39;adam&#39;, loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) net1.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 3s 2ms/step - loss: 1.0565 - accuracy: 0.7923 Epoch 2/5 1875/1875 [==============================] - 3s 1ms/step - loss: 0.4495 - accuracy: 0.8389 Epoch 3/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4135 - accuracy: 0.8525 Epoch 4/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.4001 - accuracy: 0.8579 Epoch 5/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3753 - accuracy: 0.8673 . &lt;keras.callbacks.History at 0x7f55d055a8c0&gt; . net1.evaluate(XX,yy) . 313/313 [==============================] - 1s 2ms/step - loss: 0.4420 - accuracy: 0.8570 . [0.4419509172439575, 0.8569999933242798] . net1.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense (Dense) (None, 500) 392500 dense_1 (Dense) (None, 500) 250500 dense_2 (Dense) (None, 500) 250500 dense_3 (Dense) (None, 500) 250500 dense_4 (Dense) (None, 10) 5010 ================================================================= Total params: 1,149,010 Trainable params: 1,149,010 Non-trainable params: 0 _________________________________________________________________ . - 두번째 시도 . net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Conv2D(30,(2,2),activation=&#39;relu&#39;)) net2.add(tf.keras.layers.MaxPool2D()) net2.add(tf.keras.layers.Conv2D(30,(2,2),activation=&#39;relu&#39;)) net2.add(tf.keras.layers.MaxPool2D()) net2.add(tf.keras.layers.Flatten()) #net2.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net2.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net2.compile(optimizer=&#39;adam&#39;, loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) net2.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 3s 1ms/step - loss: 0.7357 - accuracy: 0.8065 Epoch 2/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3728 - accuracy: 0.8676 Epoch 3/5 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3346 - accuracy: 0.8798 Epoch 4/5 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3110 - accuracy: 0.8875 Epoch 5/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2929 - accuracy: 0.8925 . &lt;keras.callbacks.History at 0x7f55d055be50&gt; . net2.evaluate(XX,yy) . 313/313 [==============================] - 1s 1ms/step - loss: 0.3280 - accuracy: 0.8807 . [0.32795271277427673, 0.8806999921798706] . net2.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 27, 27, 30) 150 max_pooling2d (MaxPooling2D (None, 13, 13, 30) 0 ) conv2d_1 (Conv2D) (None, 12, 12, 30) 3630 max_pooling2d_1 (MaxPooling (None, 6, 6, 30) 0 2D) flatten_1 (Flatten) (None, 1080) 0 dense_5 (Dense) (None, 10) 10810 ================================================================= Total params: 14,590 Trainable params: 14,590 Non-trainable params: 0 _________________________________________________________________ . 14590/ 1149010 . 0.012697887746842934 . c1, m1, c2, m2, flttn, dns = net2.layers . MaxPool2D . &#53580;&#49828;&#53944;1 . - 레이어생성 . m=tf.keras.layers.MaxPool2D() . - 입력데이터 . XXX = tnp.arange(1*4*4*1).reshape(1,4,4,1) XXX.reshape(1,4,4) . &lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt; . - 입력데이터가 레이어를 통과한 모습 . m(XXX).reshape(1,2,2) . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[ 5, 7], [13, 15]]])&gt; . - MaxPool2D layer의 역할: (2,2)윈도우를 만들고 (2,2)윈도우에서 max를 뽑아 값을 기록, 윈도우를 움직이면서 반복 . &#53580;&#49828;&#53944;2 . XXX = tnp.arange(1*6*6*1).reshape(1,6,6,1) XXX.reshape(1,6,6) . &lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]])&gt; . m(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy= array([[[ 7, 9, 11], [19, 21, 23], [31, 33, 35]]])&gt; . &#53580;&#49828;&#53944;3 . m=tf.keras.layers.MaxPool2D(pool_size=(3, 3)) . XXX = tnp.arange(1*6*6*1).reshape(1,6,6,1) XXX.reshape(1,6,6) . &lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]])&gt; . m(XXX).reshape(1,2,2) . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[14, 17], [32, 35]]])&gt; . &#53580;&#49828;&#53944;4 . m=tf.keras.layers.MaxPool2D(pool_size=(2, 2)) . XXX = tnp.arange(1*5*5*1).reshape(1,5,5,1) XXX.reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]])&gt; . m(XXX).reshape(1,2,2) . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[ 6, 8], [16, 18]]])&gt; . m=tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding=&quot;same&quot;) . XXX = tnp.arange(1*5*5*1).reshape(1,5,5,1) XXX.reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]])&gt; . m(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy= array([[[ 6, 8, 9], [16, 18, 19], [21, 23, 24]]])&gt; . &#53580;&#49828;&#53944;5 . XXX = tnp.arange(2*4*4*1).reshape(2,4,4,1) XXX.reshape(2,4,4) . &lt;tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]])&gt; . m(XXX).reshape(2,2,2) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy= array([[[ 5, 7], [13, 15]], [[21, 23], [29, 31]]])&gt; . &#53580;&#49828;&#53944;6 . XXX = tnp.arange(1*4*4*3).reshape(1,4,4,3) . XXX[...,0] . &lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy= array([[[ 0, 3, 6, 9], [12, 15, 18, 21], [24, 27, 30, 33], [36, 39, 42, 45]]])&gt; . m(XXX)[...,0] . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[15, 21], [39, 45]]])&gt; . Conv2D . &#53580;&#49828;&#53944;1 . - 레이어생성 . cnv = tf.keras.layers.Conv2D(1,(2,2)) . - XXX생성 . XXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) XXX.reshape(1,4,4) . &lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy= array([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]])&gt; . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[-2.8128474, -3.2847993, -3.756751 ], [-4.7006545, -5.172607 , -5.6445584], [-6.588462 , -7.060414 , -7.532366 ]]], dtype=float32)&gt; . XXX에서 cnv(XXX)로 가는 맵핑을 찾는건 쉽지 않아보인다. | 심지어 랜덤으로 결정되는 부분도 있어보임 | . - 코드정리 + 시드통일 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(1,(2,2)) XXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) . - conv의 입출력 . print(XXX.reshape(1,4,4)) print(cnv(XXX).reshape(1,3,3)) . tf.Tensor( [[[ 0. 1. 2. 3.] [ 4. 5. 6. 7.] [ 8. 9. 10. 11.] [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64) tf.Tensor( [[[ -4.125754 -5.312817 -6.4998803] [ -8.874006 -10.0610695 -11.248133 ] [-13.622259 -14.809322 -15.996386 ]]], shape=(1, 3, 3), dtype=float32) . - conv연산 추론 . tf.reshape(cnv.weights[0],(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-0.13014299, -0.23927206], [-0.20175874, -0.6158894 ]], dtype=float32)&gt; . 0 * -0.13014299 + 1 * -0.23927206 + 4 * -0.20175874 + 5 * -0.6158894 + 0 . -4.1257540200000005 . - 내가 정의한 weights를 대입하여 conv 연산 확인 . cnv.get_weights()[0].shape . (2, 2, 1, 1) . w = np.array([1/4,1/4,1/4,1/4],dtype=np.float32).reshape(2, 2, 1, 1) b = np.array([3],dtype=np.float32) . cnv.set_weights([w,b]) . XXX.reshape(1,4,4) . &lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy= array([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]])&gt; . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[ 5.5, 6.5, 7.5], [ 9.5, 10.5, 11.5], [13.5, 14.5, 15.5]]], dtype=float32)&gt; . np.mean([0,1,4,5])+3, np.mean([1,2,5,6])+3, np.mean([2,3,6,7])+3 . (5.5, 6.5, 7.5) . tf.keras.layers.Conv2D(1,kernel_size=(2,2)) &#50836;&#50557; . - 요약 . (1) size=(2,2)인 윈도우를 만듬. . (2) XXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX 를 얻음. sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다. . (3) (2)의 결과를 모두 더한다. 그리고 그 결과에 다시 conv2d.weights[1]을 수행 . (4) 윈도우를 이동시키면서 반복! . &#53580;&#49828;&#53944;2 . - 레이어와 XXX생성 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(1,(3,3)) XXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1) . XXX.reshape(1,5,5) ## 입력: XXX . &lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy= array([[[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.], [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.], [20., 21., 22., 23., 24.]]])&gt; . tf.reshape(cnv.weights[0],(3,3)) ## 커널의 가중치 . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[-0.08676198, -0.1595147 , -0.13450584], [-0.4105929 , -0.38366908, 0.07744962], [-0.09255642, 0.4915564 , 0.20828158]], dtype=float32)&gt; . cnv(XXX).reshape(1,3,3) ## 출력: conv(XXX) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[ 2.7395768 , 2.2492635 , 1.7589504 ], [ 0.28801066, -0.20230258, -0.6926158 ], [-2.1635566 , -2.6538715 , -3.1441827 ]]], dtype=float32)&gt; . tf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3))) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.739577144384384&gt; . &#53580;&#49828;&#53944;3 . ![](https://github.com/guebin/2021BDA/blob/master/_notebooks/2021-11-04-conv.png?raw=true) . /bin/bash: -c: line 0: syntax error near unexpected token `https://github.com/guebin/2021BDA/blob/master/_notebooks/2021-11-04-conv.png?raw=true&#39; /bin/bash: -c: line 0: `[](https://github.com/guebin/2021BDA/blob/master/_notebooks/2021-11-04-conv.png?raw=true)&#39; . XXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1) XXX.reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy= array([[[3., 3., 2., 1., 0.], [0., 0., 1., 3., 1.], [3., 1., 2., 2., 3.], [2., 0., 0., 2., 2.], [2., 0., 0., 0., 1.]]])&gt; . cnv = tf.keras.layers.Conv2D(1,(3,3)) . cnv.weights . [] . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[-0.6065799 , -0.69143724, -1.1179221 ], [ 2.2352955 , 1.5314975 , 1.8658445 ], [ 0.7364182 , 1.4419123 , 1.2381717 ]]], dtype=float32)&gt; . cnv.weights[0] . &lt;tf.Variable &#39;conv2d_6/kernel:0&#39; shape=(3, 3, 1, 1) dtype=float32, numpy= array([[[[-0.3444655 ]], [[ 0.4521824 ]], [[ 0.236296 ]]], [[[ 0.54707503]], [[ 0.19746327]], [[ 0.20471048]]], [[[-0.1245549 ]], [[-0.25237298]], [[-0.4904977 ]]]], dtype=float32)&gt; . _w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype=tf.float64).reshape(3,3,1,1) _b = tf.constant([0],dtype=tf.float64) . cnv.set_weights([_w,_b]) . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[12., 12., 17.], [10., 17., 19.], [ 9., 6., 14.]]], dtype=float32)&gt; . &#53580;&#49828;&#53944;4 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(1,(2,2)) XXX = tnp.arange(2*5*5*1,dtype=tf.float64).reshape(2,5,5,1) . print(XXX.reshape(2,5,5)) cnv(XXX) # weights를 초기화 시키기 위해서 레이어를 1회 통과 cnv.set_weights([w,b]) print(cnv(XXX).reshape(2,4,4)) . tf.Tensor( [[[ 0. 1. 2. 3. 4.] [ 5. 6. 7. 8. 9.] [10. 11. 12. 13. 14.] [15. 16. 17. 18. 19.] [20. 21. 22. 23. 24.]] [[25. 26. 27. 28. 29.] [30. 31. 32. 33. 34.] [35. 36. 37. 38. 39.] [40. 41. 42. 43. 44.] [45. 46. 47. 48. 49.]]], shape=(2, 5, 5), dtype=float64) tf.Tensor( [[[ 6. 7. 8. 9.] [11. 12. 13. 14.] [16. 17. 18. 19.] [21. 22. 23. 24.]] [[31. 32. 33. 34.] [36. 37. 38. 39.] [41. 42. 43. 44.] [46. 47. 48. 49.]]], shape=(2, 4, 4), dtype=float32) . np.mean([0,1,5,6])+3,np.mean([25,26,30,31])+3, . (6.0, 31.0) . &#53580;&#49828;&#53944;5 . - . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(4,(2,2),activation=&#39;relu&#39;) XXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1) . print(XXX.reshape(1,2,2)) . tf.Tensor( [[[0. 1.] [2. 3.]]], shape=(1, 2, 2), dtype=float64) . cnv(XXX) . &lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[1.048703, 0. , 0. , 0. ]]]], dtype=float32)&gt; . cnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수 . &lt;tf.Variable &#39;conv2d_8/kernel:0&#39; shape=(2, 2, 1, 4) dtype=float32, numpy= array([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]], [[-0.36398047, 0.07347518, -0.08780673, 0.46633136]]], [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]], [[ 0.33916563, -0.08248386, 0.11705655, -0.49948823]]]], dtype=float32)&gt; . cnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-0.08230966, -0.36398047], [ 0.19759327, 0.33916563]], dtype=float32)&gt; . tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과 . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0487029552459717&gt; . - 계산결과를 확인하기 쉽게 하기 위한 약간의 트릭 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(4,(2,2)) XXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1) . print(XXX.reshape(1,2,2)) . tf.Tensor( [[[1. 1.] [1. 1.]]], shape=(1, 2, 2), dtype=float64) . 이렇게 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weight들의 sum이 된다. | . cnv(XXX) . &lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy= array([[[[ 0.09046876, -0.6207628 , -0.25241536, -0.7710641 ]]]], dtype=float32)&gt; . cnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수 . &lt;tf.Variable &#39;conv2d_9/kernel:0&#39; shape=(2, 2, 1, 4) dtype=float32, numpy= array([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]], [[-0.36398047, 0.07347518, -0.08780673, 0.46633136]]], [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]], [[ 0.33916563, -0.08248386, 0.11705655, -0.49948823]]]], dtype=float32)&gt; . cnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-0.08230966, -0.36398047], [ 0.19759327, 0.33916563]], dtype=float32)&gt; . tf.reduce_sum(cnv.weights[0][...,0]) #tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.090468764&gt; . &#53580;&#49828;&#53944;6 . - 결과확인을 쉽게하기 위해서 XXX를 1로 통일 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(4,(2,2)) XXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3) . cnv(XXX) . &lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy= array([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]], dtype=float32)&gt; . cnv.weights[0] ## (2,2)는 커널의 사이즈 // 3은 XXX의채널 // 4는 cnv(XXX)의 채널 . &lt;tf.Variable &#39;conv2d_10/kernel:0&#39; shape=(2, 2, 3, 4) dtype=float32, numpy= array([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673], [-0.30761963, 0.06209785, -0.07421023, 0.3941219 ], [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]], [[ 0.28664726, -0.0697116 , 0.09893084, -0.4221446 ], [-0.23161241, -0.16410837, -0.36420006, 0.12424195], [-0.14245945, 0.36286396, -0.10751781, 0.1733647 ]]], [[[ 0.02764335, 0.15547717, -0.42024496, -0.31893867], [ 0.22414821, 0.3619454 , -0.00282967, -0.3503708 ], [ 0.4610079 , -0.17417148, 0.00401336, -0.29777044]], [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ], [ 0.37925082, 0.24236053, 0.3949356 , -0.20996472], [-0.30264795, -0.28889188, -0.3237777 , 0.37506342]]]], dtype=float32)&gt; . cnv.weights[0][...,0] ## cnv(XXX)의 첫번째 채널결과를 얻기 위해서 사용하는 w . &lt;tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy= array([[[-0.06956434, -0.30761963, 0.16699678], [ 0.28664726, -0.23161241, -0.14245945]], [[ 0.02764335, 0.22414821, 0.4610079 ], [-0.1620284 , 0.37925082, -0.30264795]]], dtype=float32)&gt; . tf.reduce_sum(cnv.weights[0][...,0]) ### cnv(XXX)의 첫번째 채널의 결과 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.32976213&gt; . print(tf.reduce_sum(cnv.weights[0][...,0])) print(tf.reduce_sum(cnv.weights[0][...,1])) print(tf.reduce_sum(cnv.weights[0][...,2])) print(tf.reduce_sum(cnv.weights[0][...,3])) ### cnv(XXX)의 결과 . tf.Tensor(0.32976213, shape=(), dtype=float32) tf.Tensor(-0.44983464, shape=(), dtype=float32) tf.Tensor(-1.0487392, shape=(), dtype=float32) tf.Tensor(-1.5800952, shape=(), dtype=float32) . w_red = cnv.weights[0][...,0][...,0] w_green = cnv.weights[0][...,0][...,1] w_blue = cnv.weights[0][...,0][...,2] . tf.reduce_sum(XXX[...,0] * w_red + XXX[...,1] * w_green + XXX[...,2] * w_blue) ## cnv(XXX)의 첫채널 출력결과 . &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.32976213097572327&gt; . hw . 아래와 같은 흑백이미지가 있다고 하자. . 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 . 위의 이미지에 아래와 같은 weight를 가진 필터를 적용하여 convolution한 결과를 계산하라. (bias는 0으로 가정한다) . -1 1 -1 1 . cnv = tf.keras.layers.Conv2D(1,(2,2)) XXX = tf.constant([[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1]],dtype=tf.float64).reshape(1,6,6,1) XXX.reshape(1,6,6) . &lt;tf.Tensor: shape=(1, 6, 6), dtype=float64, numpy= array([[[0., 0., 0., 1., 1., 1.], [0., 0., 0., 1., 1., 1.], [0., 0., 0., 1., 1., 1.], [0., 0., 0., 1., 1., 1.], [0., 0., 0., 1., 1., 1.], [0., 0., 0., 1., 1., 1.]]])&gt; . cnv(XXX).reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy= array([[[0. , 0. , 1.4988861, 1.336632 , 1.336632 ], [0. , 0. , 1.4988861, 1.336632 , 1.336632 ], [0. , 0. , 1.4988861, 1.336632 , 1.336632 ], [0. , 0. , 1.4988861, 1.336632 , 1.336632 ], [0. , 0. , 1.4988861, 1.336632 , 1.336632 ]]], dtype=float32)&gt; . cnv.weights[0] . &lt;tf.Variable &#39;conv2d_11/kernel:0&#39; shape=(2, 2, 1, 1) dtype=float32, numpy= array([[[[-0.51669824]], [[ 0.6782736 ]]], [[[ 0.3544441 ]], [[ 0.8206125 ]]]], dtype=float32)&gt; . _w = tf.constant([[-1,1],[-1,1]],dtype=tf.float64).reshape(2,2,1,1) _b = tf.constant([0],dtype=tf.float64) . cnv.set_weights([_w,_b]) . cnv.weights . [&lt;tf.Variable &#39;conv2d_11/kernel:0&#39; shape=(2, 2, 1, 1) dtype=float32, numpy= array([[[[-1.]], [[ 1.]]], [[[-1.]], [[ 1.]]]], dtype=float32)&gt;, &lt;tf.Variable &#39;conv2d_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . cnv(XXX).reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy= array([[[0., 0., 2., 0., 0.], [0., 0., 2., 0., 0.], [0., 0., 2., 0., 0.], [0., 0., 2., 0., 0.], [0., 0., 2., 0., 0.]]], dtype=float32)&gt; .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-16%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(5%EC%9B%94-16%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "5주차-4월 04일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . #!conda install -c conda-forge python-graphviz -y . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#51032; &#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ . - 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음 . tf.keras.optimizers&#47484; &#51060;&#50857;&#54620; &#52572;&#51201;&#54868;&#48169;&#48277; . &#48169;&#48277;1: opt.apply_gradients()&#47484; &#51060;&#50857; . alpha= 0.01/6 . beta= tf.Variable(-10.0) . 2022-04-25 14:39:32.750112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . opt = tf.keras.optimizers.SGD(alpha) . - iter1 . with tf.GradientTape() as tape: tape.watch(beta) loss=(beta/2-1)**2 slope = tape.gradient(loss,beta) . opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . with tf.GradientTape() as tape: tape.watch(beta) loss=(beta/2-1)**2 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문으로 정리 . alpha= 0.01/6 beta= tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) . for epoc in range(10000): with tf.GradientTape() as tape: tape.watch(beta) loss=(beta/2-1)**2 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) beta . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . opt.apply_gradients()의 입력은 pair 의 list | . &#48169;&#48277;2: opt.minimize() . alpha= 0.01/6 beta= tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) . loss_fn = lambda: (beta/2-1)**2 . lambda x: x**2 &lt;=&gt; lambda(x)=x^2 | lambda x,y: x+y &lt;=&gt; lambda(x,y)=x+y | lambda: y &lt;=&gt; lambda()=y, 입력이 없으며 출력은 항상 y인 함수 | . loss_fn() # 입력은 없고 출력은 뭔가 계산되는 함수 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt; . - iter 1 . opt.minimize(loss_fn, beta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . opt.minimize(loss_fn, beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문으로 정리하면 . alpha= 0.01/6 beta= tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000): opt.minimize(loss_fn, beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . &#54924;&#44480;&#48516;&#49437; &#47928;&#51228; . - ${ bf y} approx 2.5 + 4.0 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 y = 2.5+4*x + epsilon y_true = 2.5+4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe34c0ffb20&gt;] . &#51060;&#47200;&#51201; &#54400;&#51060; . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . $S_{xx}=$, $S_{xy}=$ | $ hat{ beta}_0=$, $ hat{ beta}_1=$ | . - 풀이 . Sxx = sum((x-x.mean())**2) Sxy = sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta1_hat . &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168&gt; . beta0_hat = y.mean() - x.mean()*beta1_hat beta0_hat . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt; . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . - 포인트 . $ hat{ beta}=(X&#39;X)^{-1}X&#39;y$ | . - 풀이 . y=y.reshape(N,1) X=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . tf.linalg.inv(X.T @ X ) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49552;&#49892;&#54632;&#49688;&#51032; &#46020;&#54632;&#49688;&#51060;&#50857; . - 포인트 . $loss&#39;( beta)=-2X&#39;y +2X&#39;X beta$ | $ beta_{new} = beta_{old} - alpha times loss&#39;( beta_{old})$ | . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tnp.array([-5,10]).reshape(2,1) beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[-5], [10]])&gt; . slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-9.10036894], [-3.52886113]])&gt; . alpha= 0.1 . step = slope*alpha step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-0.91003689], [-0.35288611]])&gt; . for epoc in range(1000): slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat)/N beta_hat = beta_hat - alpha* slope . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . GradientTape&#47484; &#51060;&#50857; . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드1: 그레디언트 테입 with tf.GradientTape() as tape: loss = ## 포인트코드2: 미분 slope = tape.gradient(loss,beta_hat) ## 포인트코드3: update beta_hat.assign_sub(slope*alph) . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: yhat= X@beta_hat loss= (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) beta_hat.assign_sub(alpha*slope) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 미분 slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) . - 풀이 . y=y.reshape(-1) y.shape,x.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: yhat= beta0_hat + x*beta1_hat loss= tf.reduce_sum((y-yhat)**2)/N #loss= sum((y-yhat)**2)/N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) beta0_hat.assign_sub(alpha*slope0) beta1_hat.assign_sub(alpha*slope1) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . GradientTape + opt.apply_gradients . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope,beta_hat)]) ## pair의 list가 입력 . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat= X@beta_hat loss= (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) opt.apply_gradients([(slope,beta_hat)]) #beta_hat.assign_sub(alpha*slope) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 . - 풀이 . y=y.reshape(-1) y.shape,x.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat= beta0_hat + beta1_hat*x #X@beta_hat loss= tf.reduce_sum((y-yhat)**2) / N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . opt.minimize . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . loss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat) / N . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . - 풀이 . y=y.reshape(-1) y.shape,x.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . loss_fn = lambda: tf.reduce_sum((y-beta0_hat-beta1_hat*x )**2) / N . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#51687;&#51008;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): return ?? . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): return (y-X@beta_hat).T @ (y-X@beta_hat) / N . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#44596;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): ?? ?? return ?? . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): yhat= X@beta_hat # 컴퓨터한테 전달할 수식1 loss = (y-yhat).T @ (y-yhat) / N # 컴퓨터한테 전달할 수식 2 return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MSE . - 포인트 . ## 포인트코드: 미리구현되어있는 손실함수 이용 tf.losses.MSE(y,yhat) . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): yhat= X@beta_hat # 컴퓨터한테 전달할 수식1 loss = tf.keras.losses.MSE(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MeaSquaredError . - 포인트 . ## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . - 풀이 . mseloss_fn = tf.losses.MeanSquaredError() . mseloss_fn = tf.keras.losses.MSE 라고 보면된다. | . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): yhat= X@beta_hat # 컴퓨터한테 전달할 수식1 loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . tf.keras.Sequential . - $ hat{y}_i= hat{ beta}_0+ hat{ beta}_1x_i$ 의 서로다른 표현 . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta0_hat&quot;] &quot;x&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta1_hat&quot;] &quot;beta0_hat + x*beta1_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7fe34059cb50&gt; . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*beta1_hat, bias=True&quot;[label=&quot;*beta1_hat&quot;] ; &quot;x*beta1_hat, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7fe32c6f5030&gt; . gv(&#39;&#39;&#39; &quot;X=[1 x]&quot; -&gt; &quot;X@beta_hat, bias=False&quot;[label=&quot;@beta_hat&quot;] ; &quot;X@beta_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7fe32c59f520&gt; . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드1: 네트워크 생성 net = tf.keras.Sequential() ## 포인트코드2: 네트워크의 아키텍처 설계 net.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) ## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 net.compile(opt,loss=loss_fn2) ## 포인트코드4: 미분 &amp; update net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . - 풀이 . net = tf.keras.Sequential() . net.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) ## yhat을 구하는 방법정의 = 아키텍처가 설계 . units는 layer의 출력의 차원, 이 경우는 yhat의 차원, yhat은 (200,1) 이므로 1임. | input_shape는 layer의 입력의 차원, 이 경우는 X의 차원, X는 (200,2) 이므로 2임. | . def loss_fn2(y,yhat): return (y-yhat).T @ (y-yhat) / N . alpha=0.1 opt =tf.optimizers.SGD(alpha) . [np.array([[-5.0],[10.0]],dtype=np.float32)] . [array([[-5.], [10.]], dtype=float32)] . net.set_weights([np.array([[-5.0],[10.0]],dtype=np.float32)]) . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[-5.], [10.]], dtype=float32)&gt;] . net.compile(opt,loss=tf.losses.MSE) # 아키텍처 + 손실함수 + 옵티마이저 =&gt; 네트워크에 다 합치자 =&gt; 네트워크를 컴파일한다. . net.fit(X,y,epochs=1000,batch_size=N,verbose=0) # 미분 + 파라메터업데이트 = net.fit . &lt;keras.callbacks.History at 0x7fe2c4621ab0&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.58366 ], [3.933048]], dtype=float32)&gt;] .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(4%EC%9B%94-4%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(4%EC%9B%94-4%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "7주차-4월 18일",
            "content": "imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . piece-wise linear regression . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . np.random.seed(43052) N=100 x = np.linspace(-1,1,N) lamb = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3 y= np.array(list(map(lamb,x))) y . array([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569, -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671, -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354, -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 , -0.69867774, -0.34811037, 0.11188474, -1.05046296, -0.03840085, -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303, -0.529586 , -0.27814478, -0.10852781, -0.38294596, 0.02669763, -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793, -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763, 0.28917092, 0.18175206, -0.48988124, -0.08084459, 0.37706178, 0.14478908, 0.07621827, -0.071864 , 0.05143365, 0.33932009, -0.35071776, 0.87742867, 0.51370399, 0.34863976, 0.55855514, 1.14196717, 0.86421076, 0.72957843, 0.57342304, 1.54803332, 0.98840018, 1.11129366, 1.42410801, 1.44322465, 1.25926455, 1.12940772, 1.46516829, 1.16365096, 1.45560853, 1.9530553 , 2.45940445, 1.52921129, 1.8606463 , 1.86406718, 1.5866523 , 1.49033473, 2.35242686, 2.12246412, 2.41951931, 2.43615052, 1.96024441, 2.65843789, 2.46854394, 2.76381882, 2.78547462, 2.56568465, 3.15212157, 3.11482949, 3.17901774, 3.31268904, 3.60977818, 3.40949166, 3.30306495, 3.74590922, 3.85610433]) . plt.plot(x,y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3171333880&gt;] . &#54400;&#51060;1: &#45800;&#49692;&#54924;&#44480;&#47784;&#54805; . x= x.reshape(N,1) y= y.reshape(N,1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . 2022-04-25 14:04:34.246393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;keras.callbacks.History at 0x7f315c2d1630&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . yhat = x * 2.2616348 + 0.6069048 yhat = net.predict(x) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f315c102890&gt;] . - 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 . 왜? 아키텍처 설계자체가 틀렸음 | 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -&gt; under fit의 문제 | . &#54400;&#51060;2: &#48708;&#49440;&#54805; &#54876;&#49457;&#54868; &#54632;&#49688;&#51032; &#46020;&#51077; . - 여기에서 비선형 활성화 함수는 relu . - 네트워크를 아래와 같이 수정하자. . (수정전) hat은 생략 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;y&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f315c0af970&gt; . (수정후) hat은 생략 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;y&quot;[label=&quot;relu&quot;] &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f30f84d5a50&gt; . 마지막에 $f(x)=x$ 라는 함수대신에 relu를 취하는 것으로 구조를 약간 변경 | 활성화함수(acitivation function)를 indentity에서 relu로 변경 | . - relu함수란? . _x = np.linspace(-1,1,100) tf.nn.relu(_x) . &lt;tf.Tensor: shape=(100,), dtype=float64, numpy= array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909, 0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919, 0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929, 0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939, 0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949, 0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 , 0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 , 0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 , 0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 , 0.91919192, 0.93939394, 0.95959596, 0.97979798, 1. ])&gt; . plt.plot(_x,_x) plt.plot(_x,tf.nn.relu(_x)) . [&lt;matplotlib.lines.Line2D at 0x7f30f8353130&gt;] . 파란색을 주황색으로 바꿔주는 것이 렐루함수임 | $f(x)= max(0,x)= begin{cases} 0 &amp; x leq 0 x &amp; x&gt;0 end{cases}$ | . - 아키텍처: $ hat{y}_i=relu( hat{w}_0+ hat{w}_1x_i)$, $relu(x)= max(0,x)$ . - 풀이시작 . 1단계 . net2 = tf.keras.Sequential() . 2단계 . tf.random.set_seed(43053) l1 = tf.keras.layers.Dense(1, input_shape=(1,)) a1 = tf.keras.layers.Activation(tf.nn.relu) . net2.add(l1) . net2.layers . [&lt;keras.layers.core.dense.Dense at 0x7f30e0307820&gt;] . net2.add(a1) . net2.layers . [&lt;keras.layers.core.dense.Dense at 0x7f30e0307820&gt;, &lt;keras.layers.core.activation.Activation at 0x7f30e0305330&gt;] . l1.get_weights() . [array([[0.41721308]], dtype=float32), array([0.], dtype=float32)] . net2.get_weights() . [array([[0.41721308]], dtype=float32), array([0.], dtype=float32)] . (네트워크 상황 확인) . u1= l1(x) #u1= x@l1.weights[0] + l1.weights[1] . v1= a1(u1) #v1= tf.nn.relu(u1) . plt.plot(x,x) plt.plot(x,u1,&#39;--r&#39;) plt.plot(x,v1,&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30e033eb00&gt;] . 3단계 . net2.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) . 4단계 . net2.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f30e037bf70&gt; . - result . yhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1]) yhat = net2.predict(x) yhat = net2(x) yhat = a1(l1(x)) yhat = net2.layers[1](net2.layers[0](x)) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30e0222860&gt;] . - discussion . 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 $ to$ 모형의 표현력이 떨어진다. | 해결책: 주황색점선이 2개 있다면 어떨까? | . &#54400;&#51060;3: &#45432;&#46300;&#49688;&#52628;&#44032; + &#47112;&#51060;&#50612;&#52628;&#44032; . 목표: 2개의 주황색 점선을 만들자. . 1단계 . net3 = tf.keras.Sequential() . 2단계 . tf.random.set_seed(43053) l1 = tf.keras.layers.Dense(2,input_shape=(1,)) a1 = tf.keras.layers.Activation(tf.nn.relu) . net3.add(l1) net3.add(a1) . (네트워크 상황 확인) . l1(x).shape # l1(x) : (100,1) -&gt; (100,2) . TensorShape([100, 2]) . plt.plot(x,x) plt.plot(x,l1(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f315c28eb00&gt;, &lt;matplotlib.lines.Line2D at 0x7f315c28ed40&gt;] . plt.plot(x,x) plt.plot(x,a1(l1(x)),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30f853bd90&gt;, &lt;matplotlib.lines.Line2D at 0x7f30f853b820&gt;] . - 이 상태에서는 yhat이 안나온다. 왜? . 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. | 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat&gt;0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음. | . - 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -&gt; (N,1) 이 되도록! . yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1] | . - 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. . 입력차원: 2 | 출력차원: 1 | . net3.layers . [&lt;keras.layers.core.dense.Dense at 0x7f30e02223b0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3248129000&gt;] . tf.random.set_seed(43053) l2 = tf.keras.layers.Dense(1, input_shape=(2,)) . net3.add(l2) . net3.layers . [&lt;keras.layers.core.dense.Dense at 0x7f30e02223b0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3248129000&gt;, &lt;keras.layers.core.dense.Dense at 0x7f315c381000&gt;] . net3.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 2) 4 activation_1 (Activation) (None, 2) 0 dense_3 (Dense) (None, 1) 3 ================================================================= Total params: 7 Trainable params: 7 Non-trainable params: 0 _________________________________________________________________ . - 추정해야할 파라메터수가 4,0,3으로 나온다. . - 수식표현: $X to X@W^{(1)}+b^{(1)} to relu(X@W^{(1)}+b^{(1)}) to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat$ . $X$: (N,1) | $W^{(1)}$: (1,2) ==&gt; 파라메터 2개 추정 | $b^{(1)}$: (2,) ==&gt; 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개 | $W^{(2)}$: (2,1) ==&gt; 파라메터 2개 추정 | $b^{(2)}$: (1,) ==&gt; 파라메터 1개가 추가 // 따라서 3개 | . - 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. . 초거대AI: 추정할 파라메터수가 엄청 많은.. | . net3.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 0.34065306], [-0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . l1.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;] . l2.weights . [&lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 0.34065306], [-0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . - 좀 더 간단한 수식표현: $X to (u_1 to v_1) to (u_2 to v_2) = yhat$ . $u_1= X@W^{(1)}+b^{(1)}$ | $v_1= relu(u_1)$ | $u_2= v_1@W^{(2)}+b^{(2)}$ | $v_2= indentity(u_2):=yhat$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f32643d78e0&gt; . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f30e02a2230&gt; . 3단계 . net3.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.1)) . 4단계 . net3.fit(x,y,epochs=1000,verbose=0, batch_size=N) . &lt;keras.callbacks.History at 0x7f30e01a17b0&gt; . - 결과확인 . net3.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 1.6352793, -0.8550755]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(2,) dtype=float32, numpy=array([-0.0828446, 0.8555219], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.6328751], [-1.2001745]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(1,) dtype=float32, numpy=array([1.0253302], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c471c340&gt;] . - 분석 . plt.plot(x,y,&#39;.&#39;) plt.plot(x,l1(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c46fe0b0&gt;, &lt;matplotlib.lines.Line2D at 0x7f30c4763ee0&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,a1(l1(x)),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c47cf8b0&gt;, &lt;matplotlib.lines.Line2D at 0x7f30c47cfaf0&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,l2(a1(l1(x))),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c463f550&gt;] . - 마지막 2개의 그림을 분석 . l2.weights . [&lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.6328751], [-1.2001745]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(1,) dtype=float32, numpy=array([1.0253302], dtype=float32)&gt;] . fig, (ax1,ax2,ax3) = plt.subplots(1,3) fig.set_figwidth(12) ax1.plot(x,y,&#39;.&#39;) ax1.plot(x,a1(l1(x))[:,0],&#39;--r&#39;) ax1.plot(x,a1(l1(x))[:,1],&#39;--b&#39;) ax2.plot(x,y,&#39;.&#39;) ax2.plot(x,a1(l1(x))[:,0]*1.6328746,&#39;--r&#39;) ax2.plot(x,a1(l1(x))[:,1]*(-1.2001747)+1.0253307,&#39;--b&#39;) ax3.plot(x,y,&#39;.&#39;) ax3.plot(x,a1(l1(x))[:,0]*1.6328746+a1(l1(x))[:,1]*(-1.2001747)+1.0253307,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c4535000&gt;] . &#54400;&#51060;3&#51032; &#49892;&#54056; . tf.random.set_seed(43054) ## 1단계 net3 = tf.keras.Sequential() ## 2단계 net3.add(tf.keras.layers.Dense(2)) net3.add(tf.keras.layers.Activation(&#39;relu&#39;)) net3.add(tf.keras.layers.Dense(1)) ## 3단계 net3.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) ## 4단계 net3.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f30c45a7640&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c446aad0&gt;] . - 엥? 에폭이 부족한가? . net3.fit(x,y,epochs=10000,verbose=0,batch_size=N) plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c4366e00&gt;] . - 실패분석 . l1,a1,l2 = net3.layers . l2.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.65121335], [1.8592643 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_5/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) fig.set_figwidth(16) ax1.plot(x,y,&#39;.&#39;) ax1.plot(x,l1(x)[:,0],&#39;--r&#39;) ax1.plot(x,l1(x)[:,1],&#39;--b&#39;) ax2.plot(x,y,&#39;.&#39;) ax2.plot(x,a1(l1(x))[:,0],&#39;--r&#39;) ax2.plot(x,a1(l1(x))[:,1],&#39;--b&#39;) ax3.plot(x,y,&#39;.&#39;) ax3.plot(x,a1(l1(x))[:,0]*0.65121335,&#39;--r&#39;) ax3.plot(x,a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),&#39;--b&#39;) ax4.plot(x,y,&#39;.&#39;) ax4.plot(x,a1(l1(x))[:,0]*0.65121335+a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c4286380&gt;] . 보니까 빨간색선이 하는 역할을 없음 | 그런데 생각해보니까 이 상황에서는 빨간색선이 할수 있는 일이 별로 없음 | 왜? 지금은 나름 파란색선에 의해서 최적화가 된 상태임 $ to$ 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가) | 즉 이 상황 자체가 나름 최적회된 상태이다. 이러한 현상을 &quot;global minimum을 찾지 못하고 local minimum에 빠졌다&quot;라고 표현한다. | . 확인: . net3.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[-0.03077251, 1.8713338 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(2,) dtype=float32, numpy=array([-0.04834982, 0.3259186 ], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.65121335], [1.8592643 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_5/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . W1= tf.Variable(tnp.array([[-0.03077251, 1.8713338 ]])) b1= tf.Variable(tnp.array([-0.04834982, 0.3259186 ])) W2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]])) b2= tf.Variable(tnp.array([-0.60076195])) . with tf.GradientTape() as tape: u = tf.constant(x) @ W1 + b1 v = tf.nn.relu(u) yhat = v@W2 + b2 loss = tf.losses.mse(y,yhat) . tape.gradient(loss,[W1,b1,W2,b2]) . [&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])&gt;, &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ 0.00000000e+00], [-4.74910706e-05]])&gt;, &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])&gt;] . 예상대로 계수값이 거의 다 0이다. . &#54400;&#51060;4: &#45432;&#46300;&#49688;&#47484; &#45908; &#52628;&#44032;&#54620;&#45796;&#47732;? . - 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; &quot;X&quot; -&gt; &quot;...&quot; &quot;X&quot; -&gt; &quot;node512&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; &quot;...&quot; -&gt; &quot;yhat&quot; &quot;node512&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f30c40ff850&gt; . tf.random.set_seed(43056) net4= tf.keras.Sequential() net4.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) # 이렇게 해도됩니다. net4.add(tf.keras.layers.Dense(1)) net4.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.1)) net4.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f30bc70d9f0&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net4(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30c411ffd0&gt;] . 잘된다.. | 한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯! | . - 노드수가 많으면 무조건 좋다? -&gt; 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit) . np.random.seed(43052) N=100 _x = np.linspace(0,1,N).reshape(N,1) _y = np.random.normal(loc=0,scale=0.001,size=(N,1)) plt.plot(_x,_y) . [&lt;matplotlib.lines.Line2D at 0x7f30bc7bef20&gt;] . tf.random.set_seed(43052) net4 = tf.keras.Sequential() net4.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) net4.add(tf.keras.layers.Dense(1)) net4.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.5)) net4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f30bc7faec0&gt; . plt.plot(_x,_y) plt.plot(_x,net4(_x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30bc6d41c0&gt;] . 이 예제는 추후 다시 공부할 예정 | . Logistic regression . motive . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 올라간다. | . - 이러한 모형은 아래와 같이 설계할 수 있음 &lt;-- 외우세요!! . $y_i sim Ber( pi_i)$, where $ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i = frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss=- frac{1}{n} sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ . | . - 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자) . &#50696;&#51228; . N = 2000 . x = tnp.linspace(-1,1,N).reshape(N,1) w0 = -1 w1 = 5 u = w0 + x*w1 #v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi v = tf.nn.sigmoid(u) y = tf.constant(np.random.binomial(1,v),dtype=tf.float64) . plt.plot(x,y,&#39;.&#39;,alpha=0.02) plt.plot(x,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30bc505ff0&gt;] . - 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f30bc52afb0&gt; . - 또는 간단하게 아래와 같이 쓸 수 있다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; x label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; x -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: sigmoid&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f30bc55a7d0&gt; . - 케라스를 이용하여 적합을 해보면 . $loss=- frac{1}{n} sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ | . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) bceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat)) net.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f30bc488490&gt; . net.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[4.33877]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.8301144], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,v,&#39;--r&#39;) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30e012b8e0&gt;] . MSE loss? . - mse loss를 쓰면 왜 안되는지? . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) mseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2) net.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f30c419aa10&gt; . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,v,&#39;--r&#39;) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f30bc5e2aa0&gt;] . 일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임 | . MSE loss vs BCE loss . - MSEloss, BCEloss의 시각화 . w0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing=&#39;ij&#39;) w0, w1 = w0.reshape(-1), w1.reshape(-1) def loss_fn1(w0,w1): u = w0+w1*x yhat = np.exp(u)/(np.exp(u)+1) return mseloss_fn(y,yhat) def loss_fn2(w0,w1): u = w0+w1*x yhat = np.exp(u)/(np.exp(u)+1) return bceloss_fn(y,yhat) loss1 = list(map(loss_fn1,w0,w1)) loss2 = list(map(loss_fn2,w0,w1)) . fig = plt.figure() fig.set_figwidth(9) fig.set_figheight(9) ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 ax1.scatter(w0,w1,loss1,s=0.1) ax2.scatter(w0,w1,loss2,s=0.1) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f30bc226350&gt; . 왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -&gt; 오른쪽 곡면에서 더 학습이 잘될것 같음 | . &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;1 . - 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 89ms/step - loss: 0.1675 . &lt;keras.callbacks.History at 0x7f30bc293d30&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 97ms/step - loss: 0.8913 . &lt;keras.callbacks.History at 0x7f30bc103d60&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[-0.8381598], [ 1.1136571]], dtype=float32)], [array([[ 0.6664642], [-0.3024917]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-3.], [-1.]], dtype=float32)], [array([[-3.], [-1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;SGD, Winit=(-3,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;2 . - 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 103ms/step - loss: 0.1665 . &lt;keras.callbacks.History at 0x7f30bc190970&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 111ms/step - loss: 0.5997 . &lt;keras.callbacks.History at 0x7f309451c310&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[-0.9536586], [ 1.453682 ]], dtype=float32)], [array([[0.33676884], [0.9878915 ]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-3.], [-1.]], dtype=float32)], [array([[-3.], [-1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;Adam, Winit=(-3,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;3 . - 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 99ms/step - loss: 0.3884 . &lt;keras.callbacks.History at 0x7f308c765ba0&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 107ms/step - loss: 0.9309 . &lt;keras.callbacks.History at 0x7f30945c6530&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[ 0.8862327 ], [-0.39477217]], dtype=float32)], [array([[ 0.08077961], [-0.770376 ]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-10.], [ -1.]], dtype=float32)], [array([[-10.], [ -1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;Adam, Winit=(-10,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect 아무리 아담이라고 해도 이건 힘듬 | . - discussion . mse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음. | 근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함) | 좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2).그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3) | . - 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가? . mse loss를 사용하면 손실함수가 convex하지 않으니까! | 그리고 bce loss를 사용하면 손실함수가 convex하니까! | .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(4%EC%9B%94-18%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(4%EC%9B%94-18%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "6주차 4월11일",
            "content": "imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . $x to hat{y}$ &#44032; &#46104;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#44536;&#47532;&#44592; . - 단순회귀분석의 예시 . $ hat{y}_i = hat{ beta}_0 + hat{ beta}_1 x_i, quad i=1,2, dots,n$ | . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xₙ&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xₙ*β̂₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;identity&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₂&quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₂*β̂₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;identity&quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₁&quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₁*β̂₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f9cd4fa9540&gt; . - 표현1의 소감? . 교수님이 고생해서 만든것 같음 | 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음. | . (표현2) . - 그냥 아래와 같이 그리고 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot;고 하면 될것 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xᵢ&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f9cd49c2830&gt; . (표현3) . - 그런데 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot; 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x*β̂₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f9cd4a199f0&gt; . (표현4) . - 위의 모델은 아래와 같이 쓸 수 있다. ($ beta_0$를 바이어스로 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*β̂₁, bias=True&quot;[label=&quot;*β̂₁&quot;] ; &quot;x*β̂₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f9cd487dab0&gt; . 실제로는 이 표현을 많이 사용함 | . (표현5) . - 벡터버전으로 표현하면 아래와 같다. 이 경우에는 ${ bf X}=[1,x]$에 포함된 1이 bias의 역할을 해주므로 bias = False 임. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@β̂, bias=False&quot;[label=&quot;@β̂&quot;] ; &quot;X@β̂, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f9cd4885270&gt; . 저는 이걸 좋아해요 | . (표현5)&#39; . - 딥러닝에서는 $ hat{ boldsymbol{ beta}}$ 대신에 $ hat$을 라고 표현한다. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@Ŵ, bias=False&quot;[label=&quot;@Ŵ&quot;] ; &quot;X@Ŵ, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f9cd4751ff0&gt; . - 실제로는 표현4 혹은 표현5를 외우면 된다. . Layer&#51032; &#44060;&#45392; . - (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다. . - 레이어는 항상 아래와 같은 규칙을 가진다. . 첫 동그라미는 레이어의 입력이다. | 첫번째 화살표는 선형변환을 의미한다. | 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) | 두번째 화살표는 두번째 동그라미에 어떠한 함수 $f$를 취하는 과정을 의미한다. (우리의 그림에서는 $f(x)=x$) | 세번째 동그라미는 레이어의 최종출력이다. | . - 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. . 레이어의 입력차원 | 선형변환의 결과로 얻어지는 차원 | 선형변환에서 바이어스를 쓸지? 안쓸지? | 함수 $f$ | - 주목: 1,2가 결정되면 자동으로 $ hat$의 차원이 결정된다. . (예시) . 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: $ hat{ bf W}$는 (2,1) 매트릭스 | 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: $ hat{ bf W}$는 (20,5) 매트릭스 | 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: $ hat{ bf W}$는 (2,50) 매트릭스 | . - 주목2: 이중에서 절대 생략불가능 것은 &quot;2. 선형변환의 결과로 얻어지는 차원&quot; 이다. . 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 $ hat{ bf W}$의 차원을 결정할 수 있음. | 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. | 함수 $f$: 기본적으로 항등함수를 가정하면 된다. | . Keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; . - 기본뼈대: net생성 $ to$ add(layer) $ to$ compile(opt,loss) $ to$ fit(data,epochs) . - 데이터정리 . $${ bf y} approx 2.5 +4*x$$ . tnp.random.seed(43052) N= 200 x= tnp.linspace(0,1,N) epsilon= tnp.random.randn(N)*0.5 y= 2.5+4*x +epsilon . 2022-04-18 13:26:23.084963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7f9cd3fc3af0&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)&gt;] . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . (0단계) 데이터정리 . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7f9c6426a2c0&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#51104;&#49884;&#47928;&#48277;&#51221;&#47532; . - 잠깐 Dense layer를 만드는 코드를 정리해보자. . (1) 아래는 모두 같은 코드이다. . tf.keras.layers.Dense(1) | tf.keras.layers.Dense(units=1) | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;) // identity 가 더 맞는것 같은데.. | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;,use_bias=True) | . (2) 아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임) . tf.keras.layers.Dense(1,input_dim=2) # 코드1 | tf.keras.layers.Dense(1,input_shape=(2,)) # 코드2 | . (3) 아래는 사용불가능한 코드이다. . tf.keras.layers.Dense(1,input_dim=(2,)) # 코드1 | tf.keras.layers.Dense(1,input_shape=2) # 코드2 | . - 왜 input_dim이 필요한가? . net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Dense(1,use_bias=False)) . net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2)) . net1.weights . ValueError Traceback (most recent call last) Input In [26], in &lt;cell line: 1&gt;() -&gt; 1 net1.weights File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2542, in Model.weights(self) 2532 @property 2533 def weights(self): 2534 &#34;&#34;&#34;Returns the list of all layer variables/weights. 2535 2536 Note: This will not track the weights of nested `tf.Modules` that are not (...) 2540 A list of variables. 2541 &#34;&#34;&#34; -&gt; 2542 return self._dedup_weights(self._undeduplicated_weights) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2547, in Model._undeduplicated_weights(self) 2544 @property 2545 def _undeduplicated_weights(self): 2546 &#34;&#34;&#34;Returns the undeduplicated list of all layer variables/weights.&#34;&#34;&#34; -&gt; 2547 self._assert_weights_created() 2548 weights = [] 2549 for layer in self._self_tracked_trackables: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/sequential.py:471, in Sequential._assert_weights_created(self) 468 return 469 # When the graph has not been initialized, use the Model&#39;s implementation to 470 # to check if the weights has been created. --&gt; 471 super(functional.Functional, self)._assert_weights_created() File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2736, in Model._assert_weights_created(self) 2728 return 2730 if (&#39;build&#39; in self.__class__.__dict__ and 2731 self.__class__ != Model and 2732 not self.built): 2733 # For any model that has customized build() method but hasn&#39;t 2734 # been invoked yet, this will cover both sequential and subclass model. 2735 # Also make sure to exclude Model class itself which has build() defined. -&gt; 2736 raise ValueError(f&#39;Weights for model {self.name} have not yet been &#39; 2737 &#39;created. &#39; 2738 &#39;Weights are created when the Model is first called on &#39; 2739 &#39;inputs or `build()` is called with an `input_shape`.&#39;) ValueError: Weights for model sequential_2 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`. . net2.weights . [&lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[-1.053657 ], [ 1.3536845]], dtype=float32)&gt;] . net1.summary() . ValueError Traceback (most recent call last) Input In [28], in &lt;cell line: 1&gt;() -&gt; 1 net1.summary() File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2579, in Model.summary(self, line_length, positions, print_fn, expand_nested) 2559 &#34;&#34;&#34;Prints a string summary of the network. 2560 2561 Args: (...) 2576 ValueError: if `summary()` is called before the model is built. 2577 &#34;&#34;&#34; 2578 if not self.built: -&gt; 2579 raise ValueError( 2580 &#39;This model has not yet been built. &#39; 2581 &#39;Build the model first by calling `build()` or by calling &#39; 2582 &#39;the model on a batch of data.&#39;) 2583 layer_utils.print_summary( 2584 self, 2585 line_length=line_length, 2586 positions=positions, 2587 print_fn=print_fn, 2588 expand_nested=expand_nested) ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data. . net2.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . &#54400;&#51060;3: &#49828;&#52860;&#46972;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,input_dim=1) . net.add(layer) . . 초기값을 설정 . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[0.534932]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . net.get_weights() . [array([[0.534932]], dtype=float32), array([0.], dtype=float32)] . weight, bias순으로 출력 | . net.set_weights? . Signature: net.set_weights(weights) Docstring: Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer&#39;s weights must be instantiated before calling this function, by calling the layer. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) &gt;&gt;&gt; layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) &gt;&gt;&gt; layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights()) &gt;&gt;&gt; layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Args: weights: a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer&#39;s specifications. File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/base_layer.py Type: method . layer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군? | . - 한번따라해보자. . _w = net.get_weights() _w . [array([[0.534932]], dtype=float32), array([0.], dtype=float32)] . 길이가 2인 리스트이고, 각 원소는 numpy array 임 | . net.set_weights( [np.array([[10.0]],dtype=np.float32), # weight, β1_hat np.array([-5.0],dtype=np.float32)] # bias, β0_hat ) . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) . (4단계) net.fit() . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f9c642f0d30&gt; . 결과확인 . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;] . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) . net.add(layer) . . 초기값을 설정하자 . net.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)]) . net.get_weights() . [array([[-5.], [10.]], dtype=float32)] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f9c284f2ce0&gt; . net.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.58366 ], [3.933048]], dtype=float32)&gt;] . - 사실 실전에서는 초기값을 설정할 필요가 별로 없음. . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204; &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) . net.add(layer) . (3단계) net.compile() . loss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N . net.compile(tf.keras.optimizers.SGD(0.1), loss_fn) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f9c28350d90&gt; . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), loss=&#39;mse&#39;) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f9c2831eb90&gt; . net.weights . [&lt;tf.Variable &#39;dense_7/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;7: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; + &#50741;&#54000;&#47560;&#51060;&#51200; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;) #net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32) #net.optimizer.lr = 0.1 . (4단계) net.fit() . net.fit(X,y,epochs=5000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f9c28244b20&gt; . net.weights . [&lt;tf.Variable &#39;dense_8/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5842712], [3.9319096]], dtype=float32)&gt;] . &#50668;&#47084;&#44032;&#51648; &#54924;&#44480;&#47784;&#54805;&#51032; &#51201;&#54633;&#44284; &#54617;&#49845;&#44284;&#51221;&#51032; &#47784;&#45768;&#53552;&#47553; . &#50696;&#51228;1 . model: $y_i approx beta_0 + beta_1 x_i$ . np.random.seed(43052) N= 100 x= np.random.randn(N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X= np.stack([np.ones(N),x],axis=1) y= y.reshape(N,1) . plt.plot(x,y,&#39;o&#39;) # 관측한 자료 . [&lt;matplotlib.lines.Line2D at 0x7f9c2829e230&gt;] . beta_hat = np.array([-3,-2]).reshape(2,1) . yhat = X@beta_hat . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9c1870cb20&gt;] . 더 좋은 적합선을 얻기위해서! . slope = (2*X.T@X@beta_hat - 2*X.T@y)/ N beta_hat2 = beta_hat - 0.1*slope yhat2 = X@beta_hat2 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) plt.plot(x,yhat2.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9c284d9360&gt;] . 초록색이 좀 더 나아보인다. . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) / N beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , 7.12238255, -1.2575366 , 5.73166742, -0.1555309 , 4.86767499, 0.51106397, 4.36611576, 0.87316777, 4.12348617, 1.01165173, 4.07771926, 0.97282343, 4.19586617, 0.77814101, 4.46653491, 0.4299822 , 4.89562729, -0.08537358, 5.50446319, -0.79684366, 6.32975688, -1.74933031, 7.42517729, -3.00603683, 8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129], [-2. , 8.70824998, 0.16165717, 6.93399596, 1.62435964, 5.72089586, 2.63858056, 4.86387722, 3.37280529, 4.22385379, 3.94259478, 3.70397678, 4.43004465, 3.23363047, 4.89701606, 2.75741782, 5.39439054, 2.22728903, 5.96886945, 1.59655409, 6.66836857, 0.81489407, 7.54676324, -0.17628423, 8.66856437, -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]]) . b0hats = beta_hats[0].tolist() b1hats = beta_hats[1].tolist() . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.5451404 ], [3.94818596]]) . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*x) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*x) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;2 . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9c184ef700&gt;] . X= np.stack([np.ones(N),np.exp(-x)],axis=1) y= y.reshape(N,1) . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat.copy() # shallow copy, deep copy &lt; 여름 방학 특강 for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) /N beta_hat = beta_hat - 0.05*slope beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -1.74671631, -0.82428979, -0.14453919, 0.35720029, 0.72834869, 1.0036803 , 1.20869624, 1.36209751, 1.47759851, 1.56525696, 1.63244908, 1.68458472, 1.72563174, 1.75850062, 1.78532638, 1.80767543, 1.82669717, 1.84323521, 1.85790889, 1.8711731 , 1.88336212, 1.89472176, 1.90543297, 1.91562909, 1.92540859, 1.93484428, 1.94399023, 1.9528867 , 1.96156382], [-2. , -0.25663415, 1.01939241, 1.95275596, 2.63488171, 3.13281171, 3.49570765, 3.75961951, 3.95098231, 4.08918044, 4.18842797, 4.2591476 , 4.30898175, 4.34353413, 4.36691339, 4.38213187, 4.39139801, 4.39633075, 4.39811673, 4.3976256 , 4.3954946 , 4.3921905 , 4.38805511, 4.3833386 , 4.37822393, 4.37284482, 4.36729887, 4.36165718, 4.35597148, 4.35027923]]) . b0hats= beta_hats[0].tolist() b1hats= beta_hats[1].tolist() . np.linalg.inv(X.T@X)@X.T@y . array([[2.46307644], [3.99681332]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x)) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x)) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;3 . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9c1813e950&gt;] . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . beta_hat = np.array([-3,-2,-1]).reshape(3,1) beta_hats = beta_hat.copy() for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat -2*X.T@y) /N beta_hat = beta_hat - 0.1 * slope beta_hats= np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -0.71767532, 0.36255782, 0.89072137, 1.16423101, 1.31925078, 1.41819551, 1.48974454, 1.54713983, 1.59655416, 1.64091846, 1.68167278, 1.71956758, 1.75503084, 1.78833646, 1.81968188, 1.84922398, 1.877096 , 1.90341567, 1.92828934, 1.95181415, 1.97407943, 1.99516755, 2.01515463, 2.0341111 , 2.05210214, 2.06918818, 2.08542523, 2.10086524, 2.11555643], [-2. , 1.16947474, 2.64116513, 3.33411605, 3.66880042, 3.83768856, 3.92897389, 3.98315095, 4.01888831, 4.04486085, 4.06516144, 4.08177665, 4.09571971, 4.10754954, 4.1176088 , 4.12613352, 4.13330391, 4.13926816, 4.14415391, 4.14807403, 4.15112966, 4.1534121 , 4.15500404, 4.15598045, 4.15640936, 4.15635249, 4.15586584, 4.15500014, 4.15380139, 4.1523112 ], [-1. , -0.95492718, -0.66119313, -0.27681968, 0.12788212, 0.52254445, 0.89491388, 1.24088224, 1.55993978, 1.85310654, 2.12199631, 2.36839745, 2.59408948, 2.8007666 , 2.99000967, 3.16327964, 3.32192026, 3.46716468, 3.60014318, 3.72189116, 3.83335689, 3.93540864, 4.02884144, 4.11438316, 4.19270026, 4.26440288, 4.33004965, 4.39015202, 4.44517824, 4.49555703]]) . b0hats,b1hats,b2hats = beta_hats . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.46597526], [4.00095138], [5.04161877]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x)) # ax2: 오른쪽그림 # β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) # β0=β0.reshape(-1) # β1=β1.reshape(-1) # loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) # loss = list(map(loss_fn, β0,β1)) # ax2.scatter(β0,β1,loss,alpha=0.02) # ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x)) # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;3: &#52992;&#46972;&#49828;&#47196; &#54644;&#48372;&#51088;! . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . net = tf.keras.Sequential() # 1: 네트워크 생성 net.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) # 3: compile net.fit(X,y,epochs=30, batch_size=N) # 4: fit . Epoch 1/30 1/1 [==============================] - 0s 75ms/step - loss: 82.1027 Epoch 2/30 1/1 [==============================] - 0s 1ms/step - loss: 23.9512 Epoch 3/30 1/1 [==============================] - 0s 1ms/step - loss: 10.7256 Epoch 4/30 1/1 [==============================] - 0s 1ms/step - loss: 7.0664 Epoch 5/30 1/1 [==============================] - 0s 1ms/step - loss: 5.5521 Epoch 6/30 1/1 [==============================] - 0s 1ms/step - loss: 4.6075 Epoch 7/30 1/1 [==============================] - 0s 2ms/step - loss: 3.8836 Epoch 8/30 1/1 [==============================] - 0s 1ms/step - loss: 3.2909 Epoch 9/30 1/1 [==============================] - 0s 2ms/step - loss: 2.7971 Epoch 10/30 1/1 [==============================] - 0s 1ms/step - loss: 2.3838 Epoch 11/30 1/1 [==============================] - 0s 1ms/step - loss: 2.0374 Epoch 12/30 1/1 [==============================] - 0s 1ms/step - loss: 1.7471 Epoch 13/30 1/1 [==============================] - 0s 1ms/step - loss: 1.5038 Epoch 14/30 1/1 [==============================] - 0s 985us/step - loss: 1.2998 Epoch 15/30 1/1 [==============================] - 0s 1ms/step - loss: 1.1288 Epoch 16/30 1/1 [==============================] - 0s 1ms/step - loss: 0.9854 Epoch 17/30 1/1 [==============================] - 0s 1ms/step - loss: 0.8652 Epoch 18/30 1/1 [==============================] - 0s 1ms/step - loss: 0.7645 Epoch 19/30 1/1 [==============================] - 0s 1ms/step - loss: 0.6800 Epoch 20/30 1/1 [==============================] - 0s 1ms/step - loss: 0.6092 Epoch 21/30 1/1 [==============================] - 0s 1ms/step - loss: 0.5499 Epoch 22/30 1/1 [==============================] - 0s 1ms/step - loss: 0.5001 Epoch 23/30 1/1 [==============================] - 0s 1ms/step - loss: 0.4584 Epoch 24/30 1/1 [==============================] - 0s 1ms/step - loss: 0.4234 Epoch 25/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3941 Epoch 26/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3695 Epoch 27/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3489 Epoch 28/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3316 Epoch 29/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3171 Epoch 30/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3050 . &lt;keras.callbacks.History at 0x7f9c06753a60&gt; . net.weights . [&lt;tf.Variable &#39;dense_9/kernel:0&#39; shape=(3, 1) dtype=float32, numpy= array([[2.4857023], [3.925291 ], [4.6923084]], dtype=float32)&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@net.weights).reshape(-1),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9c1818be50&gt;] . &#49689;&#51228; . &#50696;&#51228;2: &#52992;&#46972;&#49828;&#47484; &#51060;&#50857;&#54616;&#50668; &#50500;&#47000;&#47484; &#47564;&#51313;&#54616;&#45716; &#51201;&#51208;&#54620; $ beta_0$&#50752; $ beta_1$&#51012; &#44396;&#54616;&#46972;. &#51201;&#54633;&#44208;&#44284;&#47484; &#49884;&#44033;&#54868;&#54616;&#46972;. (&#50528;&#45768;&#47700;&#51060;&#49496; &#49884;&#44033;&#54868; X) . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . X = np.stack([np.ones(N),np.exp(-x)],axis=1) y = y.reshape(N,1) . net = tf.keras.Sequential() # 1: 네트워크 생성 net.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) # 3: compile net.fit(X,y,epochs=30, batch_size=N) # 4: fit . Epoch 1/30 1/1 [==============================] - 0s 76ms/step - loss: 64.1211 Epoch 2/30 1/1 [==============================] - 0s 1ms/step - loss: 14.2919 Epoch 3/30 1/1 [==============================] - 0s 903us/step - loss: 3.4335 Epoch 4/30 1/1 [==============================] - 0s 910us/step - loss: 1.0607 Epoch 5/30 1/1 [==============================] - 0s 1ms/step - loss: 0.5360 Epoch 6/30 1/1 [==============================] - 0s 1ms/step - loss: 0.4142 Epoch 7/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3806 Epoch 8/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3667 Epoch 9/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3575 Epoch 10/30 1/1 [==============================] - 0s 2ms/step - loss: 0.3497 Epoch 11/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3427 Epoch 12/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3361 Epoch 13/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3300 Epoch 14/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3242 Epoch 15/30 1/1 [==============================] - 0s 991us/step - loss: 0.3189 Epoch 16/30 1/1 [==============================] - 0s 1ms/step - loss: 0.3139 Epoch 17/30 1/1 [==============================] - 0s 991us/step - loss: 0.3092 Epoch 18/30 1/1 [==============================] - 0s 980us/step - loss: 0.3048 Epoch 19/30 1/1 [==============================] - 0s 998us/step - loss: 0.3007 Epoch 20/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2969 Epoch 21/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2933 Epoch 22/30 1/1 [==============================] - 0s 946us/step - loss: 0.2900 Epoch 23/30 1/1 [==============================] - 0s 968us/step - loss: 0.2869 Epoch 24/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2840 Epoch 25/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2813 Epoch 26/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2787 Epoch 27/30 1/1 [==============================] - 0s 979us/step - loss: 0.2763 Epoch 28/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2741 Epoch 29/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2720 Epoch 30/30 1/1 [==============================] - 0s 1ms/step - loss: 0.2701 . &lt;keras.callbacks.History at 0x7f9c0665e590&gt; .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(4%EC%9B%94-11%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(4%EC%9B%94-11%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "1주차-3월 07일",
            "content": "&#47196;&#46300;&#47605; . - 오늘수업할내용: 단순선형회귀 . - 단순선형회귀를 배우는 이유? . 우리가 배우고싶은것: 심층신경망(DNN) $ to$ 합성곱신경망(CNN) $ to$ 적대적생성신경망(GAN) | 심층신경망을 바로 이해하기 어려움 | 다음의 과정으로 이해해야함: (선형대수학 $ to$) 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 | . &#49440;&#54805;&#54924;&#44480; . - 상황극 . 나는 동네에 커피점을 하나 차렸음. | 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. | 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 &#39;온도 -&gt; 아이스아메리카노 판매량 예측&#39; 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능) | . - 가짜자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf . 온도 ${ bf x}$가 아래와 같다고 하자. . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . 아이스아메리카노의 판매량 ${ bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) $${ bf y} approx 10.2 +2.2 { bf x}$$ . 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 | 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 | 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음. | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - 우리는 아래와 같은 자료를 모았다고 생각하자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . - 그려보자. . plt.plot(x,y,&#39;.&#39;) # 파란점, 관측한 데이터 plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # 주황색점선, 세상의 법칙 . [&lt;matplotlib.lines.Line2D at 0x7fc3f462c910&gt;] . - 우리의 목표: 파란색점 $ to$ 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론 . - 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2, dots, 10$에 대하여 아래를 만족하는 적당한 a,b (혹은 $ beta_0, beta_1$) 가 존재할것 같다. . $y_{i} approx ax_{i}+b$ | $y_{i} approx beta_1 x_{i}+ beta_0$ | . - 어림짐작으로 $a,b$를 알아내보자. . 데이터를 살펴보자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . 적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다. . 따라서 $a=2, b=15$ 혹은 $ beta_0=15, beta_1=2$ 로 추론할 수 있겠다. . - 누군가가 $( beta_0, beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) . - 새로운 주장으로 인해서 $( beta_0, beta_1)=(15,2)$ 로 볼 수도 있고 $( beta_0, beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? . 후보1: $( beta_0, beta_1)=(15,2)$ | 후보2: $( beta_0, beta_1)=(14,2)$ | . - 가능한 $y_i approx beta_0 + beta_1 x_i$ 이 되도록 만드는 $( beta_0, beta_1)$ 이 좋을 것이다. $ to$ 후보 1,2를 비교해보자. . (관찰에 의한 비교) . 후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . 후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. . (좀 더 체계적인 비교) . $i=1,2,3, dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. . 후보 1,2에 대하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521086&gt;) . 후보1이 더 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$의 값이 작다. . 후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. . - 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $ beta_0, beta_1$을 찾으면 된다. . $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ . 그런데 결국 $ beta_0, beta_1$에 대한 이차식인데 이 식을 최소화하는 $ beta_0, beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다 . $ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$ . - 풀어보자. . $ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$ . 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - 따라서 최적의 추정치 $( hat{ beta}_0, hat{ beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.84898&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;) # 주황색선: 세상의 법칙을 추정한선 plt.plot(x,10.2 + 2.2* x, &#39;--&#39;) # 초록색선: ture, 세상의법칙 . [&lt;matplotlib.lines.Line2D at 0x7fc3f465ee00&gt;] . . Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. - 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. . (1) 공식이 좀 복잡함.. . (2) $x$가 여러개일 경우 확장이 어려움 . - 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. . - 모형의 매트릭스화 . 우리의 모형은 아래와 같다. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같이 쓸 수 있다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 정리하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이것을 벡터표현으로 하면 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . 풀어보면 . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 공식도 매트릭스로 표현하면: $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ &lt;-- 외우세요 . - 적용을 해보자. . (X를 만드는 방법1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (X를 만드는 방법2) . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X=tf.concat([[[1.0]*10],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . InternalError Traceback (most recent call last) Input In [66], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(X.T @ X) @ X.T @ y File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InternalError: Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul] . - 잘 구해진다. . - 그런데.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . 값이 좀 다르다..? . - 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. . import tensorflow.experimental.numpy as tnp . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573243234018&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.215704607783491&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . InternalError Traceback (most recent call last) Input In [72], in &lt;cell line: 2&gt;() 1 X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T -&gt; 2 tf.linalg.inv(X.T @ X) @ X.T @ y File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InternalError: Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul] . &#50526;&#51004;&#47196; &#54624;&#44163; . - 선형대수학의 미분이론.. . - 실습 (tensorflow에서 매트릭스를 자유롭게 다루기) .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-7%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-7%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "4주차-3월 28일",
            "content": "imports . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . 예제9 : 카페예제로 돌아오자. | . x = tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) . 2022-04-04 20:55:58.466144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon . y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])&gt; . beta0 = tf.Variable(9.0) beta1 = tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape: loss = sum((y-beta0-beta1*x)**2) . tape.gradient(loss, beta0), tape.gradient(loss, beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-126.78691&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3208.8396&gt;) . 예제10:카페예제의 매트릭스 버전 | . X = tnp.array([1]*10 +[20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta = tnp.array([9.0,2.0]).reshape(2,1) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . X@beta . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[49.2], [53.4], [54.4], [55.6], [57.8], [59.2], [61.4], [63.6], [65.8], [69.8]])&gt; . beta_true = tnp.array([10.2,2.2]).reshape(2,1) y = X@beta_true+epsilon.reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[55.4183651 ], [58.19427589], [61.23082496], [62.31255873], [63.1070028 ], [63.69569103], [67.24704918], [71.43650092], [73.10130336], [77.84988286]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) yhat= X@beta loss= (y-yhat).T @(y-yhat) . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -126.78690968], [-3208.83947922]])&gt; . 이론적인 값을 확인하면 | . -2*X.T @ y + 2*X.T@X@beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -126.78690968], [-3208.83947922]])&gt; . 예제11 : 위의 예제에서 이론적인 $ beta$의 최적값을 찾아보고 (즉 $ hat beta$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 0인지 확인하라. (단 0은 길이가 2이고 각 원소가 0인 벡터) | . $ beta$의 최적값은 $(X&#39;X)^{-1}X&#39;y$이다. . beta_optimal = tf.linalg.inv(X.T @ X) @ X.T @y . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_optimal) yhat= X@beta_optimal loss= (y-yhat).T @(y-yhat) . tape.gradient(loss,beta_optimal) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-5.57065505e-12], [-1.40943257e-10]])&gt; . - beta_true에서의 기울기도 계산해보자. . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_true) yhat= X@beta_true loss= (y-yhat).T @(y-yhat) . tape.gradient(loss,beta_true) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -2.74690968], [-71.45947922]])&gt; . 샘플사이즈가 커진다면 tape.gradient(loss,beta_true)$ approx$ tape.gradient(loss,beta_optimal) | 샘플사이즈가 커진다면 beta_true $ approx$ beta_optimal | . &#44221;&#49324;&#54616;&#44053;&#48277; . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss = ( frac{1}{2} beta-1)^2$를 최소하는 $ beta$를 컴퓨터를 활용하여 구하는 문제를 생각해보자. . 답은 $ beta = 2$임을 알고 있다. | . &#48169;&#48277;1 : grid search . &#50508;&#44256;&#47532;&#51608; . (1) beta = [-10.00,-9.99,...,10.00] 와 같은 리스트를 만든다. . (2) (1)의 리스트의 각원소에 해당하는 loss를 구한다. . (3) (2)에서 구한 loss를 제일 작게 만드는 beta를 찾는다. . &#44396;&#54788;&#53076;&#46300; . beta = np.linspace(-10,10,100) loss = (beta/2-1)**2 . tnp.argmin([1,2,-3,3,4]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt; . tnp.argmin([1,2,3,-3,4]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt; . beta[59] . 1.9191919191919187 . (beta[60]/2-1)**2 . 0.0036730945821854847 . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . &#48169;&#48277;2:gradient descent . &#50508;&#44256;&#47532;&#51608;! . (1) beta = -5로 셋팅한다. (초깃값으로 셋팅) . (-5/2-1)**2 . 12.25 . (2) beta= -5 근처에서 조금씩 이동하여 loss를 조사해본다. . (-4.99/2-1)**2 ## 오른쪽으로 0.01 이동하고 loss조사 (미분) . 12.215025 . (-5.01/2-1)**2 ## 왼쪽으로 0.01 이동하고 loss조사 (미분) . 12.285025 . (3) (2)의 결과를 잘 해석하고 더 유리한 쪽으로 이동 (미분 결과 관찰 후 유리한 쪽으로 이동) . (4) 위의 과정을 반복하고 왼쪽, 오른쪽 어느쪽으로 움직여도 이득이 없다면 멈춘다. . &#50508;&#44256;&#47532;&#51608; &#48516;&#49437; . - (2)-(3)의 과정은 beta=-5 에서 미분계수를 구하고 미분계수가 양수이면 왼쪽으로 움직이고 음수면 오른쪽으로 움직인다고 해석가능. 아래그림을 보면 더 잘 이해가 된다. . plt.plot(beta,loss) . [&lt;matplotlib.lines.Line2D at 0x7f10180e2560&gt;] . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . - 아래와 같이 해석가능 . 오른쪽으로 0.01 간다 = beta_old에 0.01을 더함. (미분계수가 음수이면) | 왼쪽으로 0.01 간다 = beta_old에 0.01을 뺀다. (미분계수가 양수이면) | . - 그렇다면 . $ beta_{new} = begin{cases} beta_{old} + 0.01, &amp; loss&#39;( beta_{old})&lt;0 beta_{old} - 0.01, &amp; loss&#39;( beta_{old})&gt;0 end{cases} $ . &#54841;&#49884; &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624; &#49688; &#51080;&#51012;&#44620;? . - 항상 0.01씩 움직여야 하는가? . plt.plot(beta,loss) . [&lt;matplotlib.lines.Line2D at 0x7f10107a4430&gt;] . - $ beta= -10$일 경우의 접선의 기울기? $ beta=-4$일때 접선의 기울기? . $ beta=-10$ =&gt; 기울기는 -6 | $ beta=-4$ =&gt; 기울기는 -3 | . - 실제로 6,3씩 이동할수는 없으니 적당한 $ alpha(예를 들면 alpha = 0.01)$를 잡아서 곱한만큼 이동하자. . - 수식화하면 . $ beta_{new} = beta_{old} - alpha~ loss&#39;( beta_{old})$ | $ beta_{new} = beta_{old} - alpha~ left[ frac{ partial}{ partial beta}loss( beta) right]_{ beta= beta_{old}}$ | . - $ alpha$의 의미 . $ alpha$가 크면 크게크게 움직이고 작으면 작게작게 움직인다. | $ alpha&gt;0$ 이어야 한다. | . &#44396;&#54788;&#53076;&#46300; . - iter 1 . - $ beta=-10$이라고 하자. . beta = tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . $ beta = -10$에서 0.01만큼 움직이고 싶다. . alpha = 0.01/6 . alpha * tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.01&gt; . beta.assign_sub(alpha * tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . -iter 2 . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . beta.assign_sub(tape.gradient(loss,beta)*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for 문을 이용하자. . (강의용) . beta = tf.Variable(-10.0) . for k in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.997125&gt; . (시도 1) . beta = tf.Variable(-10.0) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . (시도 2) . beta = tf.Variable(-10.0) . for k in range(1000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-3.2133687&gt; . - 너무 느린 것 같다? $ to$ $ alpha$를 키워보자! . &#54617;&#49845;&#47456; . - 목표 : $ alpha$에 따라서 수렴과정이 어떻게 달라지는지 시각화해보자. . [시각화 코드 예비학습] . fig = plt.figure() # 도화지가 만들어지고 fig라는 이름을 붙인다. . &lt;Figure size 432x288 with 0 Axes&gt; . fig . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() #fig는 ax라는 물체를 만든다. . id(fig.axes[0]) . 139706970921024 . id(ax) . 139706970921024 . pnts, = ax.plot([1,2,3],[4,5,6],&#39;or&#39;) pnts . &lt;matplotlib.lines.Line2D at 0x7f101062b670&gt; . pnts.get_xdata() . array([1, 2, 3]) . pnts.get_ydata() . array([4, 5, 6]) . fig . pnts.set_ydata([5,5,5]) . pnts.get_ydata() . [5, 5, 5] . fig . -응용 . plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; from matplotlib import animation . def animate(i): if i%2 == 0: pnts.set_ydata([4,5,6]) else: pnts.set_ydata([5,5,5]) . ani=animation.FuncAnimation(fig,animate,frames=10) ani . &lt;/input&gt; Once Loop Reflect 예비학습 끝! . - beta_lst = [-10,-9,-8] 로 이동한다고 하자. . beta_lst = [-10,-9,-8] loss_lst = [(-10/2-1)**2, (-9/2-1)**2, (-8/2-1)**2] . fig = plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() . _beta = np.linspace(-15,19,100) . ax.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f1010504ee0&gt;] . fig . pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;ro&#39;) fig . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . ani = animation.FuncAnimation(fig, animate, frames=3) ani . &lt;/input&gt; Once Loop Reflect -최종아웃풋 . beta = tf.Variable(-10.0) alpha = 0.01/6 . beta_lst = [] loss_lst = [] . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) loss = (beta/2-1)**2 . beta.assign_sub(tape.gradient(loss,beta)*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . beta_lst, loss_lst . ([-10.0, -9.99], [36.0, 35.94002362785341]) . - for . beta = tf.Variable(-10.0) alpha = 0.01/6 beta_lst = [] loss_lst = [] beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) for k in range(100): with tf.GradientTape(persistent=True) as tape: tape.watch(beta) loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts,= ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani = animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect &#44284;&#51228; . - y=(x-1)^2을 최소화하는 x를 확률적 경사하강법으로 찾고 애니메이션으로 찾고 시각화할것 . beta = tf.Variable(-3.0) alpha = 0.01/3 beta_lst=[] loss_lst=[] beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()-1)**2) for k in range(100): with tf.GradientTape(persistent=True) as tape: tape.watch(beta) loss = (beta-1)**2 beta.assign_sub(tape.gradient(loss, beta)*alpha) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()-1)**2) . _beta = np.linspace(-20,22,100) . fig = plt.figure() ax = fig.add_subplot() ax.plot(_beta,(_beta-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . ani = animation.FuncAnimation(fig, animate, frames=100) ani . &lt;/input&gt; Once Loop Reflect",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-28%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-28%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "3주차-3월 21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . 2022-04-25 14:41:14.910157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . - max, min, sum, mean . a= tf.constant([1.0,2.0,3.0,4.0]) a . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . tf.reduce_mean(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt; . concat, stack . - 예제: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?) . a = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b = -a . case1 (1,2,3,4,5) stack (1,2,3,4,5) --&gt; (2,2,3,4,5) # axis=0 . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]]], [[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case2 (2,1,3,4,5) stack (2,1,3,4,5) --&gt; (2,2,3,4,5) # axis=1 . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case3 (2,3,1,4,5) stack (2,3,1,4,5) --&gt; (2,3,2,4,5) # axis=2 . tf.stack([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]]], [[[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]]], [[[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]]], [[[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]]], [[[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case4 (2,3,4,1,5) stack (2,3,4,1,5) --&gt; (2,3,4,2,5) # axis=3 . tf.stack([a,b],axis=-2) . &lt;tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 0, -1, -2, -3, -4]], [[ 5, 6, 7, 8, 9], [ -5, -6, -7, -8, -9]], [[ 10, 11, 12, 13, 14], [ -10, -11, -12, -13, -14]], [[ 15, 16, 17, 18, 19], [ -15, -16, -17, -18, -19]]], [[[ 20, 21, 22, 23, 24], [ -20, -21, -22, -23, -24]], [[ 25, 26, 27, 28, 29], [ -25, -26, -27, -28, -29]], [[ 30, 31, 32, 33, 34], [ -30, -31, -32, -33, -34]], [[ 35, 36, 37, 38, 39], [ -35, -36, -37, -38, -39]]], [[[ 40, 41, 42, 43, 44], [ -40, -41, -42, -43, -44]], [[ 45, 46, 47, 48, 49], [ -45, -46, -47, -48, -49]], [[ 50, 51, 52, 53, 54], [ -50, -51, -52, -53, -54]], [[ 55, 56, 57, 58, 59], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ -60, -61, -62, -63, -64]], [[ 65, 66, 67, 68, 69], [ -65, -66, -67, -68, -69]], [[ 70, 71, 72, 73, 74], [ -70, -71, -72, -73, -74]], [[ 75, 76, 77, 78, 79], [ -75, -76, -77, -78, -79]]], [[[ 80, 81, 82, 83, 84], [ -80, -81, -82, -83, -84]], [[ 85, 86, 87, 88, 89], [ -85, -86, -87, -88, -89]], [[ 90, 91, 92, 93, 94], [ -90, -91, -92, -93, -94]], [[ 95, 96, 97, 98, 99], [ -95, -96, -97, -98, -99]]], [[[ 100, 101, 102, 103, 104], [-100, -101, -102, -103, -104]], [[ 105, 106, 107, 108, 109], [-105, -106, -107, -108, -109]], [[ 110, 111, 112, 113, 114], [-110, -111, -112, -113, -114]], [[ 115, 116, 117, 118, 119], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case5 (2,3,4,5,1) stack (2,3,4,5,1) --&gt; (2,3,4,5,2) # axis=4 . tf.stack([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy= array([[[[[ 0, 0], [ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], [[ 5, -5], [ 6, -6], [ 7, -7], [ 8, -8], [ 9, -9]], [[ 10, -10], [ 11, -11], [ 12, -12], [ 13, -13], [ 14, -14]], [[ 15, -15], [ 16, -16], [ 17, -17], [ 18, -18], [ 19, -19]]], [[[ 20, -20], [ 21, -21], [ 22, -22], [ 23, -23], [ 24, -24]], [[ 25, -25], [ 26, -26], [ 27, -27], [ 28, -28], [ 29, -29]], [[ 30, -30], [ 31, -31], [ 32, -32], [ 33, -33], [ 34, -34]], [[ 35, -35], [ 36, -36], [ 37, -37], [ 38, -38], [ 39, -39]]], [[[ 40, -40], [ 41, -41], [ 42, -42], [ 43, -43], [ 44, -44]], [[ 45, -45], [ 46, -46], [ 47, -47], [ 48, -48], [ 49, -49]], [[ 50, -50], [ 51, -51], [ 52, -52], [ 53, -53], [ 54, -54]], [[ 55, -55], [ 56, -56], [ 57, -57], [ 58, -58], [ 59, -59]]]], [[[[ 60, -60], [ 61, -61], [ 62, -62], [ 63, -63], [ 64, -64]], [[ 65, -65], [ 66, -66], [ 67, -67], [ 68, -68], [ 69, -69]], [[ 70, -70], [ 71, -71], [ 72, -72], [ 73, -73], [ 74, -74]], [[ 75, -75], [ 76, -76], [ 77, -77], [ 78, -78], [ 79, -79]]], [[[ 80, -80], [ 81, -81], [ 82, -82], [ 83, -83], [ 84, -84]], [[ 85, -85], [ 86, -86], [ 87, -87], [ 88, -88], [ 89, -89]], [[ 90, -90], [ 91, -91], [ 92, -92], [ 93, -93], [ 94, -94]], [[ 95, -95], [ 96, -96], [ 97, -97], [ 98, -98], [ 99, -99]]], [[[ 100, -100], [ 101, -101], [ 102, -102], [ 103, -103], [ 104, -104]], [[ 105, -105], [ 106, -106], [ 107, -107], [ 108, -108], [ 109, -109]], [[ 110, -110], [ 111, -111], [ 112, -112], [ 113, -113], [ 114, -114]], [[ 115, -115], [ 116, -116], [ 117, -117], [ 118, -118], [ 119, -119]]]]], dtype=int32)&gt; . - 예제: (2,3,4), (2,3,4), (2,3,4) . a= tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b= -a c= 2*a . (예시1) (2,3,4), (2,3,4), (2,3,4) $ to$ (6,3,4) . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시2) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,9,4) . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11], [ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23], [ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시3) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,12) . tf.concat([a,b,c],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 0, -1, -2, -3, 0, 2, 4, 6], [ 4, 5, 6, 7, -4, -5, -6, -7, 8, 10, 12, 14], [ 8, 9, 10, 11, -8, -9, -10, -11, 16, 18, 20, 22]], [[ 12, 13, 14, 15, -12, -13, -14, -15, 24, 26, 28, 30], [ 16, 17, 18, 19, -16, -17, -18, -19, 32, 34, 36, 38], [ 20, 21, 22, 23, -20, -21, -22, -23, 40, 42, 44, 46]]], dtype=int32)&gt; . (예시4) (2,3,4), (2,3,4), (2,3,4) $ to$ (3,2,3,4) . tf.stack([a,b,c],axis=0) . &lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]], [[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], [[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시5) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시6) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 0, -1, -2, -3], [ 0, 2, 4, 6]], [[ 4, 5, 6, 7], [ -4, -5, -6, -7], [ 8, 10, 12, 14]], [[ 8, 9, 10, 11], [ -8, -9, -10, -11], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [-12, -13, -14, -15], [ 24, 26, 28, 30]], [[ 16, 17, 18, 19], [-16, -17, -18, -19], [ 32, 34, 36, 38]], [[ 20, 21, 22, 23], [-20, -21, -22, -23], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시7) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,4,3) . tf.stack([a,b,c],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy= array([[[[ 0, 0, 0], [ 1, -1, 2], [ 2, -2, 4], [ 3, -3, 6]], [[ 4, -4, 8], [ 5, -5, 10], [ 6, -6, 12], [ 7, -7, 14]], [[ 8, -8, 16], [ 9, -9, 18], [ 10, -10, 20], [ 11, -11, 22]]], [[[ 12, -12, 24], [ 13, -13, 26], [ 14, -14, 28], [ 15, -15, 30]], [[ 16, -16, 32], [ 17, -17, 34], [ 18, -18, 36], [ 19, -19, 38]], [[ 20, -20, 40], [ 21, -21, 42], [ 22, -22, 44], [ 23, -23, 46]]]], dtype=int32)&gt; . - 예제: (2,3,4) (4,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]], dtype=int32)&gt; . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [21], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) . InvalidArgumentError Traceback (most recent call last) Input In [22], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=2) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . - (2,2) @ (2,) 의 연산? . numpy . np.array([[1,0],[0,1]]) @ np.array([77,-88]) . array([ 77, -88]) . np.array([77,-88]) @ np.array([[1,0],[0,1]]) . array([ 77, -88]) . np.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1) . array([[ 77], [-88]]) . np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) . ValueError Traceback (most recent call last) Input In [26], in &lt;cell line: 1&gt;() -&gt; 1 np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . np.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) . array([[ 77, -88]]) . tensorflow . I = tf.constant([[1.0,0.0],[0.0,1.0]]) x = tf.constant([77.0,-88.0]) . I @ x . InvalidArgumentError Traceback (most recent call last) Input In [29], in &lt;cell line: 1&gt;() -&gt; 1 I @ x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul] . x @ I . InvalidArgumentError Traceback (most recent call last) Input In [30], in &lt;cell line: 1&gt;() -&gt; 1 x @ I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul] . I @ tf.reshape(x,(2,1)) . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[ 77.], [-88.]], dtype=float32)&gt; . tf.reshape(x,(1,2)) @ I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt; . . tf.Variable . &#49440;&#50616; . - tf.Variable()로 선언 . tf.Variable([1,2,3,4]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.Variable([1.0,2.0,3.0,4.0]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . - tf.constant() 선언후 변환 . tf.Variable(tf.constant([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . - np 등으로 선언후 변환 . tf.Variable(np.array([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt; . &#53440;&#51077; . type(tf.Variable([1,2,3,4])) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . &#51064;&#45937;&#49905; . a=tf.Variable([1,2,3,4]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . &#50672;&#49328;&#44032;&#45733; . a=tf.Variable([1,2,3,4]) b=tf.Variable([-1,-2,-3,-4]) . a+b . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt; . tf.Variable&#46020; &#50416;&#44592; &#48520;&#54200;&#54632; . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . InvalidArgumentError Traceback (most recent call last) Input In [42], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2])+tf.Variable([3.14,3.14]) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/variables.py:1078, in Variable._OverloadOperator.&lt;locals&gt;._run_op(a, *args, **kwargs) 1076 def _run_op(a, *args, **kwargs): 1077 # pylint: disable=protected-access -&gt; 1078 return tensor_oper(a.value(), *args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tnp&#51032; &#51008;&#52509;&#46020; &#51068;&#48512;&#47564; &#44032;&#45733; . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . - 알아서 형 변환 . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt; . - .reshape 메소드 . tf.Variable([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [45], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . &#45824;&#48512;&#48516;&#51032; &#46041;&#51089;&#51008; tf.constant&#46993; &#53360; &#52264;&#51060;&#47484; &#47784;&#47476;&#44192;&#51020; . - tf.concat . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]], dtype=int32)&gt; . - tf.stack . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]], dtype=int32)&gt; . &#48320;&#49688;&#44050;&#48320;&#44221;&#44032;&#45733;(?) . a= tf.Variable([1,2,3,4]) id(a) . 140135926541824 . a.assign_add([-1,-2,-3,-4]) id(a) . 140135926541824 . &#50836;&#50557; . - tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음. . - 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐. . &#48120;&#48516; . &#47784;&#54000;&#48652; . - 예제: 컴퓨터를 이용하여 $x=2$에서 $y=3x^2$의 접선의 기울기를 구해보자. . (손풀이) . $$ frac{dy}{dx}=6x$$ . 이므로 $x=2$를 대입하면 12이다. . (컴퓨터를 이용한 풀이) . 단계1 . x1=2 y1= 3*x1**2 . x2=2+0.000000001 y2= 3*x2**2 . (y2-y1)/(x2-x1) . 12.0 . 단계2 . def f(x): return(3*x**2) . f(3) . 27 . def d(f,x): return (f(x+0.000000001)-f(x))/0.000000001 . d(f,2) . 12.000000992884452 . 단계3 . d(lambda x: 3*x**2 ,2) . 12.000000992884452 . d(lambda x: x**2 ,0) . 1e-09 . 단계4 . $$f(x,y)= x^2 +3y$$ . def f(x,y): return(x**2 +3*y) . d(f,(2,3)) . TypeError Traceback (most recent call last) Input In [60], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(2,3)) Input In [55], in d(f, x) 1 def d(f,x): -&gt; 2 return (f(x+0.000000001)-f(x))/0.000000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제1: $x=2$에서 $y=3x^2$의 도함수값을 구하라. . x=tf.Variable(2.0) a=tf.constant(3.0) . mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 y=a*x**2 # y=ax^2 = 3x^2 mytape.__exit__(None,None,None) # 기록 끝 . mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제2: 조금 다른예제 . x=tf.Variable(2.0) #a=tf.constant(3.0) mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.__exit__(None,None,None) # 기록 끝 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . $$a= frac{3}{2}x$$ $$y=ax^2= frac{3}{2}x^3$$ . $$ frac{dy}{dx}= frac{3}{2} 3x^2$$ . 3/2*3*4 . 18.0 . - 테이프의 개념 ($ star$) . (상황) . 우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 $y=3x^2$) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 $y=3x^2$이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함. . (1) mytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다. . (2) mytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다) . (3) a=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다 . (4) mytape.__exit__(None,None,None): 공책을 닫는다. . (5) mytape.gradient(y,x): $y$를 $x$로 미분하라는 메모를 남기고 컴퓨터에게 전달한다. . - 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다. . x=tf.Variable(2.0) a=(x/2)*3 ## a=(3/2)x mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.__exit__(None,None,None) # 기록 끝 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제4: with문과 함께 쓰는 tf.GradientTape() . x=tf.Variable(2.0) a=(x/2)*3 . with tf.GradientTape() as mytape: ## with문 시작 y=a*x**2 ## with문 끝 . mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . (문법해설) . 아래와 같이 쓴다. . with expression as myname: ## with문 시작: myname.__enter__() blabla ~ yadiyadi !! ## with문 끝: myname.__exit__() . (1) expression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다. . (2) with문이 시작되면서 myname.__enter__()이 실행된다. . (3) 블라블라와 야디야디가 실행된다. . (4) with문이 종료되면서 myname.__exit__()이 실행된다. . - 예제5: 예제2를 with문과 함께 구현 . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제6: persistent = True . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . mytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . mytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제7: watch . (관찰1) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) # 수동감시 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰3) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰4) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰5) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . - 예제9: 카페예제로 돌아오자. . - 예제10: 카페예제의 매트릭스 버전 . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ hat{ boldsymbol{ beta}}$을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 $ begin{bmatrix}0 0 end{bmatrix}$ 임을 확인하라. .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-21%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-21%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "2주차-3월 14일",
            "content": "import . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . 2022-04-25 14:46:52.378177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . - 리스트 . lst = [1,2,4,5,6] lst . [1, 2, 4, 5, 6] . lst[1] # 두번쨰원소 . 2 . lst[-1] # 마지막원소 . 6 . - (2,2) matrix 느낌의 list . lst= [[1,2],[3,4]] lst . [[1, 2], [3, 4]] . 위를 아래와 같은 매트릭스로 생각할수 있다. . 1 2 3 4 . print(lst[0][0]) # (1,1) print(lst[0][1]) # (1,2) print(lst[1][0]) # (2,1) print(lst[1][1]) # (2,2) . 1 2 3 4 . - (4,1) matrix 느낌의 list . lst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector lst . [[1], [2], [3], [4]] . - (1,4) matrix 느낌의 list . lst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector lst . [[1, 2, 3, 4]] . &#49440;&#50616; . - 스칼라 . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(3.14)+tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt; . - 벡터 . _vector=tf.constant([1,2,3]) . _vector[-1] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt; . - 매트릭스 . _matrix= tf.constant([[1,0],[0,1]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 1]], dtype=int32)&gt; . - array . tf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]]) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 1], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]]], dtype=int32)&gt; . &#53440;&#51077; . type(tf.constant(3.14)) . tensorflow.python.framework.ops.EagerTensor . &#51064;&#45937;&#49905; . _matrix = tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0][0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _matrix[0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,:] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[:,0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - 불편한점 . 모든 원소가 같은 dtype을 가지고 있어야함. | 원소 수정이 불가능함. | 묵시적 형변환이 불가능하다. | - 원소수정이 불가능함 . a=tf.constant([1,22,33]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt; . a[0]=11 . TypeError Traceback (most recent call last) Input In [23], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . - 묵시적 형변환이 불가능하다 . tf.constant(1)+tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) Input In [24], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant(1)+tf.constant(3.14) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tf.constant(1.0)+tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt; . - 같은 float도 안되는 경우가 있음 . tf.constant(1.0,dtype=tf.float64) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt; . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(1.0,dtype=tf.float64)+tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) Input In [28], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant(1.0,dtype=tf.float64)+tf.constant(3.14) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . np.array(tf.constant(1)) # 방법1 . array(1, dtype=int32) . a=tf.constant([3.14,-3.14]) type(a) . tensorflow.python.framework.ops.EagerTensor . a.numpy() . array([ 3.14, -3.14], dtype=float32) . . - 더하기 . a=tf.constant([1,2]) b=tf.constant([3,4]) a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . - 곱하기 . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[5,6],[7,8]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . tf.multiply(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . - 매트릭스의곱 . a=tf.constant([[1,0],[0,1]]) # (2,2) b=tf.constant([[5],[7]]) # (2,1) a@b . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . - 역행렬 . a=tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) Input In [40], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(a) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1506, in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . a=tf.constant([[1.0,0.0],[0.0,2.0]]) tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1. , 0. ], [0. , 0.5]], dtype=float32)&gt; . - tf.linalg. + tab을 누르면 좋아보이는 연산들 많음 . a=tf.constant([[1.0,2.0],[3.0,4.0]]) print(a) tf.linalg.det(a) . tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . &#54805;&#53468;&#48320;&#54872; . - 기본: tf.reshape() 를 이용 . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(4,1)) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.reshape(a,(2,2,1)) . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[1], [2]], [[3], [4]]], dtype=int32)&gt; . - 다차원 . a=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . - tf.resh . a=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(4,-1)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . tf.reshape(a,(2,2,-1)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . b=tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다. . np.diag([1,2,3,4]) . array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) . tf.constant(np.diag([1,2,3,4])) . &lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy= array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt; . - tf.ones, tf.zeros . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . tf.reshape(tf.constant([0]*9),(3,3)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int32)&gt; . - range(10) . a=range(0,12) tf.constant(a) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . tf.constant(range(1,20,3)) . &lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1, 4, 7, 10, 13, 16, 19], dtype=int32)&gt; . - tf.linspace . tf.linspace(0,1,10) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ])&gt; . tf.concat . - (2,1) concat (2,1) =&gt; (2,2) . 두번째 축이 바뀌었다. =&gt; axis=1 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . - (2,1) concat (2,1) =&gt; (4,1) . 첫번째 축이 바뀌었다. =&gt; axis=0 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (2,2) . 첫번째 // axis=0 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (1,4) . 첫번째 // axis=0 | . - (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) . 첫번째 // axis=0 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) . 두번째 // axis=1 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) . 세번째 // axis=2 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19], [ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39], [ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59], [ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79], [ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99], [ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119], [-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) . 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4, 0, -1, -2, -3, -4], [ 5, 6, 7, 8, 9, -5, -6, -7, -8, -9], [ 10, 11, 12, 13, 14, -10, -11, -12, -13, -14], [ 15, 16, 17, 18, 19, -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24, -20, -21, -22, -23, -24], [ 25, 26, 27, 28, 29, -25, -26, -27, -28, -29], [ 30, 31, 32, 33, 34, -30, -31, -32, -33, -34], [ 35, 36, 37, 38, 39, -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44, -40, -41, -42, -43, -44], [ 45, 46, 47, 48, 49, -45, -46, -47, -48, -49], [ 50, 51, 52, 53, 54, -50, -51, -52, -53, -54], [ 55, 56, 57, 58, 59, -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64, -60, -61, -62, -63, -64], [ 65, 66, 67, 68, 69, -65, -66, -67, -68, -69], [ 70, 71, 72, 73, 74, -70, -71, -72, -73, -74], [ 75, 76, 77, 78, 79, -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84, -80, -81, -82, -83, -84], [ 85, 86, 87, 88, 89, -85, -86, -87, -88, -89], [ 90, 91, 92, 93, 94, -90, -91, -92, -93, -94], [ 95, 96, 97, 98, 99, -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104, -100, -101, -102, -103, -104], [ 105, 106, 107, 108, 109, -105, -106, -107, -108, -109], [ 110, 111, 112, 113, 114, -110, -111, -112, -113, -114], [ 115, 116, 117, 118, 119, -115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (8,) . 첫번째축? // axis=0 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (4,2) . 두번째축? // axis=1 ==&gt; 이런거없다.. | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [80], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.stack . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]], dtype=int32)&gt; . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . tnp . - tf는 넘파이에 비하여 텐서만들기가 너무힘듬 . np.diag([1,2,3]).reshape(-1) . array([1, 0, 0, 0, 2, 0, 0, 0, 3]) . 넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데... #### tnp 사용방법 (불만해결방법) | . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . type(tnp.array([1,2,3])) . tensorflow.python.framework.ops.EagerTensor . - int와 float을 더할 수 있음 . tnp.array([1,2,3])+tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tf.constant([1,2,3])+tf.constant([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tnp.array(1)+tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt; . tnp.diag([1,2,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . a=tnp.diag([1,2,3]) type(a) . tensorflow.python.framework.ops.EagerTensor . a=tf.constant([1,2,3]) a.reshape(3,1) . &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy= array([[1], [2], [3]], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . np.random.randn(5) . array([-0.26554076, 0.93945129, -1.12311997, -0.89946852, -0.72582148]) . tnp.random.randn(5) # 넘파이가 되면 나도 된다. . &lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.00898484, 0.31565541, 1.12455947, 0.44374772, 0.77793125])&gt; . &#53440;&#51077; . type(tnp.random.randn(5)) . tensorflow.python.framework.ops.EagerTensor . tf.contant&#47196; &#47564;&#46308;&#50612;&#46020; &#47560;&#52824; &#45336;&#54028;&#51060;&#51064;&#46319; &#50416;&#45716; &#44592;&#45733;&#46308; . - 묵시적형변환이 가능 . tf.constant([1,1])+tf.constant([2.2,3.3]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])&gt; . - 메소드를 쓸수 있음. . a= tnp.array([[1,2,3,4]]) a.T . &lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy= array([[1], [2], [3], [4]])&gt; . &#44536;&#47111;&#51648;&#47564; np.array&#45716; &#50500;&#45784; . - 원소를 할당하는것은 불가능 . a=tf.constant([1,2,3]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . a[0]=11 . TypeError Traceback (most recent call last) Input In [99], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-14%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EC%A3%BC%EC%B0%A8(3%EC%9B%94-14%EC%9D%BC)_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "빅데이터분석 특강 중간고사",
            "content": "import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . 2022-04-25 20:47:56.696349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . N = 10000 . y_true=(x-3)**2/2**2 . epsilon = tnp.random.randn(N)*0.5 y=(x-3)**2/2**2+epsilon . x.shape, y.shape . (TensorShape([10000]), TensorShape([10000])) . beta = tf.Variable(2.0) alpha = tf.Variable(3.0) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = (x-beta)**2/(alpha**2) loss = tf.reduce_sum((y-yhat)**2)/N slope0,slope1 = tape.gradient(loss,[beta,alpha]) beta.assign_sub(alpha * slope0) alpha.assign_sub(alpha * slope1) . beta, alpha . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=161.78185&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=159.62784&gt;) . yhat=(x-beta)**2/(alpha**2) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;r.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe4294ced70&gt;] . (2) 아래는 $X_i overset{iid}{ sim} Ber(0.8)$을 생성하는 코드이다. . tf.random.set_seed(43052) x= tf.constant(np.random.binomial(1,0.8,(10000,))) x . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 1, 1, ..., 1, 0, 1])&gt; . 함수 $L(p)$을 최대화하는 $p$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $p$의 초기값은 0.3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)=p^{x_i}(1-p)^{1-x_i}$$ . N = 10000 . y_true=(x-0.8)**2 . epsilon = tnp.random.randn(N)*0.5 y=(x-0.8)**2+epsilon . x.shape, y.shape . (TensorShape([10000]), TensorShape([10000])) . beta = tf.Variable(2.0) alpha = tf.Variable(3.0) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = (x-beta)**2/(alpha**2) loss = tf.reduce_sum((y-yhat)**2)/N slope0,slope1 = tape.gradient(loss,[beta,alpha]) beta.assign_sub(alpha * slope0) alpha.assign_sub(alpha * slope1) . beta, alpha . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.3362731&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.6516612&gt;) . yhat=(x-beta)**2/(alpha**2) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;r.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe4201269e0&gt;] . (3) 아래의 모형에 따라서 $ {Y_i }_{i=1}^{10000}$를 생성하는 코드를 작성하라. . $Y_i overset{iid}{ sim} N( mu_i,1)$ | $ mu_i = beta_0 + beta_1 x_i = 0.5 + 2 x_i$ , where $x_i = frac{i}{10000}$ | . 함수 $L( beta_0, beta_1)$을 최대화하는 $( beta_0, beta_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ beta_0, beta_1$의 초기값은 모두 1로 설정할 것) . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]) # X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], # [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . N = 10 . y = y.reshape(N,1) . X = tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) . y=y.reshape(N,1) x=x.reshape(N,1) y.shape, X.shape . (TensorShape([10, 1]), TensorShape([10, 2])) . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.94457323], [2.21570461]])&gt; . y_hat=9.94457323+2.21570461*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe40c5884c0&gt;] . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . N = 10 . y=y.reshape(N,1) X=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1) y.shape,X.shape . (TensorShape([10, 1]), TensorShape([10, 2])) . y=y.reshape(N,1) X.shape, y.shape . (TensorShape([10, 2]), TensorShape([10, 1])) . beta_hat = tnp.array([9,2]).reshape(2,1) beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[9], [2]])&gt; . alpha = 0.001 . for epoc in range(1000): slope = (-2*X.T @ y + 2*X.T@ X @ beta_hat)/N beta_hat = beta_hat - alpha * slope . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.03545357], [2.25155218]])&gt; . y_hat=9.03545357+2.25155218*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe3b417efe0&gt;] . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . opt.apply_gradients([(slope,beta_hat)]) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.3362731&gt; . X.shape,y.shape . (TensorShape([10, 2]), TensorShape([10, 1])) . beta_hat = tf.Variable(tnp.array([9,2],dtype=&#39;float64&#39;).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.], [2.]])&gt; . alpha=0.05 opt = tf.keras.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) opt.apply_gradients( [(slope,beta_hat)] ) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[nan], [nan]])&gt; . y_hat=9.03545358+2.25155218*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe354769180&gt;] . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=nan&gt; . mseloss_fn=tf.losses.MeanSquaredError() . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([10, 2]), TensorShape([10, 1])) . beta_hat = tf.Variable(tnp.array([9,2],dtype=&#39;float64&#39;).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.], [2.]])&gt; . alpha=0.0015 opt = tf.keras.optimizers.SGD(alpha) . mseloss_fn(y.reshape(-1),yhat.reshape(-1)) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=nan&gt; . def loss_fn(): yhat= X@beta_hat loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.04793931], [2.25105986]])&gt; . y_hat=9.03545358+2.25155218*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_hat,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe3547af790&gt;] . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7fe35461de10&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . X = np.stack([np.ones(1000),np.cos(1*t),np.cos(2*t),np.cos(3*t),np.cos(4*t),np.cos(5*t)],axis=1) y = y.reshape(1000,1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,use_bias=False)) net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) net.fit(X,y,epochs=30, batch_size=N) . Epoch 1/30 100/100 [==============================] - 0s 790us/step - loss: 0.2753 Epoch 2/30 100/100 [==============================] - 0s 738us/step - loss: 0.0374 Epoch 3/30 100/100 [==============================] - 0s 709us/step - loss: 0.0378 Epoch 4/30 100/100 [==============================] - 0s 718us/step - loss: 0.0375 Epoch 5/30 100/100 [==============================] - 0s 710us/step - loss: 0.0378 Epoch 6/30 100/100 [==============================] - 0s 753us/step - loss: 0.0377 Epoch 7/30 100/100 [==============================] - 0s 806us/step - loss: 0.0369 Epoch 8/30 100/100 [==============================] - 0s 830us/step - loss: 0.0373 Epoch 9/30 100/100 [==============================] - 0s 831us/step - loss: 0.0372 Epoch 10/30 100/100 [==============================] - 0s 868us/step - loss: 0.0379 Epoch 11/30 100/100 [==============================] - 0s 831us/step - loss: 0.0372 Epoch 12/30 100/100 [==============================] - 0s 827us/step - loss: 0.0372 Epoch 13/30 100/100 [==============================] - 0s 846us/step - loss: 0.0374 Epoch 14/30 100/100 [==============================] - 0s 820us/step - loss: 0.0379 Epoch 15/30 100/100 [==============================] - 0s 926us/step - loss: 0.0375 Epoch 16/30 100/100 [==============================] - 0s 949us/step - loss: 0.0373 Epoch 17/30 100/100 [==============================] - 0s 833us/step - loss: 0.0379 Epoch 18/30 100/100 [==============================] - 0s 706us/step - loss: 0.0377 Epoch 19/30 100/100 [==============================] - 0s 700us/step - loss: 0.0380 Epoch 20/30 100/100 [==============================] - 0s 744us/step - loss: 0.0381 Epoch 21/30 100/100 [==============================] - 0s 689us/step - loss: 0.0370 Epoch 22/30 100/100 [==============================] - 0s 733us/step - loss: 0.0377 Epoch 23/30 100/100 [==============================] - 0s 733us/step - loss: 0.0379 Epoch 24/30 100/100 [==============================] - 0s 719us/step - loss: 0.0377 Epoch 25/30 100/100 [==============================] - 0s 681us/step - loss: 0.0377 Epoch 26/30 100/100 [==============================] - 0s 700us/step - loss: 0.0380 Epoch 27/30 100/100 [==============================] - 0s 718us/step - loss: 0.0381 Epoch 28/30 100/100 [==============================] - 0s 689us/step - loss: 0.0375 Epoch 29/30 100/100 [==============================] - 0s 763us/step - loss: 0.0376 Epoch 30/30 100/100 [==============================] - 0s 768us/step - loss: 0.0379 . &lt;keras.callbacks.History at 0x7fe354554850&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(6, 1) dtype=float32, numpy= array([[-1.9798379 ], [ 3.0361695 ], [ 1.0182086 ], [-0.00371684], [-0.00969306], [ 0.4876245 ]], dtype=float32)&gt;] . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7fe3543117e0&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . x.shape, y.shape . (TensorShape([2000]), TensorShape([2000])) . x=x.reshape(2000,1) x.shape, y.shape . (TensorShape([2000, 1]), TensorShape([2000])) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) bceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat)) net.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=10000,verbose=0,batch_size=2000) . &lt;keras.callbacks.History at 0x7fe354356980&gt; . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[5.09306]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_1/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-1.0963831], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe354357250&gt;] . 4. Piecewise-linear regression (15&#51216;) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (1) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ hat{y} = hat{ beta}_0+ hat{ beta}_1x $ | . tf.random.set_seed(43054) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . &lt;keras.callbacks.History at 0x7fe3542e8e80&gt; . 케라스에 의해 추정된 $ hat{ beta}_0, hat{ beta}_1$을 구하라. . net.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;,alpha=0.5) plt.plot(x,net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe35418ffa0&gt;] . (2) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ boldsymbol{u}= x boldsymbol{W}^{(1)}+ boldsymbol{b}^{(1)}$ | $ boldsymbol{v}= text{relu}(u)$ | $y= boldsymbol{v} boldsymbol{W}^{(2)}+b^{(2)}$ | . tf.random.set_seed(43056) ## 1단계 net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7fe3541d1090&gt; . ${ boldsymbol u}$를 이용하여 ${ boldsymbol v}$를 만드는 코드와 ${ boldsymbol v}$를 이용하여 $y$를 만드는 코드를 작성하라. . net.weights . [&lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[1.9178674, 0.7250776]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(2,) dtype=float32, numpy=array([ 0.33402315, -0.72684675], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.8141525 ], [-0.67835057]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . W1= tf.Variable(tnp.array([[-0.03077251, 1.8713338 ]])) b1= tf.Variable(tnp.array([-0.04834982, 0.3259186 ])) W2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]])) b2= tf.Variable(tnp.array([-0.60076195])) . with tf.GradientTape() as tape: u = tf.constant(x) @ W1 + b1 v = tf.nn.relu(u) yhat = v@W2 + b2 loss = tf.losses.mse(y,yhat) . tape.gradient(loss,[W1,b1,W2,b2]) . [&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])&gt;, &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ 0.00000000e+00], [-4.74910706e-05]])&gt;, &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe3387a6530&gt;] . (3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라. . (곤이) (2) 모형은 활성화함수로 relu를 사용하였다. (O) . (철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다. . (아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. (X) . (짝귀) (1) 의 모형은 오버피팅의 위험이 있다. (O) . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. (참) . (2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. (참) . (3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. (참) . (4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. (참) . (5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. (거짓) .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EC%8B%AC%EC%9E%AC%EC%9D%B8_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95_%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC.html",
            "relUrl": "/2022/05/29/%EC%8B%AC%EC%9E%AC%EC%9D%B8_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95_%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "기말고사 예상문제",
            "content": "imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . %load_ext tensorboard . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . 1. Fashion_mnist, DNN (30&#51216;) . (1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. . 평가지표로 accuracy를 이용할 것 | epoch은 10으로 설정할 것 | optimizer는 adam을 이용할 것 | . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node20&quot; &quot;x2&quot; -&gt; &quot;node20&quot; &quot;..&quot; -&gt; &quot;node20&quot; &quot;x784&quot; -&gt; &quot;node20&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;node1 &quot; &quot;node2&quot; -&gt; &quot;node1 &quot; &quot;...&quot; -&gt; &quot;node1 &quot; &quot;node20&quot; -&gt; &quot;node1 &quot; &quot;node1&quot; -&gt; &quot;node2 &quot; &quot;node2&quot; -&gt; &quot;node2 &quot; &quot;...&quot; -&gt; &quot;node2 &quot; &quot;node20&quot; -&gt; &quot;node2 &quot; &quot;node1&quot; -&gt; &quot;... &quot; &quot;node2&quot; -&gt; &quot;... &quot; &quot;...&quot; -&gt; &quot;... &quot; &quot;node20&quot; -&gt; &quot;... &quot; &quot;node1&quot; -&gt; &quot;node30 &quot; &quot;node2&quot; -&gt; &quot;node30 &quot; &quot;...&quot; -&gt; &quot;node30 &quot; &quot;node20&quot; -&gt; &quot;node30 &quot; label = &quot;Layer 2: relu&quot; } subgraph cluster_4{ style=filled; color=lightgrey; &quot;node1 &quot; -&gt; &quot;y10&quot; &quot;node2 &quot; -&gt; &quot;y10&quot; &quot;... &quot; -&gt; &quot;y10&quot; &quot;node30 &quot; -&gt; &quot;y10&quot; &quot;node1 &quot; -&gt; &quot;y1&quot; &quot;node2 &quot; -&gt; &quot;y1&quot; &quot;... &quot; -&gt; &quot;y1&quot; &quot;node30 &quot; -&gt; &quot;y1&quot; &quot;node1 &quot; -&gt; &quot;.&quot; &quot;node2 &quot; -&gt; &quot;.&quot; &quot;... &quot; -&gt; &quot;.&quot; &quot;node30 &quot; -&gt; &quot;.&quot; label = &quot;Layer 3: softmax&quot; } &#39;&#39;&#39;) . . FileNotFoundError Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:79, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 78 kwargs[&#39;stdout&#39;] = kwargs[&#39;stderr&#39;] = subprocess.PIPE &gt; 79 proc = _run_input_lines(cmd, input_lines, kwargs=kwargs) 80 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:99, in _run_input_lines(cmd, input_lines, kwargs) 98 def _run_input_lines(cmd, input_lines, *, kwargs): &gt; 99 popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs) 101 stdin_write = popen.stdin.write File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize) 963 self.stderr = io.TextIOWrapper(self.stderr, 964 encoding=encoding, errors=errors) --&gt; 966 self._execute_child(args, executable, preexec_fn, close_fds, 967 pass_fds, cwd, env, 968 startupinfo, creationflags, shell, 969 p2cread, p2cwrite, 970 c2pread, c2pwrite, 971 errread, errwrite, 972 restore_signals, 973 gid, gids, uid, umask, 974 start_new_session) 975 except: 976 # Cleanup if the child failed starting. File ~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session) 1841 err_msg = os.strerror(errno_num) -&gt; 1842 raise child_exception_type(errno_num, err_msg, err_filename) 1843 raise child_exception_type(err_msg) FileNotFoundError: [Errno 2] No such file or directory: PosixPath(&#39;dot&#39;) The above exception was the direct cause of the following exception: ExecutableNotFound Traceback (most recent call last) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --&gt; 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in JupyterIntegration._repr_mimebundle_(self, include, exclude, **_) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:98, in &lt;dictcomp&gt;(.0) 96 include = set(include) if include is not None else {self._jupyter_mimetype} 97 include -= set(exclude or []) &gt; 98 return {mimetype: getattr(self, method_name)() 99 for mimetype, method_name in MIME_TYPES.items() 100 if mimetype in include} File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/jupyter_integration.py:112, in JupyterIntegration._repr_image_svg_xml(self) 110 def _repr_image_svg_xml(self) -&gt; str: 111 &#34;&#34;&#34;Return the rendered graph as SVG string.&#34;&#34;&#34; --&gt; 112 return self.pipe(format=&#39;svg&#39;, encoding=SVG_ENCODING) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 55 def pipe(self, 56 format: typing.Optional[str] = None, 57 renderer: typing.Optional[str] = None, (...) 61 engine: typing.Optional[str] = None, 62 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: 63 &#34;&#34;&#34;Return the source piped through the Graphviz layout command. 64 65 Args: (...) 102 &#39;&lt;?xml version=&#39; 103 &#34;&#34;&#34; --&gt; 104 return self._pipe_legacy(format, 105 renderer=renderer, 106 formatter=formatter, 107 neato_no_op=neato_no_op, 108 quiet=quiet, 109 engine=engine, 110 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/_tools.py:171, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs) 162 wanted = &#39;, &#39;.join(f&#39;{name}={value!r}&#39; 163 for name, value in deprecated.items()) 164 warnings.warn(f&#39;The signature of {func.__name__} will be reduced&#39; 165 f&#39; to {supported_number} positional args&#39; 166 f&#39; {list(supported)}: pass {wanted}&#39; 167 &#39; as keyword arg(s)&#39;, 168 stacklevel=stacklevel, 169 category=category) --&gt; 171 return func(*args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 112 @_tools.deprecate_positional_args(supported_number=2) 113 def _pipe_legacy(self, 114 format: typing.Optional[str] = None, (...) 119 engine: typing.Optional[str] = None, 120 encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]: --&gt; 121 return self._pipe_future(format, 122 renderer=renderer, 123 formatter=formatter, 124 neato_no_op=neato_no_op, 125 quiet=quiet, 126 engine=engine, 127 encoding=encoding) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/piping.py:149, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding) 146 if encoding is not None: 147 if codecs.lookup(encoding) is codecs.lookup(self.encoding): 148 # common case: both stdin and stdout need the same encoding --&gt; 149 return self._pipe_lines_string(*args, encoding=encoding, **kwargs) 150 try: 151 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/piping.py:212, in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet) 206 cmd = dot_command.command(engine, format, 207 renderer=renderer, 208 formatter=formatter, 209 neato_no_op=neato_no_op) 210 kwargs = {&#39;input_lines&#39;: input_lines, &#39;encoding&#39;: encoding} --&gt; 212 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs) 213 return proc.stdout File ~/anaconda3/envs/py310/lib/python3.10/site-packages/graphviz/backend/execute.py:84, in run_check(cmd, input_lines, encoding, quiet, **kwargs) 82 except OSError as e: 83 if e.errno == errno.ENOENT: &gt; 84 raise ExecutableNotFound(cmd) from e 85 raise 87 if not quiet and proc.stderr: ExecutableNotFound: failed to execute PosixPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . &lt;graphviz.sources.Source at 0x7f77f9331090&gt; . (2) (1)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. . (3) train set에서 20%의 자료를 validation 으로 분리하여 50에폭동안 학습하라. 텐서보드를 이용하여 train accuracy와 validation accuracy를 시각화 하고 결과를 해석하라. 오버피팅이라고 볼 수 있는가? . (4) (3)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (2)의 결과와 비교하라. . (5) 조기종료기능을 이용하여 (3)의 네트워크를 다시 학습하라. 학습결과를 텐서보드를 이용하여 시각화 하라. . patience=3 으로 설정할 것 | . 2. Fashion_mnist, CNN (30&#51216;) . (1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. . 이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다. | net.summary()를 출력하여 설계결과를 확인하라. | . . (2) n1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라. 결과를 텐서보드로 시각화하는 코드를 작성하라. . epoc은 3회로 한정한다. | validation_split은 0.2로 설정한다. | . 3. CIFAR10 (30&#51216;) . tf.keras.datasets.cifar10.load_data()을 이용하여 CIFAR10을 불러온 뒤 적당한 네트워크를 사용하여 적합하라. . 결과를 텐서보드로 시각화할 필요는 없다. | 자유롭게 모형을 설계하여 적합하라. | test set의 accuracy가 70%이상인 경우만 정답으로 인정한다. | . 4. &#45796;&#51020;&#51012; &#51069;&#44256; &#47932;&#51020;&#50640; &#45813;&#54616;&#46972;. (10&#51216;) . (1) (1,128,128,3)의 shape을 가진 텐서가 tf.keras.layers.Conv2D(5,(2,2))으로 만들어진 커널을 통과할시 나오는 shape은? . (2) (1,24,24,16)의 shape을 가진 텐서가 tf.keras.layers.Flatten()을 통과할때 나오는 텐서의 shape은? . (3) . (4) . (5) .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "relUrl": "/2022/05/29/%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%ED%8A%B9%EA%B0%95.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Title",
            "content": "import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . _X1 = tnp.ones([50,25])*10 _X1 . 2022-05-23 23:36:31.857883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy= array([[10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], ..., [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.]])&gt; . _X2 = tnp.zeros([50,25])*10 _X2 . &lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]])&gt; . tf.concat([_X1,_X2],axis=1) . &lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy= array([[10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], ..., [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.]])&gt; . _noise = tnp.random.randn(50*50).reshape(50,50) _noise . &lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy= array([[-0.35893472, 1.34715133, -1.09867888, ..., 0.86637446, 0.76534142, 0.02492512], [-2.0924809 , 0.40532776, -0.22088057, ..., -0.40185935, 0.45020357, 1.668251 ], [ 1.5649497 , 0.09772148, -0.44497024, ..., 0.95732265, 1.36476409, 0.81405914], ..., [-0.15583511, -0.59527225, -0.46381306, ..., 0.16362863, 1.08391654, 0.49281776], [ 1.07869912, 1.07151975, -0.0196227 , ..., 1.25430225, 0.43976791, 0.40134943], [ 0.15613959, 0.10136015, 2.32804407, ..., -1.03957414, -2.6255478 , -0.08386856]])&gt; . XXX = tf.concat([_X1,_X2],axis=1) + _noise . XXX=XXX.reshape(1,50,50,1) . plt.imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) . NameError Traceback (most recent call last) Input In [9], in &lt;cell line: 1&gt;() -&gt; 1 plt.imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) NameError: name &#39;plt&#39; is not defined . conv = tf.keras.layers.Conv2D(2,(2,2)) .",
            "url": "https://simjaein.github.io/ji1598/2022/05/29/Untitled.html",
            "relUrl": "/2022/05/29/Untitled.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Variable Selection",
            "content": "Model Selection . Goal : explain the response with minimum number of explanatory variables | Full Model $y_i = beta_0+ beta_1x_{i1}+ dots+ beta_px_{ip}+ dots+ beta_qx_{iq}+ epsilon_i$ | $ epsilon_i sim_{idd}N(0, epsilon^2)$ | $SSE_q = ||Y-X hat{ beta^*}||^2, quad hat{ sigma_{q}^2}=SSE_q/(n-p-1)$ -Current Model | $y_i = beta_0+ beta_1x_{i1}+ dots+ beta_px_{ip} epsilon_i$, | $ epsilon_i sim_{idd}N(0, epsilon_{p}^2)$ | $SSE_p = ||Y-X_{p} hat{ beta^p}||^2, quad hat{ sigma_{p}^2}=SSE_p/(n-p-1)$ | . | Statistics used in model selection Residual mean squares error (MSE) : $MSE = frac{SSE_P}{(n-p-1)}$ | coefficient of determination : $R^2 = frac{SSR}{SST}=1- frac{SSE_p}{SST}$ | Adjusted $R^2 : R^2_{adj} = 1- frac{SSE_p/(n-p-1)}{SST/(n-1)}$ | Partial F-test statistics | . | . Partial F-test statistics . 부분 F-검정통계량 $H_0: beta_p=0_{| beta_0, beta_1, dots, beta_{p-1}} ;vs. H_1: beta_p neq 0_{| beta_0, beta_1, dots, beta_{p-1}}$ 부분 검정통계량 : $F_0 = frac{SSR_{FM}-SSR_{RM}}{MSE_{FM}} qquad 유의확률 begin{cases}낮으면 ;추가 높으면 ;제거 end{cases}$ | $FM$ : 변수 $p$개, $RM$ : 변수 $(p-1)$개 | $F_0$ ~ $F(1,n-p-1), under ;H_0$ | $F_0 geq F_c = F_{ alpha}(1,n-p-1) Rightarrow H_0$ 기각 $ qquad f_0 begin{cases}높으면 ;추가 낮으면 ;제거 end{cases}$ | . | . Variable Selection . Variable selection. All possible regression : 모든 가능한 회귀 $ Rightarrow$ 수가 많아지면 너무 오래 걸린다 | Backward Elimination : 후진 제거법 $ Rightarrow$ 필요없는것부터 제거 $ qquad ;$ (단, 한번 제거되면 무조건 제거) | Forward Selection : 전진 선택법 $ Rightarrow$ 하나씩 추가해보며 하는 것 $ qquad$ (단, 한번 추가되면 무조건 추가) | Stepwise regression : 단계별 회귀 $ Rightarrow$ 후진 제거법 + 전진 선택법 | . | All possible regression 모든 가능한 변수들의 조합 $(2^p)$을 회귀분석하여 결과 비교 | 시간이 오래 걸림 | $R^2$또는 $MSE$ 사용 | . | Backward Elimination (step 0) 모든 변수를 포함한 회귀방정식 적합 (Full Model). (step 1) 변수 하나하나씩에 대한 부분 F-검정통계량 $F_0$ 구함 (step 2) 가장 작은 부분 F-검정통계량 $F_L$과 $F_c$를 구함 (step 3) $F_L &lt; F_c$ 이면 $x_L$ 제거 $ Rightarrow$ (step 2)로 $ qquad quad F_L geq F_c$ 이면 멈춘 후 최종모형으로 선택 | . Forward Selection (step 0) 변수 하나하나씩에 대한 회귀모형 적합 후 $R^2$ 를 가장 크게 하는 설명변수 선택 (step 1) 변수를 하나하나씩 추가하여 $R^2$를 가장 크게 하는 변수 선택 $(x_s)$ (step 2) 위에서 추가된 변수 $x_s$에 대해 부분 F-검정 수행 (step 3) 위의 결과가 유의하면 $x_s$ 추가하고 (step 1)으로, 유의하지 않으면 멈춘 후 $x_s$를 제외한 모형을 최종모형으로 선택 | Stepwise regression (step 0) 전진선택법의 (step 0)와 동일 (step 1) 변수를 하나하나씩 추가하여 $R^2$를 가장 크게 하는 변수 선택 $x_s$ (step 2) 위에서 추가된 변수 $x_s$에 대해 부분 F-검정 수행 (step 3) 위의 결과가 유의하면 $x_s$ 포함하고 (step 4)로, 유의하지 않으면 멈춘 후 $x_s$를 제외한 모형을 최종모형으로 선택 (step 4) 포함된 변수에 대해 부분 F-검정을 실시하여 유의하지 않은 변수가 있으면 제거하고 (step 1)로 | .",
            "url": "https://simjaein.github.io/ji1598/2022/01/14/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EB%B3%80%EC%88%98%EC%84%A0%ED%83%9D.html",
            "relUrl": "/2022/01/14/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EB%B3%80%EC%88%98%EC%84%A0%ED%83%9D.html",
            "date": " • Jan 14, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Regression Diagnostic",
            "content": "&#54924;&#44480;&#51652;&#45800; . 오차항의 가정 검토 | 적절한 모형의 선택 | 독립변수들간의 상관관계 검토 | 이상치(outlier) 확인 | 영향을 크게 주는 측정값 (influential observation) ## Hat matrix | 잔차 $e=Y- hat Y=Y-X hat{ beta}$ $ qquad qquad = Y-X(X^TX)^{-1}X^{T}Y = [I-X(X^TX)^{-1}X^T]Y$ $E(e)=0$ | $Var(e) = [I-X(X^TX)^{-1}X^T] sigma^2$ | . | matrix Hat matrix : $H = X(X^TX)^{-1}X^T, n times n$ matrix $h_{ij} = X_{i}^T(X^TX)^{-1}x_{j},$ for $i,j = 1, dots,n,$ where $x_i = (1,x_{i1},x_{i2}, dots,x_{ip})$ $ Rightarrow E(e_i) = 0, quad Var(e_i) = (1-h_{ii}) sigma^2$ $tr(H) = p+1, ;0 leq h_{ii}&lt;1$ | $p=1 : h_{ii} = frac{1}{n}+ frac{(x_{i}- overline x)^2}{S_{xx}}$ | $p&gt;1 : h_{ii}= frac{1}{n}+(x_{i1}- overline x_1, dots,x_{ip}- overline x_p)S^{-1}$ $ begin{pmatrix} x_{i1} - { overline x_{1}} vdots x_{ip} - { overline x_{p}} end{pmatrix}$ | . | . | . &#51092;&#52264;&#48516;&#49437; . Model : $Y=X beta+ epsilon, epsilon_i sim_{iid}N(0, sigma^2)$ $ qquad y_i= beta_0+ beta_1x_{i1}+ dots+ beta_px_{ip}+ epsilon_i$ | Assumption (linearity) $E(y|x_i, dots,x_p) = beta_0+ beta_1 x_1+ dots+ beta_p x_p$ | (homogeneous variance) $Var( epsilon_1) = dots = Var( epsilon_n) = sigma^2$ | (normality) $ epsilon_i, sim N(0, sigma^2)$ | (independent) $ epsilon_1, dots, epsilon_n$ : independent | . | . Residual . 잔차(residual) : $ hat{e_i} = y_i- hat{y_i}$ 잔차의 분산 : $Var(Y_i- hat Y) = sigma^2(1-h_{ii})$ | 스튜던트화 잔차 $ hat{e_{st,i}} = frac{ hat{e}_i}{ hat{sd( hat{e}_i)}} = frac{y_i- hat{y_i}}{ sqrt{MSE(1-h_{ii})}}$ | 잔차도(residual plot) : $(x_1, hat{e_{st,i}}), dots,(x_n, hat{e_{st,n}})$ 의 산점도 | . | . Residual Anlaysis . 잔차분석 대략 0에 관하여 대칭적으로 나타나고$ qquad qquad qquad$ (선형성) | 설명변수의 값에 따른 잔차의 산포가 크게 다르지 않고$ quad ;$ (등분산성) | 점들이 특정한 형식을 가지고 나타남이 없으며$ qquad qquad$ (독립성) | 거의 모든 점이 $ pm$2(3)의 범위내에 나타나야한다$ qquad quad$ (정규성) | . | . Outlier . 표준화잔차 내적으로 스튜던트화된 잔차 (internally studentized residual) $$r_i = frac{e_i}{ hat{ sigma} sqrt{1-h_{ii}}}$$ | 외적으로 스튜던트화된 잔차 (externally studentized residual) $$r_i^* = frac{e_i}{ hat{ sigma_{(i)}} sqrt{1-h_{ii}}}$$ 단, $ hat{ sigma_{(i)}} = Big[(n-p-1) hat{ sigma}^2- frac{e_i^2}{1-h_{ii}} Big] / (n-p-2)$ | . | . Influence Observation . 영향점 : 회귀분석의 결과가 몇 개의 특정값에 의해 크게 영향을 받는 경우, 영향점이 있다고 말함 | 영향점을 검출하는 방법 (1) 행렬 $H$의 대각원소 : $ quad hat Y = X(X^TX)^{-1}X^TY = HY$ $ quad Rightarrow Var( hat Y) = h_{ii} sigma^2$ $ quad Rightarrow h_{ii} geq 2(p+1)/n$ : 영향점으로 판단 (2) DFFITS (Difference if Fits) $$DFFITS(i) = frac{ hat{y}_i- hat{y}_i(i)}{ hat{ sigma}_{(i)} sqrt{h_{ii}}}$$ . $ hat{y}_j(i) :i$번쨰 데이터를 제외시키고 $n-1$개 데이터에서 얻은 예측값 &gt; $|DFFITS(i)| geq 2 sqrt{ frac{p+1}{n-p-1}} Rightarrow$ 영향점 . (3) Cook&#39;s Distance $$C_i = frac{ sum_{j=1}^n( hat{y}_j - hat{y}_j(i))^2}{(p+1) hat{ sigma}^2}$$ . $C_i = frac{h_{ii}}{1-h_{ii}} cdot frac{r_{i}^2}{p+1}$ $C_i geq F_{0.5}(p+1,n-p-1)$ 또는 $C_i geq 1 Rightarrow$ 영향점 . | .",
            "url": "https://simjaein.github.io/ji1598/2022/01/14/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EA%B7%80%EC%A7%84%EB%8B%A8.html",
            "relUrl": "/2022/01/14/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EA%B7%80%EC%A7%84%EB%8B%A8.html",
            "date": " • Jan 14, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Multiple Linear Regression",
            "content": "Data Structure and Model . 자료구조 | . response$ quad$ explanatory . $ begin{pmatrix} y_{1} y_{2} vdots y_{n} end{pmatrix}$ $ begin{pmatrix} x_{11} &amp; cdots &amp; x_{1p} x_{21} &amp; cdots &amp; x_{2p} vdots &amp; ddots &amp; vdots x_{n1} &amp; cdots &amp; x_{np} end{pmatrix}$ . &#51473;&#54924;&#44480;&#47784;&#54805;(Multiple Linear Regression) . 설명변수가 $p$개인 다중(선형)회귀모형 $ qquad qquad y_i= beta_0+ beta_1x_{1i}+ dots+ beta_{p}x_{ip}+ epsilon_i quad i = 1,2, dots,n$ . 회귀모수 : $ beta_0, beta_1, dots, beta_p qquad longrightarrow (p+1)개$ | 설명변수(독립변수) : $X_1 = (x_{11}, dots,x_{n1})^T, dots,X_p = (x_{1p}, dots,x_{np})^T$ | 반응변수(종속변수) : $Y=(y_1, dots,y_n)^T$ | 오차항 : $ epsilon_1, dots, epsilon_n, ;( sim_{i.i.d}N(0, sigma^2))$ | . | 설명변수가 $p$개인 다중(선형)회귀모형 : 행렬형식 $ qquad qquad Y=X beta + epsilon$ . 회귀모수 : $ beta = ( beta_0, beta_1, dots, beta_p)^T, (p+1)$ vetor | 설명변수(독립변수) : $X = (1,X_1, dots,X_p)^T, (n times(P+1))$ matrix | 반응변수(종속변수) : $Y = (y_1, dots,y_n)^T, ;n$ vector | 오차항 : $ epsilon = ( epsilon_1, dots, epsilon_n)^T ;n$ vector | 자료구조 | . $ begin{pmatrix} y_{1} y_{2} vdots y_{n} end{pmatrix}$ = $ begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; cdots &amp; x_{1p} 1 &amp; x_{21} &amp; x_{22} &amp; cdots &amp; x_{2p} vdots &amp; vdots &amp; ddots &amp; vdots 1 &amp; x_{n1} &amp; x_{n2} &amp; cdots &amp; x_{np} end{pmatrix}$ $ begin{pmatrix} beta_{0} beta_{1} vdots beta_{p} end{pmatrix}$ + $ begin{pmatrix} epsilon_{1} epsilon_{2} vdots epsilon_{n} end{pmatrix}$ . $ quad ; n times 1 qquad qquad n times(p+1) qquad ;(p+1) times 1 quad n times 1$ . | . Least Square Estimation . 최소제곱추정량 : $( hat{ beta_0}, hat{ beta_1}, dots, hat{ beta_p})$ = $_{({ beta_0}, dots,{ beta_p}) in R^{p+1}} argmin displaystyle sum_{i=1}^{n} {y_i - ( beta_0+ beta_1x_{i1}+ dots + beta_px_{ip}) }^2$ | . 또는 $ qquad hat{ beta} = _{ beta in R^{p+1}} argmin ;||Y - X beta||^2 = (X^T X)^{-1}X^T Y$ . Estimation of error variance . 잔차 (residual) : $e_i = y_i - hat{y_i}$ - 오차분산 $( sigma^2)$의 추정 : 잔차(오차) 제곱합 (residual (or error) sum of squares) : $$SSE = displaystyle sum_{i=1}^{n}(y_i - hat{y_i})^2= displaystyle sum_{i=1}^{n}e_i^2$$ | 평균제곱오차 (mean squared error) : $MSE= frac{SSE}{n-(p+1)}$ | 오차분산의 추정값 : $ hat{ sigma}^2 = MSE$ | . | . Decomposition of deviations . 총편차의 분해 $y_i - overline y = (y_i- hat{y_i})+( hat{y_i}- overline y), ; forall i$ | 총편차(total deviation) = $y_i - overline y$ | 추측값의 편차 = $( hat{y_i}- overline{ hat{y_i}})+( hat{y_i}- overline y)$ $ Rightarrow$ 총편차 = 잔차 + 추측값의 편차 | . | 제곱합의 분해 : SST = SSE + SSR $$ displaystyle sum_{i=1}^{n}(y_i - overline y)^2 = displaystyle sum_{i=1}^{n}(y_i - hat{y_i})^2 + displaystyle sum_{i=1}^{n}( hat{y_i} - overline y)^2$$ | . . 제곱합의 종류 $ qquad qquad$ 정의 및 기호 $ qquad qquad$ 자유도 . . 총제곱함 $ qquad qquad SST = sum_{i=1}^{n}(y_i - overline y)^2 qquad n-1$ 잔차제곱합 $ qquad quad ; SSE = sum_{i=1}^{n}(y_i - hat{y_i})^2 quad n-(p+1)$ 회귀제곱합 $ qquad quad ; SSR = sum_{i=1}^{n}( hat{y_i} - overline y)^2 qquad quad p$ . . Coefficient of determination . 결정계수 (Coefficient of determination) $$R^2 = frac{SSR}{SST} = 1- frac{SSE}{SST}$$ | 수정된 결정계수 (Adjusted multiple correlation coefficient) $$R_{adj}^2 = 1- frac{SSE/(n-p-1)}{SST/(n-1)}$$ | . &#54924;&#44480;&#51649;&#49440;&#51032; &#50976;&#51032;&#49457; &#44160;&#51221; . 회귀직선의 유의성 검정 (F-test) 가설 : $H_0 : beta_1 = dots = beta_p=0_{vs.}H_1:notH_0$ | 검정통계량 : $F = frac{MSR}{MSE}= frac{SSR/p}{SSE/(n-(p+1))} sim_{H_0}F(p,n-p-1)$ | 검정통계량의 관측값 : $f$. | 유의수준 $ alpha$에서의 기각역 : $f geq F_{ alpha}(p,n-p-1)$ | 유의확률 = $P(F geq f)$ | . | 회귀직선의 유의성 검정을 위한 분산분석표 | . . 요인 $ qquad$ 제곱합 $ qquad$ 자유도(df) $ qquad qquad$ 평균제곱(MS) $ qquad qquad f qquad quad$ 유의확률 . . 회귀 $ qquad quad SSR qquad quad p qquad qquad quad MSR= frac{SSR}{P} qquad ; f= frac{MSR}{MSE} qquad P(F geq f)$ 잔차 $ qquad quad SSE qquad quad n-(p+1) quad ; MSE= frac{SSE}{n-p-1}$ . . 계 $ qquad qquad SST qquad quad n-1$ . . Reduced model(RM)$_{vs.}$ Full model(FM) $ qquad FM : y_i = beta_0+ beta_1x_{i1}+ beta_qx_{iq}+ dots+ beta_p x_{ip}+ epsilon_i$ $ qquad RM : y_i = beta_0+ beta_1x_{i1}+ beta_qx_{iq}+ epsilon_i$ 가설 : $H_0: beta_{q+1} = dots= beta_p=0 ;_{vs.} ;H_1:not ;H_0$ | 검정통계량 $$F = frac{SSR_{FM}-SSR_{RM}/(p-q)}{SSE_{FM}/(n-p-1)} sim_{H_o} ;F(p-q,n-p-1)$$ 검정통계량의 관측값 : $f$ | 유의수준 $ alpha$에서의 기각역 : $f geq F_{ alpha}(p-q,n-p-1)$ | 유의확률 = $P(F geq f)$ | . | . | General Linear Hypothesis $$H_0:H beta=0 ;_{vs.} ;H_1:not ;H_0$$ $H:r times(p+1) ;matrix, rank(H) = r$ | $ beta = ( beta_0, beta_1, dots, beta_p)^T$ | 검정통계량 $$F= frac{(SSR_{FM}-SSR_{RM})/(p-q)}{SSE_{FM}/(n-p-1)} sim_{H_0} F(r,n-p-1)$$ | . | . &#54924;&#44480;&#44228;&#49688;&#50640; &#45824;&#54620; &#52628;&#47200; . $ beta_1, beta_2, dots, beta_p$ 에 대한 추론 $ hat{ beta}=X(X^TX)^{-1}Y$ | $ frac{ hat{ beta_i}- beta_i}{_{s.e.}( hat{ beta_i})} sim t(n-p-1), ;_{s.e.}( hat{ beta_i})=d_{ii} hat{ sigma}$ | $d_{ii}$ ; diagonal elements of $D^{-1}, i=1, dots,p,$ $D = begin{pmatrix} s_{11} &amp; cdots &amp; s_{1p} vdots &amp; ddots &amp; vdots s_{p1} &amp; cdots &amp; s_{pp} end{pmatrix}$, $s_{ij} = sum_{k=11}^{n}(x_{ki}- overline x_i)(x_{kj}- overline x_j)$ | 가설검정 : $H_0 : beta_i= beta_i^0$. | 검정통계량 : $T = frac{ hat{ beta_i}- beta_i^0}{d_{ii} hat{ sigma}} sim_{H0} t(n-p-1), 관측값 : t$ | . | . . 대립가설 $ qquad qquad ;$ 유의확률 $ qquad quad$ 유의수준 $ alpha$ 기각역 . . $H_1: beta_i&gt; beta_i^0 qquad P(T geq t) qquad quad t geq t_{ alpha}(n-p-1)$ $H_1: beta_i&lt; beta_i^0 qquad P(T leq t) qquad quad t geq t_{ alpha}(n-p-1)$ $H_1: beta_i neq beta_i^0 qquad P(|T| geq |t|) qquad |t| geq t_{ alpha/2}(n-p-1)$ . . 모회귀계수(절편)$ beta_0$ 에 대한 추론 $ frac{ hat{ beta_0}- beta_0}{_{s.e.}( hat{ beta_0})} sim t(n-p-1)$, | $_{s.e.}( hat{ beta}_0) = hat{ sigma}( frac{1}{n}+ displaystyle sum_{i=1}^{p} displaystyle sum_{j=1}^{p} overline x_i d_{ij} overline x_{j})^{1/2}$ | . | . &#54217;&#44512;&#48152;&#51025;&#50696;&#52769; . $X=X0=(x{01}, dots,x_{0p})^T 가 주어졌을 때 평균반응의 예측 . 평균반응 (mean response) : $ mu_0 = E(Y|X_0) = beta_0+ beta_1x_{01}+ dots+ beta_px_{0p}$ | 평균반응 추정량 : $ hat{ mu_0}= hat{ beta_0}+ hat{ beta_1}x_{01}+ dots+ hat{ beta_p}x_{op}$ | $ frac{ hat{ mu_0}- mu_0}{_{s.e.}( hat{ mu_0})} sim t(n-p-1)$ | $_{s.e.}( hat{mu_{0}}) = hat{ sigma} bigg( frac{1}{n}+ displaystyle sum_{i=1}^{p} displaystyle sum_{j=1}^{p}(x_{0i}- overline x_i)d_{ij}(x_{0j}- overline x_{j}) bigg)^{1/2}$ | $ hat{ mu_0}의 ;100(1- alpha)$% 신뢰구간 : $ hat{ mu_{0}} pm t_{ alpha/2}(n-p-1)_{s.e.}( hat{ mu_{0}})$ | . | . &#50696;&#52769; . $X=X_0$ 가 주어졌을 때 $y=y_0$ 예측 $y_0= beta_0+ beta_1 x_{01}+ dots+ beta_p x_{0p}+ epsilon_0$ | 예측값 : $ hat{y_0}= hat{ beta_0}+ hat{ beta_1}x_{01}+ dots+ hat{ beta_{p}}x_{0p}$ | $ frac{ hat{y_0}-y_0}{_{s.e.}( hat{y_0})} sim t(n-p-1)$ | $_{s.e.}( hat{y_0}) = hat{ sigma} bigg(1+ frac{1}{n}+ displaystyle sum_{i=1}^{p} displaystyle sum_{j=1}^{p}(x_{0i}- overline x_i)d_{ij}(x_{0j}- overline x_{j}) bigg)^{1/2}$ | $ hat{y_0}$의 $100(1- alpha)$ % 신뢰구간 : $ hat{y_0} pm t_{ alpha/2}(n-p-1)_{s.e.}( hat{y_{0}})$ | . | .",
            "url": "https://simjaein.github.io/ji1598/2022/01/13/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EC%A4%91%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95.html",
            "relUrl": "/2022/01/13/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EC%A4%91%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Simple Linear Regression",
            "content": "&#46160; &#48320;&#49688; &#49324;&#51060;&#51032; &#44288;&#44228; . 대략적 파악 : 산점도(scatter plot) . | 상관분석(correlation analysis) . 두 변수 사이의 상관관계 분석 . 확률변수 $X,Y rightarrow rho$ = Corr($X,Y$) - 직선적인 관련성 파악 . | 회귀분석(regression analysis) . | . 두 변수 사이의 함수관계를 분석 . $x$ :독립변수 또는 설명변수, $Y$ : 종속변수 또는 반응변수 . $Y$ = $f(x) + epsilon, epsilon$ : 오차항 $ rightarrow f(x)?$ . 단순선형회귀분석 - 직선관계를 모형으로 분석 . $ quad$ $ quad$ $ quad$ $(f(x) = a+bx)$ . 중회귀분석 - 두 개 이상의 설명변수 사용 . $ quad$ $ quad$ $ quad$ $(f(x)=a+b_1x_1+ dots+b_{k}x_k)$ . Simple Linear Regression Model . (1) 자료구조 . 자료구조 : $(x_1,Y_1), dots,(x_n,Y_n)$ $ quad quad quad quad quad quad quad quad quad quad quad quad begin{cases}소문자:설명변수 rightarrow 상수 대문자:확률변수 end{cases}$ . | $(x_1, dots,x_n)$ : 설명변수(explanatory variable)(또는 독립변수) . 두 변수가 있을 때, 다른 한 변수에 영향을 주는 변수 . | $(Y_1, dots,Y_n)$ : 반응변수(response variable)(또는 종속변수) . 두 변수가 있을 때, 다른 한 변수에 영향을 받는 변수 . | 관측값 : $(x_1,y_1), dots,(x_n,y_n)$ . | . (2) Model . $Y_i = beta_0 + beta_{1}x_i + epsilon_i, quad i = 1,2, dots,n$ . | $( epsilon_1, dots, epsilon_n)$ : 오차항(random error) . 서로 독립이면서 평균이 0, 분산이 $ sigma^2$인 확률 변수 . | . (3) 용어 . 회귀계수(regression coefficient) (or 모수, parameter)$ beta_0$ :상수항 또는 절편 (constant coefficient or intercept)&gt; $ beta_1$ :기울기 (slope) . | . Basic Assumption . 기본 가정 . 선형성(Linearity) : $E(Y|x) = beta_0 + beta_1x$ . | 등분산성(Homoscedastic) : $Var(Y|x) = sigma^2$ . | 정규성(Normality) : $ epsilon_i$ ~ $N(0, sigma^2)$ . | 독립성(Independency) : $ epsilon_i$ are mutually independent $i = 1, dots,n$ . | . Least Square Estimation (LSE) . 최소제곱법(method of least squares)에 의한 추정 . 최소제곱추정량(LSE) | . $ quad quad quad$ $( hat{ beta_0}, hat{ beta_1})$ = argmin $ displaystyle sum_{i=1}^{n} {y_i - ( beta_0 + beta_{1}x_i) }^2$ . Least square fit : $ hat{y} big( equiv E( hat{Y}|x_0) big) = hat{ beta_0} + hat{ beta_1}x_0$ | 잔차(Residual) : $e_i = y_i - hat{y_i}$ | 오차제곱합 $ quad quad quad S = displaystyle sum_{i=1}^{n} epsilon^2_{i} displaystyle sum_{i=1}^{n} {y_i - ( beta_0 + beta_{1}x_i) }^2$ . | 정규방정식(normal equation) . | . $ frac{ partial{S}}{ partial beta_0}$ = $ displaystyle sum_{i=1}^{n}(y_i - beta_0 - beta_{1}x_i)$ . $ frac{ partial{S}}{ partial beta_1}$ = $-2 displaystyle sum_{i=1}^{n}x_{i}(y_i - beta_0 - beta_{1}x_i)$ . 최소제곱추정량 = 정규방정식의 해 | . $ quad quad quad begin{cases}-2 sum_{i=1}^{n}(y_i - hat{ beta_0} - hat beta_{1}x_i)=0 -2 sum_{i=1}^{n}x_{i}(y_i - hat{ beta_0} - hat beta_{1}x_i)=0 end{cases}$ . $ Rightarrow begin{cases}n hat{ beta_0} + hat{ beta_1} sum_{i=1}^{n}x_{i} = sum_{i=1}^{n}y_{i} sum_{i=1}^{n}x_{i} hat{ beta_0} + sum_{i=1}^{n}x^2_{i} hat{ beta_1} sum_{i=1}^{n}x_{i}y_i end{cases}$ . 최소제곱추정량 $ hat{ beta_1}$ = $ frac{ sum_{i=1}^{n}(x_i - overline{x})(y_i - overline{y})}{ sum_{i=1}^{n}(x_i - overline{x})^2}$ = $ frac{S_{xy}}{S_{xx}}$ | . $ hat{ beta_0}$ = $ overline{y} - hat{ beta_1} overline{x}$ . | . Estimation of error variance . 잔차(residual) : $e_i = y_i - hat{y_i}$, $( sum_{i=1}^{n}e_i = 0, sum_{i=1}^{n}x_{i}e_i = 0)$ $e_i$ = 오차의 관측값$ quad y_i = 실제값 quad hat{y_i} = 추정값$ . | 오차분산 $( sigma^2)$의 추정:- 잔차(오차) 제곱합 (residual (or error) sum of squares) : $$ SSE = displaystyle sum_{i=1}^{n}(y_i- hat{y_i})^2 = displaystyle sum_{i=1}^{n}e_{i}^2$$ 평균제곱오차 (mean squared error) : $MSE = frac{SEE}{n-2}$ | 오차분산의 추정값 : $ hat sigma^2 = MSE$ | . | . Decomposition of deviations . 총편차의 분해 $y_i- hat y = (y_i - hat y_i) + ( hat y_i - hat y), quad forall_i$ | 총편차(total deviation) = $y_i - overline y$ | 추측값의 편차 = $( hat y_i - overline{ hat y})$ = $( hat y_i - overline y), quad quad quad overline{ hat y} = frac{1}{n} displaystyle sum_{i=1}^{n} hat y_i = hat y$ $ Rightarrow$ 총편차 = 잔차 + 추측값의 편차 | . | . Decomposition of sum of squares . 제곱합의 분해 : $SST = SSE + SSR$ $$ displaystyle sum_{i=1}^{n}(y_i - overline y)^2= displaystyle sum_{i=1}^{n}(y_i - hat y_i)^2 + displaystyle sum_{i=1}^{n}( hat y_i - overline y)^2$$ | . . $ quad quad quad$ 제곱합의 종류 $ quad quad quad$ 정의 및 기호 $ quad quad quad$ 자유도 . . $ quad$총제곱합 (total sum of squares)$ qquad qquad qquad SST = displaystyle sum_{i=1}^{n}(y_i - overline y)^2 qquad qquad quad n-1$ . 잔차제곱합 (residual sum of squares)$ qquad qquad quad SSE = displaystyle sum_{i=1}^{n}(y_i - hat y_i)^2 qquad qquad quad n-2$ . 회귀제곱합 (regression sum of squares)$ qquad qquad SSR = displaystyle sum_{i=1}^{n}( hat y_i - overline y)^2 qquad qquad qquad 1$ . . Coefficient of determination . 결정계수 (Coefficient of determination) 정의 : $R^2 = frac{SSR}{SST} = 1 - frac{SSE}{SST}$ | 의미 : 회귀직선의 기여울 (총변동 가운데 회귀직선으로 설명되는 변동의 비율) | 성질 $0 leq R^2 leq1$ $R^2$ 값이 1에 가까울수록 회귀에 의한 설명이 잘 됨을 뜻함 $R^2 = r^2 (r :sample correlation) qquad 표본상관계수$ (단순선형회귀모형에서만 성립) . | . | . &#54924;&#44480;&#51649;&#49440;&#51032; &#50976;&#51032;&#49457; &#44160;&#51221; . Model : $Y_i = beta_0+ beta_{1}x_i+ epsilon_i, quad i = 1,2, dots,n, epsilon_i sim_{iid} N(0, sigma^2)$ | 회귀직선의 유의성 검정 (F-test) 가설 : $H_0 : beta_1 = 0 ;vs. H_1 : beta_1 neq0$ | 검정통계량 : $F = frac{MSR}{MSE} = frac{SSR/1}{SSE/(n-2)} sim_{H_0} ;F(1,n-2)$ | 검정통계량의 관측값 : $f$ | 유의수준 $ alpha$에서의 기각역 : $f geq F_{ alpha}(1,n-2)$ | 유의확률 = $P(F geq f)$ | . | . &#54924;&#44480;&#51649;&#49440;&#51032; &#50976;&#51032;&#49457; &#44160;&#51221; . - &#54924;&#44480;&#51649;&#49440;&#51032; &#50976;&#51032;&#49457; &#44160;&#51221;&#51012; &#50948;&#54620; &#48516;&#49328;&#48516;&#49437;&#54364; . 요인 $ qquad$ 제곱합(SS) $ qquad$ 자유도(df) $ qquad quad$ 평균제곱(MS) $ qquad qquad f qquad qquad qquad$ 유의확률 . . 회귀 $ qquad SSR qquad qquad quad$ 1 $ qquad qquad MSR= frac{SSR}{1} qquad quad f= frac{MSR}{MSE} qquad qquad ; P(F geq f)$ . 잔차 $ qquad SSE qquad qquad quad n-2 qquad quad MSE= frac{SSE}{n-2}$ . . 계 $ qquad quad SST qquad qquad quad n-1$ . . &#54924;&#44480;&#44228;&#49688;&#50640; &#45824;&#54620; &#52628;&#47200; . 모회귀계수(기울기) $ beta_1$ 에 대한 추론 $ beta_1$의 최소제곱추정량 : $ hat{ beta_1} = frac{S_{xY}}{S_{xx}}$ | 추정값 : $ hat{ beta_1} = frac{S_{xY}}{S_{xx}}$ | 추정량$ ; hat{ beta_1}$의 분포 : $ hat{ beta_1} sim N( beta_1, frac{ sigma^2}{S_{xx}})$ | studentized $ hat{ beta_1}$ 의 분포 : $ frac{ hat{ beta_1}- beta_1}{ hat{ sigma}/ sqrt{S_{xx}}} sim ; t(n-2), ; hat{ sigma} = sqrt{MSE}$ | $ hat{ beta_1}$의 $100(1- alpha)$% 신뢰구간 : $ hat{ beta_1} pm t_{ alpha/2}(n-2) hat{ sigma} = sqrt{S_{xx}}$ | . | . &#54924;&#44480;&#44228;&#49688;&#50640; &#45824;&#54620; &#52628;&#47200; . 모회귀계수(기울기) $ beta_1$에 대한 추론 가설검정 : $H_0: beta_1= beta_1^0$ | 검정통계량 : $T= frac{ hat{ beta_1}-{ beta_1^0}}{ hat{ sigma}/ sqrt{S_{xx}}} sim_{H_0}t(n-2)$, 관측값 : t | . | . . $ qquad qquad quad$대립가설$ qquad qquad quad$유의확률 $ qquad qquad quad유의수준 alpha$ 기각역 . . $ qquad qquad H_1: beta_1&gt; beta_{1}^0 qquad quad P(T geq t) qquad qquad quad t geq t_{ alpha} (n-2)$ $ qquad qquad H_1: beta_1&lt; beta_{1}^0 qquad quad P(T leq t) qquad qquad quad t geq t_{ alpha} (n-2)$ . $ qquad qquad H_1: beta_1 neq beta_{1}^0 qquad quad P(|T| geq |t|) qquad quad |t| geq t_{ alpha/2} (n-2)$ . &#54924;&#44480;&#44228;&#49688;&#50640; &#45824;&#54620; &#52628;&#47200; . 모회귀계수(절편) $ beta_0$에 대한 추론 $ beta_0$의 최소제곱추정량 : $ hat{ beta_0}= overline{Y}- hat{ beta_1} overline{x}$ | 추정값 : $ hat{ beta_0}$ = $ overline y- hat{ beta_1} overline x$ | 추정량 $ hat{ beta_0}$의 분포 : $ hat{ beta_0} sim N big( beta_0, sigma^2( frac{1}{n}+ frac{ overline{x}^2}{S_{xx}}) big)$ $$ frac{ hat{ beta_0}-{ beta_0}}{_{s.e.}( hat{ beta_0})} sim t(n-2), ;_{s.e.}( hat{ beta_0})= hat{ sigma} sqrt{ frac{1}{n}+ frac{ overline x^2}{S_{xx}}}$$ | $ hat{ beta_0}$의 $100(1- alpha)$% 신뢰구간 : $ hat{ beta_0} pm t_{ alpha/2}(n-2)_{s.e.}( hat{ beta_0})$ | . | . &#54924;&#44480;&#44228;&#49688;&#50640; &#45824;&#54620; &#52628;&#47200; . 모회귀계수(기울기) $ beta_0$에 대한 추론 가설검정 : $H_0: beta_0= beta_0^0$ | 검정통계량 : $T= frac{ hat{ beta_0}-{ beta_0^0}}{_{s.e.}( hat{ beta_0})} sim_{H_0}t(n-2)$, 관측값:t | . | . . $ qquad qquad quad$대립가설$ qquad qquad quad$유의확률 $ qquad qquad quad유의수준 alpha$ 기각역 . . $ qquad qquad H_1: beta_0&gt; beta_{0}^0 qquad quad P(T geq t) qquad qquad quad t geq t_{ alpha} (n-2)$ $ qquad qquad H_1: beta_0&lt; beta_{0}^0 qquad quad P(T leq t) qquad qquad quad t geq t_{ alpha} (n-2)$ . $ qquad qquad H_1: beta_0 neq beta_{0}^0 qquad quad P(|T| geq |t|) qquad quad |t| geq t_{ alpha/2} (n-2)$ . &#54217;&#44512;&#48152;&#51025;&#50696;&#52769; . $x=x_0$ 가 주어졌을 때 평균반응의 예측 평균반응 (mean response) : $ mu_0 = E(Y|x_0) = beta_0+ beta_1x_0$ | 평균반응 추정량 : $ hat{ mu_0} = hat{ beta_0} + hat{ beta_1}x_0$ | $ hat{ mu_0}$ 의 분포 : $ hat{ mu_0} sim N big( mu_0,( frac{1}{n} + frac{(x_0- overline x)^2}{S_{xx}}) sigma^2 big)$ $$ frac{ hat{ mu_0}- mu_0}{_{s.e.}( hat{ mu_0})} sim t(n-2), ; _{s.e.}( hat{ mu_0}) = hat{ sigma} sqrt{ frac{1}{n}+ frac{(x_0- overline x)^2}{S_{xx}}}$$ | $ hat{ mu_0}$의 $100(1- alpha)$% 신뢰구간 : $ hat{ mu_0} pm t_{ alpha/2}(n-2)_{s.e.}( hat{ mu_0})$ | . | . &#50696;&#52769; . $x=x_0$가 주어졌을 때 $y=y_0$ 예측 $y_0 = beta_0+ beta_1x_0+ epsilon_0$ | 예측값 : $ hat y_0 = hat{ beta_0}+ hat{ beta_1}x_0$ | $ hat y_0$ 의 분포 : $ hat y_0 sim N big( mu_0,(1+ frac{1}{n} + frac{(x_0- overline x)^2}{S_{xx}}) sigma^2 big)$ $$ frac{ hat{y_0}-y_0}{_{s.e.}( hat{y_0})} sim t(n-2), ; _{s.e.}( hat{y_0}) = hat{ sigma} sqrt{1+ frac{1}{n}+ frac{(x_0- overline x)^2}{S_{xx}}}$$ | $ hat{y_0}$의 $100(1- alpha)$% 신뢰구간 : $ hat{y_0} pm t_{ alpha/2}(n-2)_{s.e.}( hat{y_0})$ | . | .",
            "url": "https://simjaein.github.io/ji1598/2022/01/12/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EB%8B%A8%EC%88%9C%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html",
            "relUrl": "/2022/01/12/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EB%8B%A8%EC%88%9C%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "선형대수 고유값 고유벡터",
            "content": "&#44256;&#50976;&#44050;, &#44256;&#50976;&#48289;&#53552;&#51032; &#51221;&#51032; . 임의의 $n times n$ 행렬 ${ bf A}$에 대하여, 0이 아닌 솔루션 벡터 $ vec{x}$ 가 존재한다면 숫자 $ lambda$ 는 행렬 ${ bf A}$의 고유값이라고 할 수 있다. . ${ bf A} vec{x}$ = $ lambda vec{x}$ . 이 때, 솔루션 벡터 $ vec{x}$ 는 고유값 $ lambda$ 에 대응하는 고유벡터이다. . [주의사항1] 영벡터는 고유벡터로 보지 않는다. . ${ bf A} begin{bmatrix} 0 0 dots 0 end{bmatrix} = begin{bmatrix} 0 0 dots 0 end{bmatrix}$ . 즉, $ vec{x}$ = 영벡터라면 ${ bf A} vec{x}$도 영벡터이다. . ${ bf A} vec{x} = vec{x} = 1 cdot vec{x} = 2 vec{x} = 3 vec{x} = dots$ 모든 실수가 고유값임. . [주의사항2] 고유벡터는 무수히 많다. . - $ vec{x}$ : $ lambda$ 에 대한 고유벡터 $ Longrightarrow$ $k vec{x}$ : $ lambda$ 에 대한 고유벡터 . $2 vec{x}, 3 vec{x}, - vec{x}, dots$ 역시 모두 고유벡터 | . - ${ bf A} vec{x} = lambda vec{x}$ $ Longrightarrow A(2 vec{x}) = 2(A vec{x}) = 2 lambda vec{x} = lambda(2 vec{x})$ . $ because 2 vec{x}$ 도 $ lambda$ 에 대한 고유벡터 | . $ because$ | $ therefore$ | . [&#44256;&#50976;&#44050; &#44396;&#54616;&#44592;] . ex) ${ bf A}$ = $ begin{bmatrix} 0&amp;1 1&amp;0 end{bmatrix}$ . 1) &#44256;&#50976;&#44050; &#44396;&#54616;&#44592; . $|{ bf A}- lambda { bf I}| = 0$의 해 $ lambda^2 - 1 = 0$ $ because lambda = pm 1$ $ because$ $1$,$-1$이 ${ bf A}$의 고유값 . [&#44256;&#50976;&#48289;&#53552; &#44396;&#54616;&#44592;] $ leftarrow$ &#50672;&#47549;&#51068;&#52264;&#48169;&#51221;&#49885; $({ bf A}- lambda{ bf I}) vec{x}= vec{0}$&#51032; &#54644; . (i) $ lambda=1$에 대한 고유벡터 : $ vec{x} = begin{bmatrix} t t end{bmatrix}, quad t neq 0 $ . ${ bf A} - lambda{ bf I} = begin{bmatrix} -1&amp;1 1&amp;-1 end{bmatrix}$ . ${ bf A} = begin{bmatrix} 0&amp;1 1&amp;0 end{bmatrix}$ . ${ bf A} = begin{bmatrix} 0&amp;1|0 1&amp;0|0 end{bmatrix} sim begin{bmatrix} 0&amp;0|0 1&amp;-1|0 end{bmatrix} sim begin{bmatrix} 1&amp;-1|0 0&amp;0|0 end{bmatrix}$ . $ vec{x} - vec{y} = 0$ . $ because begin{cases} x= t y=t end{cases}$ . (ii) $ lambda$ = -1에 대한 고유벡터 : $ vec{x} = begin{bmatrix} s -s end{bmatrix}$ ($s neq 0$ ) . ${ bf A}- lambda { bf I}= begin{bmatrix} 1&amp;1 1&amp;1 end{bmatrix}$ . ${ bf A} = begin{bmatrix} 1&amp;1|0 1&amp;1|0 end{bmatrix} sim begin{bmatrix} 1&amp;1|0 0&amp;0|0 end{bmatrix}$ . $ because x + y = 0$ . $ because begin{cases} x= s y=-s end{cases}$ .",
            "url": "https://simjaein.github.io/ji1598/2021/12/31/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98_%EA%B3%A0%EC%9C%A0%EA%B0%92_%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0.html",
            "relUrl": "/2021/12/31/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98_%EA%B3%A0%EC%9C%A0%EA%B0%92_%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "python을 이용한 고유값과 고유벡터 문제풀이",
            "content": "선형대수학 . import numpy as np import numpy.linalg as lin . 1. . $A$ = $ left[ begin{array}{rrr} 0&amp;1 1&amp;0 end{array} right]$ 의 고유값과 고유벡터를 구해라. . exEin = np.array([[0,1],[1,0]]) . lin.eig(exEin) . (array([ 1., -1.]), array([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])) . 2. . $A$ = $ left[ begin{array}{rrr} 4&amp;2 3&amp;5 end{array} right]$ 의 고유값과 고유벡터를 구해라. . exEin = np.array([[4,2],[3,5]]) . lin.eig(exEin) . (array([2., 7.]), array([[-0.70710678, -0.5547002 ], [ 0.70710678, -0.83205029]])) . 3. . $A$ = $ left[ begin{array}{rrr} 1&amp;0&amp;0 0&amp;1&amp;0 0&amp;0&amp;1 end{array} right]$ 의 고유값과 고유벡터를 구해라. . exEin = np.array([[1,0,0],[0,1,0],[0,0,1]]) . lin.eig(exEin) . (array([1., 1., 1.]), array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])) . 4. . $A$ = $ left[ begin{array}{rrr} -4&amp;-6 3&amp;5 end{array} right]$ 의 고유값과 고유벡터를 구해라. . exEin = np.array([[-4,-6],[3,5]]) . lin.eig(exEin) . (array([-1., 2.]), array([[-0.89442719, 0.70710678], [ 0.4472136 , -0.70710678]])) . 5. . $A$ = $ left[ begin{array}{rrr} 5&amp;-1 -2&amp;1 end{array} right]$ 의 고유값과 고유벡터를 구해라. . exEin = np.array([[5, -1], [-2, 1]]) . lin.eig(exEin) . (array([5.44948974, 0.55051026]), array([[ 0.91209559, 0.21927526], [-0.40997761, 0.97566304]])) .",
            "url": "https://simjaein.github.io/ji1598/2021/12/31/python%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EA%B3%A0%EC%9C%A0%EA%B0%92%EA%B3%BC-%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0-%EA%B5%AC%ED%95%98%EA%B8%B0.html",
            "relUrl": "/2021/12/31/python%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EA%B3%A0%EC%9C%A0%EA%B0%92%EA%B3%BC-%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0-%EA%B5%AC%ED%95%98%EA%B8%B0.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "기말고사 1~2번풀이 (심재인)",
            "content": "#1. COVID19 &#49884;&#46020;&#48324; &#51217;&#51333;&#47456; &#49884;&#44033;&#54868; (30&#51216;) . 아래의 그림은 COVID19 예방접종의 시도별 현황을 캡쳐한 것이다. . . 이 정보는 특정 주기로 업데이트 되며 아래의 웹페이지 2번째 테이블에서 확인할 수 있다. . https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000 . 판다스의 pd.read_html() 함수를 이용해 위의 페이지에서 그림1의 테이블을 읽어오라. 그리고 folium의 choroplethmap을 활용하여 시도별 2차접종의 접종률을 시각화 하라. 시각화 예시는 아래와 같다. . . (풀이) . - 데이터프레임 읽어오기 . import pandas as pd . df=pd.read_html(&#39;https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000&#39;,encoding=&#39;utf-8&#39;)[1] df . 구분 1차접종 2차접종 3차접종 . 구분 당일 실적 당일 누계 당일 실적 당일 누계 당일 실적 당일 누계 . 0 합계 | 5851 | 43493325 | 1743 | 42070660 | 29240 | 11565083 | . 1 서울 | 1092 | 8081511 | 327 | 7848156 | 6576 | 2173116 | . 2 부산 | 323 | 2785658 | 96 | 2686254 | 1691 | 766316 | . 3 대구 | 173 | 1938156 | 37 | 1867302 | 596 | 443843 | . 4 인천 | 525 | 2480025 | 168 | 2400474 | 2436 | 632558 | . 5 광주 | 72 | 1216505 | 14 | 1173233 | 267 | 316210 | . 6 대전 | 197 | 1199583 | 42 | 1157132 | 582 | 293704 | . 7 울산 | 80 | 928762 | 19 | 896509 | 388 | 207834 | . 8 세종 | 54 | 279728 | 15 | 268580 | 166 | 64957 | . 9 경기 | 2272 | 11384986 | 705 | 11016264 | 11819 | 2817113 | . 10 강원 | 77 | 1303677 | 21 | 1263853 | 269 | 385809 | . 11 충북 | 113 | 1376943 | 34 | 1334316 | 805 | 382193 | . 12 충남 | 245 | 1825333 | 77 | 1765603 | 1057 | 521523 | . 13 전북 | 111 | 1545850 | 29 | 1497144 | 584 | 508956 | . 14 전남 | 101 | 1603261 | 28 | 1554519 | 481 | 544769 | . 15 경북 | 106 | 2201519 | 41 | 2126360 | 338 | 618715 | . 16 경남 | 218 | 2778313 | 62 | 2673402 | 833 | 750044 | . 17 제주 | 92 | 563515 | 28 | 541559 | 352 | 137423 | . - json 파일 . import json import requests . global_distriction_jsonurl=&#39;https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json&#39; global_dict = json.loads(requests.get(global_distriction_jsonurl).text) . prov=[global_dict[&#39;features&#39;][i][&#39;properties&#39;][&#39;name&#39;] for i in range(17)] prov . [&#39;서울특별시&#39;, &#39;부산광역시&#39;, &#39;대구광역시&#39;, &#39;인천광역시&#39;, &#39;광주광역시&#39;, &#39;대전광역시&#39;, &#39;울산광역시&#39;, &#39;세종특별자치시&#39;, &#39;경기도&#39;, &#39;강원도&#39;, &#39;충청북도&#39;, &#39;충청남도&#39;, &#39;전라북도&#39;, &#39;전라남도&#39;, &#39;경상북도&#39;, &#39;경상남도&#39;, &#39;제주특별자치도&#39;] . - 변형 . df.iloc[1:,1:].assign(prov=prov) . 1차접종 2차접종 3차접종 prov . 당일 실적 당일 누계 당일 실적 당일 누계 당일 실적 당일 누계 . 1 1092 | 8081511 | 327 | 7848156 | 6576 | 2173116 | 서울특별시 | . 2 323 | 2785658 | 96 | 2686254 | 1691 | 766316 | 부산광역시 | . 3 173 | 1938156 | 37 | 1867302 | 596 | 443843 | 대구광역시 | . 4 525 | 2480025 | 168 | 2400474 | 2436 | 632558 | 인천광역시 | . 5 72 | 1216505 | 14 | 1173233 | 267 | 316210 | 광주광역시 | . 6 197 | 1199583 | 42 | 1157132 | 582 | 293704 | 대전광역시 | . 7 80 | 928762 | 19 | 896509 | 388 | 207834 | 울산광역시 | . 8 54 | 279728 | 15 | 268580 | 166 | 64957 | 세종특별자치시 | . 9 2272 | 11384986 | 705 | 11016264 | 11819 | 2817113 | 경기도 | . 10 77 | 1303677 | 21 | 1263853 | 269 | 385809 | 강원도 | . 11 113 | 1376943 | 34 | 1334316 | 805 | 382193 | 충청북도 | . 12 245 | 1825333 | 77 | 1765603 | 1057 | 521523 | 충청남도 | . 13 111 | 1545850 | 29 | 1497144 | 584 | 508956 | 전라북도 | . 14 101 | 1603261 | 28 | 1554519 | 481 | 544769 | 전라남도 | . 15 106 | 2201519 | 41 | 2126360 | 338 | 618715 | 경상북도 | . 16 218 | 2778313 | 62 | 2673402 | 833 | 750044 | 경상남도 | . 17 92 | 563515 | 28 | 541559 | 352 | 137423 | 제주특별자치도 | . df.iloc[1:,1:].assign(prov=prov). set_index(&#39;prov&#39;).stack().stack().reset_index().rename(columns={0:&#39;value&#39;}) . prov level_1 level_2 value . 0 서울특별시 | 당일 누계 | 1차접종 | 8081511 | . 1 서울특별시 | 당일 누계 | 2차접종 | 7848156 | . 2 서울특별시 | 당일 누계 | 3차접종 | 2173116 | . 3 서울특별시 | 당일 실적 | 1차접종 | 1092 | . 4 서울특별시 | 당일 실적 | 2차접종 | 327 | . ... ... | ... | ... | ... | . 97 제주특별자치도 | 당일 누계 | 2차접종 | 541559 | . 98 제주특별자치도 | 당일 누계 | 3차접종 | 137423 | . 99 제주특별자치도 | 당일 실적 | 1차접종 | 92 | . 100 제주특별자치도 | 당일 실적 | 2차접종 | 28 | . 101 제주특별자치도 | 당일 실적 | 3차접종 | 352 | . 102 rows × 4 columns . df.iloc[1:,1:].assign(prov=prov). set_index(&#39;prov&#39;).stack().stack().reset_index().rename(columns={0:&#39;value&#39;}). query(&#39;level_1 == &quot;당일 누계&quot; and level_2 == &quot;2차접종&quot; &#39; ) . prov level_1 level_2 value . 1 서울특별시 | 당일 누계 | 2차접종 | 7848156 | . 7 부산광역시 | 당일 누계 | 2차접종 | 2686254 | . 13 대구광역시 | 당일 누계 | 2차접종 | 1867302 | . 19 인천광역시 | 당일 누계 | 2차접종 | 2400474 | . 25 광주광역시 | 당일 누계 | 2차접종 | 1173233 | . 31 대전광역시 | 당일 누계 | 2차접종 | 1157132 | . 37 울산광역시 | 당일 누계 | 2차접종 | 896509 | . 43 세종특별자치시 | 당일 누계 | 2차접종 | 268580 | . 49 경기도 | 당일 누계 | 2차접종 | 11016264 | . 55 강원도 | 당일 누계 | 2차접종 | 1263853 | . 61 충청북도 | 당일 누계 | 2차접종 | 1334316 | . 67 충청남도 | 당일 누계 | 2차접종 | 1765603 | . 73 전라북도 | 당일 누계 | 2차접종 | 1497144 | . 79 전라남도 | 당일 누계 | 2차접종 | 1554519 | . 85 경상북도 | 당일 누계 | 2차접종 | 2126360 | . 91 경상남도 | 당일 누계 | 2차접종 | 2673402 | . 97 제주특별자치도 | 당일 누계 | 2차접종 | 541559 | . - 머지할 데이터프레임을 찾자 . pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;) . 행정구역(시군구)별 총인구수 (명) . 0 서울특별시 | 9532428 | . 1 부산광역시 | 3356311 | . 2 대구광역시 | 2390721 | . 3 인천광역시 | 2945009 | . 4 광주광역시 | 1442454 | . 5 대전광역시 | 1454228 | . 6 울산광역시 | 1122566 | . 7 세종특별자치시 | 368276 | . 8 경기도 | 13549577 | . 9 강원도 | 1537717 | . 10 충청북도 | 1596948 | . 11 충청남도 | 2118977 | . 12 전라북도 | 1789770 | . 13 전라남도 | 1834653 | . 14 경상북도 | 2627925 | . 15 경상남도 | 3318161 | . 16 제주특별자치도 | 676569 | . pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;}) ## 머지할 df &lt;-- 이름 줄 가치가 X . prov pop . 0 서울특별시 | 9532428 | . 1 부산광역시 | 3356311 | . 2 대구광역시 | 2390721 | . 3 인천광역시 | 2945009 | . 4 광주광역시 | 1442454 | . 5 대전광역시 | 1454228 | . 6 울산광역시 | 1122566 | . 7 세종특별자치시 | 368276 | . 8 경기도 | 13549577 | . 9 강원도 | 1537717 | . 10 충청북도 | 1596948 | . 11 충청남도 | 2118977 | . 12 전라북도 | 1789770 | . 13 전라남도 | 1834653 | . 14 경상북도 | 2627925 | . 15 경상남도 | 3318161 | . 16 제주특별자치도 | 676569 | . - 머지 . df.iloc[1:,1:].assign(prov=prov). set_index(&#39;prov&#39;).stack().stack().reset_index().rename(columns={0:&#39;value&#39;}). query(&#39;level_1 == &quot;당일 누계&quot; and level_2 == &quot;2차접종&quot; &#39; ). merge(pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;})) . prov level_1 level_2 value pop . 0 서울특별시 | 당일 누계 | 2차접종 | 7848156 | 9532428 | . 1 부산광역시 | 당일 누계 | 2차접종 | 2686254 | 3356311 | . 2 대구광역시 | 당일 누계 | 2차접종 | 1867302 | 2390721 | . 3 인천광역시 | 당일 누계 | 2차접종 | 2400474 | 2945009 | . 4 광주광역시 | 당일 누계 | 2차접종 | 1173233 | 1442454 | . 5 대전광역시 | 당일 누계 | 2차접종 | 1157132 | 1454228 | . 6 울산광역시 | 당일 누계 | 2차접종 | 896509 | 1122566 | . 7 세종특별자치시 | 당일 누계 | 2차접종 | 268580 | 368276 | . 8 경기도 | 당일 누계 | 2차접종 | 11016264 | 13549577 | . 9 강원도 | 당일 누계 | 2차접종 | 1263853 | 1537717 | . 10 충청북도 | 당일 누계 | 2차접종 | 1334316 | 1596948 | . 11 충청남도 | 당일 누계 | 2차접종 | 1765603 | 2118977 | . 12 전라북도 | 당일 누계 | 2차접종 | 1497144 | 1789770 | . 13 전라남도 | 당일 누계 | 2차접종 | 1554519 | 1834653 | . 14 경상북도 | 당일 누계 | 2차접종 | 2126360 | 2627925 | . 15 경상남도 | 당일 누계 | 2차접종 | 2673402 | 3318161 | . 16 제주특별자치도 | 당일 누계 | 2차접종 | 541559 | 676569 | . df.iloc[1:,1:].assign(prov=prov). set_index(&#39;prov&#39;).stack().stack().reset_index().rename(columns={0:&#39;value&#39;}). query(&#39;level_1 == &quot;당일 누계&quot; and level_2 == &quot;2차접종&quot; &#39; ). merge(pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;})). eval(&#39;prop= value / pop&#39;) . prov level_1 level_2 value pop prop . 0 서울특별시 | 당일 누계 | 2차접종 | 7848156 | 9532428 | 0.823311 | . 1 부산광역시 | 당일 누계 | 2차접종 | 2686254 | 3356311 | 0.800359 | . 2 대구광역시 | 당일 누계 | 2차접종 | 1867302 | 2390721 | 0.781062 | . 3 인천광역시 | 당일 누계 | 2차접종 | 2400474 | 2945009 | 0.815099 | . 4 광주광역시 | 당일 누계 | 2차접종 | 1173233 | 1442454 | 0.813359 | . 5 대전광역시 | 당일 누계 | 2차접종 | 1157132 | 1454228 | 0.795702 | . 6 울산광역시 | 당일 누계 | 2차접종 | 896509 | 1122566 | 0.798625 | . 7 세종특별자치시 | 당일 누계 | 2차접종 | 268580 | 368276 | 0.729290 | . 8 경기도 | 당일 누계 | 2차접종 | 11016264 | 13549577 | 0.813034 | . 9 강원도 | 당일 누계 | 2차접종 | 1263853 | 1537717 | 0.821902 | . 10 충청북도 | 당일 누계 | 2차접종 | 1334316 | 1596948 | 0.835541 | . 11 충청남도 | 당일 누계 | 2차접종 | 1765603 | 2118977 | 0.833234 | . 12 전라북도 | 당일 누계 | 2차접종 | 1497144 | 1789770 | 0.836501 | . 13 전라남도 | 당일 누계 | 2차접종 | 1554519 | 1834653 | 0.847310 | . 14 경상북도 | 당일 누계 | 2차접종 | 2126360 | 2627925 | 0.809140 | . 15 경상남도 | 당일 누계 | 2차접종 | 2673402 | 3318161 | 0.805688 | . 16 제주특별자치도 | 당일 누계 | 2차접종 | 541559 | 676569 | 0.800449 | . _df=df.iloc[1:,1:].assign(prov=prov). set_index(&#39;prov&#39;).stack().stack().reset_index().rename(columns={0:&#39;value&#39;}). query(&#39;level_1 == &quot;당일 누계&quot; and level_2 == &quot;2차접종&quot; &#39; ). merge(pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;})). eval(&#39;prop= value / pop&#39;) . - 저장 . import folium . m = folium.Map([36,128],zoom_start=7,scrollWheelZoom=False) choro = folium.Choropleth( data = _df, geo_data= global_dict, columns=[&#39;prov&#39;,&#39;prop&#39;], key_on = &#39;feature.properties.name&#39; ) choro.add_to(m) #m . &lt;folium.features.Choropleth at 0x7fecc0a489a0&gt; . #2. COVID19 &#49884;&#46020;&#48324;/&#50900;&#48324; &#54869;&#49328;&#44284;&#51221; &#49884;&#44033;&#54868; (40&#51216;) . 아래는 COVID19 확진자수를 지역별로 매일 기록한 자료이다. . https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv . 자료를 판다스로 불러온 결과는 아래와 같다. . 일자 계(명) 서울 부산 대구 인천 광주 대전 울산 세종 경기 강원 충북 충남 전북 전남 경북 경남 제주 검역 . 0 누적(명) | 457,612 | 158,774 | 16,555 | 19,114 | 25,299 | 6,353 | 8,809 | 5,675 | 1,588 | 136,546 | 8,889 | 8,942 | 13,174 | 6,453 | 4,498 | 11,471 | 15,236 | 3,762 | 6,474 | . 1 2020-01-20 | 1 | - | - | - | 1 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 2 2020-01-21 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 3 2020-01-22 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 4 2020-01-23 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 679 2021-11-28 | 3,925 | 1,673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1,090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | 10 | . 680 2021-11-29 | 3,308 | 1,393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | 3 | . 681 2021-11-30 | 3,032 | 1,186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | 8 | . 682 2021-12-01 | 5,123 | 2,222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1,582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | 20 | . 683 2021-12-02 | 5,266 | 2,268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1,495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | 5 | . 일별로 기록된 COVID19 확진자수를 월별로 통합한 뒤 2021-01 ~ 2021-10 기간의 발생률을 계산하여 시각화하라. 시각화는 plotly의 choropleth_mapbox를 이용하며 시간의 추이를 표현하기 위해 animation_frame 옵션을 사용한다. 시각화 예시는 생략함. . (풀이) . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv&#39;) df . 일자 계(명) 서울 부산 대구 인천 광주 대전 울산 세종 경기 강원 충북 충남 전북 전남 경북 경남 제주 검역 . 0 누적(명) | 457,612 | 158,774 | 16,555 | 19,114 | 25,299 | 6,353 | 8,809 | 5,675 | 1,588 | 136,546 | 8,889 | 8,942 | 13,174 | 6,453 | 4,498 | 11,471 | 15,236 | 3,762 | 6,474 | . 1 2020-01-20 | 1 | - | - | - | 1 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 2 2020-01-21 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 3 2020-01-22 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 4 2020-01-23 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 679 2021-11-28 | 3,925 | 1,673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1,090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | 10 | . 680 2021-11-29 | 3,308 | 1,393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | 3 | . 681 2021-11-30 | 3,032 | 1,186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | 8 | . 682 2021-12-01 | 5,123 | 2,222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1,582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | 20 | . 683 2021-12-02 | 5,266 | 2,268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1,495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | 5 | . 684 rows × 20 columns . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)) . 서울 부산 대구 인천 광주 대전 울산 세종 경기 강원 충북 충남 전북 전남 경북 경남 제주 . 일자 . 2020-01-20 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2020-01-21 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2020-01-22 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2020-01-23 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2020-01-24 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-11-28 1673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | . 2021-11-29 1393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | . 2021-11-30 1186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | . 2021-12-01 2222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | . 2021-12-02 2268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | . 683 rows × 17 columns . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index() . prov 일자 서울특별시 부산광역시 대구광역시 인천광역시 광주광역시 대전광역시 울산광역시 세종특별자치시 경기도 강원도 충청북도 충청남도 전라북도 전라남도 경상북도 경상남도 제주특별자치도 . 0 2020-01-20 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2020-01-21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 2020-01-22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 2020-01-23 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 2020-01-24 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 678 2021-11-28 | 1673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | . 679 2021-11-29 | 1393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | . 680 2021-11-30 | 1186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | . 681 2021-12-01 | 2222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | . 682 2021-12-02 | 2268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | . 683 rows × 18 columns . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index() . prov 일자 서울특별시 부산광역시 대구광역시 인천광역시 광주광역시 대전광역시 울산광역시 세종특별자치시 경기도 강원도 충청북도 충청남도 전라북도 전라남도 경상북도 경상남도 제주특별자치도 . 0 2020-01-20 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2020-01-21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 2020-01-22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 2020-01-23 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 2020-01-24 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 678 2021-11-28 | 1673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | . 679 2021-11-29 | 1393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | . 680 2021-11-30 | 1186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | . 681 2021-12-01 | 2222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | . 682 2021-12-02 | 2268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | . 683 rows × 18 columns . - 여기가 어려움. 아래의 변환이 필요함 . &#39;2020-01-20&#39;, &#39;2020-01&#39; . (&#39;2020-01-20&#39;, &#39;2020-01&#39;) . &#39;2020-01-20&#39;[:7] . &#39;2020-01&#39; . &#39;2020-01-20&#39;.split(&#39;-&#39;)[0]+ &#39;-&#39; + &#39;2020-01-20&#39;.split(&#39;-&#39;)[1] . &#39;2020-01&#39; . - 방법1 . _df=df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index() _df.assign(일자=list(map(lambda x: x[:7] , _df[&#39;일자&#39;]))) . prov 일자 서울특별시 부산광역시 대구광역시 인천광역시 광주광역시 대전광역시 울산광역시 세종특별자치시 경기도 강원도 충청북도 충청남도 전라북도 전라남도 경상북도 경상남도 제주특별자치도 . 0 2020-01 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2020-01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 2020-01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 2020-01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 2020-01 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 678 2021-11 | 1673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | . 679 2021-11 | 1393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | . 680 2021-11 | 1186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | . 681 2021-12 | 2222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | . 682 2021-12 | 2268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | . 683 rows × 18 columns . - 방법2 . df.assign(일자=list(map(lambda x: x[:7], df.일자 ))) . 일자 계(명) 서울 부산 대구 인천 광주 대전 울산 세종 경기 강원 충북 충남 전북 전남 경북 경남 제주 검역 . 0 누적(명) | 457,612 | 158,774 | 16,555 | 19,114 | 25,299 | 6,353 | 8,809 | 5,675 | 1,588 | 136,546 | 8,889 | 8,942 | 13,174 | 6,453 | 4,498 | 11,471 | 15,236 | 3,762 | 6,474 | . 1 2020-01 | 1 | - | - | - | 1 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 2 2020-01 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 3 2020-01 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . 4 2020-01 | 0 | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 679 2021-11 | 3,925 | 1,673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1,090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | 10 | . 680 2021-11 | 3,308 | 1,393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | 3 | . 681 2021-11 | 3,032 | 1,186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | 8 | . 682 2021-12 | 5,123 | 2,222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1,582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | 20 | . 683 2021-12 | 5,266 | 2,268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1,495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | 5 | . 684 rows × 20 columns . 현실적으로 이정도까지 앞을 내다보는건 힘든것 같아요.. | . - 방법3 . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))) . prov 일자 서울특별시 부산광역시 대구광역시 인천광역시 광주광역시 대전광역시 울산광역시 세종특별자치시 경기도 강원도 충청북도 충청남도 전라북도 전라남도 경상북도 경상남도 제주특별자치도 . 0 2020-01 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2020-01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 2020-01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 2020-01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 2020-01 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 678 2021-11 | 1673 | 148 | 106 | 278 | 52 | 53 | 4 | 5 | 1090 | 63 | 25 | 121 | 45 | 25 | 103 | 89 | 35 | . 679 2021-11 | 1393 | 144 | 88 | 233 | 61 | 43 | 2 | 15 | 910 | 56 | 33 | 52 | 49 | 28 | 68 | 86 | 44 | . 680 2021-11 | 1186 | 79 | 78 | 192 | 52 | 43 | 3 | 22 | 909 | 84 | 59 | 81 | 50 | 36 | 68 | 60 | 22 | . 681 2021-12 | 2222 | 143 | 86 | 326 | 29 | 88 | 17 | 20 | 1582 | 105 | 48 | 96 | 50 | 40 | 97 | 127 | 27 | . 682 2021-12 | 2268 | 158 | 70 | 355 | 39 | 166 | 18 | 8 | 1495 | 145 | 49 | 149 | 71 | 39 | 106 | 94 | 31 | . 683 rows × 18 columns . - 방법3을 택하도록 하자. 위에서 일자를 ym으로 바꾸고 tidydata를 만들자. . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))). rename(columns={&#39;일자&#39;:&#39;ym&#39;}).set_index(&#39;ym&#39;).stack().reset_index().rename(columns={0:&#39;value&#39;}) . ym prov value . 0 2020-01 | 서울특별시 | 0 | . 1 2020-01 | 부산광역시 | 0 | . 2 2020-01 | 대구광역시 | 0 | . 3 2020-01 | 인천광역시 | 1 | . 4 2020-01 | 광주광역시 | 0 | . ... ... | ... | ... | . 11606 2021-12 | 전라북도 | 71 | . 11607 2021-12 | 전라남도 | 39 | . 11608 2021-12 | 경상북도 | 106 | . 11609 2021-12 | 경상남도 | 94 | . 11610 2021-12 | 제주특별자치도 | 31 | . 11611 rows × 3 columns . - groupby 적용 . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))). rename(columns={&#39;일자&#39;:&#39;ym&#39;}).set_index(&#39;ym&#39;).stack().reset_index().rename(columns={0:&#39;value&#39;}). groupby([&#39;ym&#39;,&#39;prov&#39;]).agg({&#39;value&#39;:sum}).reset_index() . ym prov value . 0 2020-01 | 강원도 | 0 | . 1 2020-01 | 경기도 | 2 | . 2 2020-01 | 경상남도 | 0 | . 3 2020-01 | 경상북도 | 0 | . 4 2020-01 | 광주광역시 | 0 | . ... ... | ... | ... | . 403 2021-12 | 전라남도 | 79 | . 404 2021-12 | 전라북도 | 121 | . 405 2021-12 | 제주특별자치도 | 58 | . 406 2021-12 | 충청남도 | 245 | . 407 2021-12 | 충청북도 | 97 | . 408 rows × 3 columns . - query . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))). rename(columns={&#39;일자&#39;:&#39;ym&#39;}).set_index(&#39;ym&#39;).stack().reset_index().rename(columns={0:&#39;value&#39;}). groupby([&#39;ym&#39;,&#39;prov&#39;]).agg({&#39;value&#39;:sum}).reset_index(). query(&#39;ym &gt;= &quot;2021-01&quot; and ym&lt;=&quot;2021-10&quot;&#39;) . ym prov value . 204 2021-01 | 강원도 | 488 | . 205 2021-01 | 경기도 | 5353 | . 206 2021-01 | 경상남도 | 686 | . 207 2021-01 | 경상북도 | 593 | . 208 2021-01 | 광주광역시 | 702 | . ... ... | ... | ... | . 369 2021-10 | 전라남도 | 459 | . 370 2021-10 | 전라북도 | 672 | . 371 2021-10 | 제주특별자치도 | 225 | . 372 2021-10 | 충청남도 | 1467 | . 373 2021-10 | 충청북도 | 1556 | . 170 rows × 3 columns . - merge . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))). rename(columns={&#39;일자&#39;:&#39;ym&#39;}).set_index(&#39;ym&#39;).stack().reset_index().rename(columns={0:&#39;value&#39;}). groupby([&#39;ym&#39;,&#39;prov&#39;]).agg({&#39;value&#39;:sum}).reset_index(). query(&#39;ym &gt;= &quot;2021-01&quot; and ym&lt;=&quot;2021-10&quot;&#39;). merge(pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;})) ## 머지할 df &lt;-- 이름 줄 가치가 X . ym prov value pop . 0 2021-01 | 강원도 | 488 | 1537717 | . 1 2021-02 | 강원도 | 169 | 1537717 | . 2 2021-03 | 강원도 | 466 | 1537717 | . 3 2021-04 | 강원도 | 354 | 1537717 | . 4 2021-05 | 강원도 | 501 | 1537717 | . ... ... | ... | ... | ... | . 165 2021-06 | 충청북도 | 363 | 1596948 | . 166 2021-07 | 충청북도 | 544 | 1596948 | . 167 2021-08 | 충청북도 | 1302 | 1596948 | . 168 2021-09 | 충청북도 | 1192 | 1596948 | . 169 2021-10 | 충청북도 | 1556 | 1596948 | . 170 rows × 4 columns . - prop = value / pop 을 계산하자 . df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))). rename(columns={&#39;일자&#39;:&#39;ym&#39;}).set_index(&#39;ym&#39;).stack().reset_index().rename(columns={0:&#39;value&#39;}). groupby([&#39;ym&#39;,&#39;prov&#39;]).agg({&#39;value&#39;:sum}).reset_index(). query(&#39;ym &gt;= &quot;2021-01&quot; and ym&lt;=&quot;2021-10&quot;&#39;). merge(pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;})). eval(&#39;prop = value/pop&#39;) . ym prov value pop prop . 0 2021-01 | 강원도 | 488 | 1537717 | 0.000317 | . 1 2021-02 | 강원도 | 169 | 1537717 | 0.000110 | . 2 2021-03 | 강원도 | 466 | 1537717 | 0.000303 | . 3 2021-04 | 강원도 | 354 | 1537717 | 0.000230 | . 4 2021-05 | 강원도 | 501 | 1537717 | 0.000326 | . ... ... | ... | ... | ... | ... | . 165 2021-06 | 충청북도 | 363 | 1596948 | 0.000227 | . 166 2021-07 | 충청북도 | 544 | 1596948 | 0.000341 | . 167 2021-08 | 충청북도 | 1302 | 1596948 | 0.000815 | . 168 2021-09 | 충청북도 | 1192 | 1596948 | 0.000746 | . 169 2021-10 | 충청북도 | 1556 | 1596948 | 0.000974 | . 170 rows × 5 columns . df2=df.iloc[1:].set_index(&#39;일자&#39;).iloc[:,1:-1].applymap(lambda x: int(x.replace(&#39;,&#39;,&#39;&#39;) if x!=&#39;-&#39; else 0)). T.reset_index().rename(columns={&#39;index&#39;:&#39;prov&#39;}).assign(prov=prov).set_index(&#39;prov&#39;).T.reset_index(). assign(일자=lambda df: list(map(lambda x: x[:7] , df[&#39;일자&#39;]))). rename(columns={&#39;일자&#39;:&#39;ym&#39;}).set_index(&#39;ym&#39;).stack().reset_index().rename(columns={0:&#39;value&#39;}). groupby([&#39;ym&#39;,&#39;prov&#39;]).agg({&#39;value&#39;:sum}).reset_index(). query(&#39;ym &gt;= &quot;2021-01&quot; and ym&lt;=&quot;2021-10&quot;&#39;). merge(pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv&#39;). rename(columns={&#39;행정구역(시군구)별&#39;:&#39;prov&#39;,&#39;총인구수 (명)&#39;:&#39;pop&#39;})). eval(&#39;prop = value/pop&#39;) . # from IPython.display import HTML # fig=px.choropleth_mapbox(df2, # geojson=global_dict, # color=&#39;prop&#39;, # locations=&#39;prov&#39;, # animation_frame=&#39;ym&#39;, # featureidkey=&#39;properties.name&#39;, # center={&quot;lat&quot;: 36, &quot;lon&quot;: 128}, # mapbox_style=&quot;carto-positron&quot;, # range_color=(0, df2.prop.max()), # height=1200, # zoom=6.5) # fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) # HTML(fig.to_html(include_mathjax=False, config=dict({&#39;scrollZoom&#39;:False}))) .",
            "url": "https://simjaein.github.io/ji1598/2021/12/20/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94_%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC%ED%92%80%EC%9D%B4.html",
            "relUrl": "/2021/12/20/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94_%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC%ED%92%80%EC%9D%B4.html",
            "date": " • Dec 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "프로젝트 . 프로그램 . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://simjaein.github.io/ji1598/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simjaein.github.io/ji1598/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}